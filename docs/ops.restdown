---
title: Marlin operations notes
markdown2extras: wiki-tables, code-friendly
apisections: .
---

# Marlin operations notes

This document describes Marlin, the compute engine of Manta, and the tools
useful for monitoring and operating it.

**Before reading this document, you should be very familiar with using Manta and
especially the compute (jobs) part of Manta.  You should also be comfortable
with all the [reference material](http://apidocs.joyent.com/manta/) on how the
system works from a user's perspective.**


# Marlin overview

There are three core components of Marlin:

* A small fleet of **supervisors** manages the execution of jobs.  (Supervisors
  used to be called **workers**, and you may still see that terminology).
  Supervisors pick up new inputs, locate the servers where the input objects are
  stored, issue tasks to execute on those servers, monitor the execution of
  those tasks, and decide when the job is done.  Each supervisor can manage many
  jobs, but each job is managed by only one supervisor at a time.
* Job tasks execute directly on the Manta storage nodes.  Each node has an
  **agent** (i.e., the "marlin agent") running in the global zone that manages
  tasks assigned to that node and the zones available for running those tasks.
* Within each compute zone, task execution is managed by a **lackey** under the
  control of the agent.

All of Marlin's state is stored in **Moray**, which is a key-value store based
on **Manatee**, which is itself a highly-available cluster of Postgresql
instances.  Moray is the same component used for Manta metadata.  Marlin data is
stored on its own shard, separate from the storage metadata.

A few other components are involved in executing jobs:

* **Muskie** is the Manta frontend web server (behind the loadbalancer).  All
  user requests related to jobs (creating jobs, submitting input, and fetching
  status, outputs, and errors) go through muskie.  To create jobs and submit
  inputs, Muskie creates and updates records in Moray.
* **Wrasse** is a separate component that periodically scans for recently
  completed jobs, archives the saved state into flat objects back in Manta, and
  then removes job state from Moray.


# Monitoring Marlin

As with other Manta components, there are amon alarms for critical Marlin
issues.

The easiest way to monitor Marlin state is to set up the dashboard.  It's just
a GUI built on top of the JSON state exported over HTTP by each Marlin
component.  This dashboard will eventually become part of the Manta software
stack, but it's currently a separate component running in a regular zone in
us-east-2.  Instructions for setting it up are on [the
wiki](https://hub.joyent.com/wiki/display/dev/Manta+Production+Deployment#MantaProductionDeployment-Marlindashboard).

The dashboard shows the state of all supervisors and agents and the last time
each was restarted; the state of all compute zones, which gives a sense of
overall system health and utilization; and information about jobs, groups, and
task streams, which represent all the work in the system (both currently
executing and queued up).


# Debugging Marlin (common tasks): overall system state

The "mrjob" and "mrerrors" tools summarize Marlin state by reading it directly
from Moray.  This is usually the first step towards figuring out what's going on
with Marlin overall or with a particular job.

Instructions for setting up "mrjob" and the other Marlin tools are available on
[the
wiki](https://hub.joyent.com/wiki/display/dev/Manta+Production+Deployment#MantaProductionDeployment-Marlintools%28mrjob%2Cmrerrors%2Cmrjobreport%29).

Some examples below use the Moray client tools, which should also be available
if you follow the above procedure.


## List running jobs

Use `mrjob list -s running`:

    $ mrjob list -s running
    JOBID                                LOGIN          S NOUT NERR NRET NDISP NCOMM
    b1f8c8ce-8afe-445e-8846-484ac908ebd0 jason          R    0    0    0     1     0

## List recently completed jobs

Use `mrjob list -t 60`, where "60" is the number of seconds back to look:

    $ mrjob list -t 60
    JOBID                                LOGIN          S NOUT NERR NRET NDISP NCOMM
    ed4eff3c-5e2e-4fcb-ad67-3ac09629056f jason          D    1    0    0   485   485

## Fetch details about a job

Use `mrjob get`:

    $ mrjob get a2922490-c8de-e4b3-abbc-f3367464b651 
           Job a2922490-c8de-e4b3-abbc-f3367464b651
      Job name interactive compute job
          User thoth (aed35417-4c53-4d6c-a127-fd8a6e55723b)
         State running
    Supervisor dd55ea98-9dc9-4b57-84ab-380ba5252fed
       Created 2014-01-16T22:22:49.173Z (1h10m12.679s ago)
      Progress 1 inputs read, 1 tasks dispatched, 0 tasks committed
       Results 0 outputs, 0 errors, 0 retries
       Pending 0 uncommitted done, 0 intermediate objects
       Phase 0 map

You can use `-p` to get details about what the job runs in each phase:

    $ mrjob get -p a2922490-c8de-e4b3-abbc-f3367464b651 
           Job a2922490-c8de-e4b3-abbc-f3367464b651
      Job name interactive compute job
          User thoth (aed35417-4c53-4d6c-a127-fd8a6e55723b)
         State running
    Supervisor dd55ea98-9dc9-4b57-84ab-380ba5252fed
       Created 2014-01-16T22:22:49.173Z (1h11m00.527s ago)
      Progress 1 inputs read, 1 tasks dispatched, 0 tasks committed
       Results 0 outputs, 0 errors, 0 retries
       Pending 0 uncommitted done, 0 intermediate objects
       Phase 0 map
         asset /poseidon/public/medusa/agent.sh
         asset /thoth/stor/medusa-config-fa937d4b-f699-4f1b-bea5-69147fa97977.json
         asset /thoth/stor/thoth/analyzers/.thoth.87707.1389910968.355
          exec "/assets/poseidon/public/medusa/agent.sh"

See `mrjob --help` for more options.

## List job inputs, outputs, retries, and errors (as a user would see them)

All of these are capped at 1000 results by default.

`mrjob inputs JOBID`: list input objects for a job (capped at 1000)

`mrjob outputs JOBID`: list output objects from a job (capped at 1000)

`mrjob errors JOBID`: list retries from a job (capped at 1000)

`mrjob retries JOBID`: list retries from a job (capped at 1000)

## Fetch summary of errors for a job

Use `mrerrors -j JOBID`.  The output includes internal error messages and
retried errors, which are not exposed to end users.

## List tasks not-yet-completed for a given job (and see where they're running)

Use `mrjob where` to list uncompleted tasks and see where they're running:

    $ mrjob where e493ab87-fcf0-e991-8b82-8f649696d197
    TASKID                               PH       NIN SERVER                        
    6ce64b78-691b-4703-970a-de2fb84b69f1  0         - 1.cn.us-east.joyent.us        
         map: /dap/stor/mdb.log

Note that physical storage nodes in Manta are identified by mantaComputeId
rather than server\_uuid or hostname.  You need to translate this to figure out
which physical server that corresponds to.

## Translating from mantaComputeId to hostname

The mapping from mantaComputeId to server name is stored in SAPI metadata for
the "manta" application, but there's a cache in a text file on each of the
us-east-{1,2,3} headnodes.  You can look up CNs using `manta-cn`:

    [root@headnode (us-east-2) ~]# /var/tmp/common/manta-cn -h 25.cn.us-east.joyent.us
    MS08214

## See the history of a given task

You may find a task through `mrjob get` or `mrjob where` and want to know its
history: what inputs or previous-phase tasks caused this task to be created?  Is
it a retry of a previous task?  How many times was it retried, and on which
hosts?  `mrjob taskhistory` helps answer these questions.

The point of this tool is to show two things: how a given input moved through
multiple phases in Marlin, and how individual tasks are retried.  The goal is
that given any taskid, it will find predecessors in previous phases,
predecessors in the same phase (retries), successors in the same phase
(retries), and successors in subsequent phases.  The main thing it doesn't do
is go *through* reduce phases, because that's usually counter-productive and in
general it's not possible to correlate inputs with outputs across a reduce
phase.

Here's an example usage.  I ran this job (note the phases):


    $ mrjob get -p ea784e03-a735-cd29-d913-b1b9cb5f0503
           Job ea784e03-a735-cd29-d913-b1b9cb5f0503
          User dap (ddb63097-4093-4a74-b8e8-56b23eb253e0)
         State done
    Supervisor eff884b4-f678-4069-8522-1bbe2e4fcb90
       Created 2014-01-17T19:22:02.326Z (16m38.446s ago)
          Done 2014-01-17T19:22:35.110Z (32.784s total)
      Archived 2014-01-17T19:22:38.064Z (16m02.708s ago)
      Progress 100 inputs read, 242 tasks dispatched, 242 tasks committed
       Results 15 outputs, 0 errors, 12 retries
       Phase 0 map
          exec "wc"
       Phase 1 map
          exec "wc"
       Phase 2 reduce (15)
          exec "wc"
       Phase 3 map
          exec "wc"


So it's 100 inputs -> map -> map -> 15 reducers -> map.  Because Marlin is
currently disabled on 9.stor in production, there were retries in both map
phases and reduce phases, so this is a good example to show what
`mrjob taskhistory` does.  I used `mrjob get -t` to find some of these tasks,
although if a job was currently stuck, you could also use `mrjob where`.

Here's a normal history for one of the first phase tasks:


    $ mrjob taskhistory 33b7f617-141e-4673-8056-a27a6f511b60
    2014-01-17T19:22:02.814Z  jobinput   318ed0ad-8aed-4f74-af90-a7cfefc87880
        /dap/stor/datasets/cmd/cmd/acct/acctcms.c
    
    2014-01-17T19:22:03.746Z  map task   33b7f617-141e-4673-8056-a27a6f511b60
                                         (attempt 1, host 11.cn.us-east.joyent.us)
        /dap/stor/datasets/cmd/cmd/acct/acctcms.c
    
    2014-01-17T19:22:04.142Z  taskoutput b9bb82de-416d-4ac3-a962-09f75a140932
        /dap/jobs/ea784e03-a735-cd29-d913-b1b9cb5f0503/stor/dap/stor/datasets/cmd/cmd/acct/acctcms.c.0.33b7f617-141e-4673-8056-a27a6f511b60
    
    2014-01-17T19:22:08.663Z  map task   1b370599-90b3-4c03-a388-c8967798d396
                                         (attempt 1, host 25.cn.us-east.joyent.us)
    
    2014-01-17T19:22:09.078Z  taskoutput c98b68d1-1b76-4adf-845c-f7392d860694
        /dap/jobs/ea784e03-a735-cd29-d913-b1b9cb5f0503/stor/dap/stor/datasets/cmd/cmd/acct/acctcms.c.1.1b370599-90b3-4c03-a388-c8967798d396
    
    2014-01-17T19:22:13.398Z  taskinput  1d4cf4de-9c4c-4841-b801-0595f69b2ff0
        /dap/jobs/ea784e03-a735-cd29-d913-b1b9cb5f0503/stor/dap/stor/datasets/cmd/cmd/acct/acctcms.c.1.1b370599-90b3-4c03-a388-c8967798d396
    
    2014-01-17T19:22:02.447Z  reducer    015f6dbb-8bb3-4b07-b806-12bdf46fd8a8
                                         (attempt 1, host 11.cn.us-east.joyent.us)
                                         4 inputs

This shows the jobinput that created the task, the output from the task that
created the next map task, the output from that task that becamse a taskinput
for the reducer, and the reducer.  For each task, it shows the host assigned to
run it.  You'd get the exact same output if you specified tasks
1b370599-90b3-4c03-a388-c8967798d396 or 015f6dbb-8bb3-4b07-b806-12bdf46fd8a8.

Here's an example where one of the early phase map tasks failed and had to be retried:


    $ mrjob taskhistory a322ebe6-3279-40ee-8c7e-88130def17b4
    2014-01-17T19:22:03.220Z  jobinput   5f72c373-9155-459e-9032-0ac18ecaef6a
        /dap/stor/datasets/cmd/cmd/addbadsec/addbadsec.c
    
    2014-01-17T19:22:03.660Z  map task   a322ebe6-3279-40ee-8c7e-88130def17b4
                                         (attempt 1, host 9.cn.us-east.joyent.us)
        /dap/stor/datasets/cmd/cmd/addbadsec/addbadsec.c
    
    2014-01-17T19:22:06.411Z  error      bb08a232-75d6-4b59-a80c-3f369f84d64a
                                         InternalError
                                         internal error: agent timed out
    
    2014-01-17T19:22:07.551Z  map task   e4c80edb-5302-4f6f-af19-336be9834e92
                                         (attempt 2, host 26.cn.us-east.joyent.us)
        /dap/stor/datasets/cmd/cmd/addbadsec/addbadsec.c
    
    2014-01-17T19:22:07.679Z  taskoutput c79dc94d-5800-4e59-b808-998c94024c2f
        /dap/jobs/ea784e03-a735-cd29-d913-b1b9cb5f0503/stor/dap/stor/datasets/cmd/cmd/addbadsec/addbadsec.c.0.e4c80edb-5302-4f6f-af19-336be9834e92
    
    2014-01-17T19:22:12.279Z  map task   c6cbe67f-2c9d-4c50-ae0c-53590303f544
                                         (attempt 1, host 19.cn.us-east.joyent.us)
    
    2014-01-17T19:22:12.529Z  taskoutput 70275969-c92f-4700-96c2-9564158be0b2
        /dap/jobs/ea784e03-a735-cd29-d913-b1b9cb5f0503/stor/dap/stor/datasets/cmd/cmd/addbadsec/addbadsec.c.1.c6cbe67f-2c9d-4c50-ae0c-53590303f544
    
    2014-01-17T19:22:15.271Z  taskinput  22b96b55-86f2-43a1-9271-9a9334726a61
        /dap/jobs/ea784e03-a735-cd29-d913-b1b9cb5f0503/stor/dap/stor/datasets/cmd/cmd/addbadsec/addbadsec.c.1.c6cbe67f-2c9d-4c50-ae0c-53590303f544
    
    2014-01-17T19:22:02.447Z  reducer    6e009faa-a0d8-478a-9b66-ae82852fbf45
                                         (attempt 1, host 25.cn.us-east.joyent.us)
                                         7 inputs

In this case, we see the map task we asked about, the error it produced, and the map retry task below it on a different host.

Here's what happens when we select a reducer that failed:

    $ mrjob taskhistory 3ac67932-75ba-4ce0-891e-1b295949a3be
    2014-01-17T19:22:02.447Z  reducer    3ac67932-75ba-4ce0-891e-1b295949a3be
                                         (attempt 1, host 9.cn.us-east.joyent.us)
                                         input stream open
    
    2014-01-17T19:22:06.424Z  error      c3923a5f-93ea-425e-92c3-694b6bf08b3d
                                         InternalError
                                         internal error: agent timed out
    
    2014-01-17T19:22:07.542Z  reducer    c98bd373-6c08-4032-b405-bf0a0e18f820
                                         (attempt 2, host 12.cn.us-east.joyent.us)
                                         9 inputs

We don't see any of the previous or subsequent phase tasks because
"taskhistory" doesn't cross reduce phases.  (That would usually degenerate to
showing everything in the job, which isn't useful here.)

Here's a particularly complicated case.  The input went through two normal map
phases, then became a taskinput to the *second* attempt for a reducer.  We
still show the first reducer here in the output, but it shows up before the
taskinput, indicating that the first reducer failed logically before this
object was assigned to the reducer, so it was assigned directly to the second
attempt:

    $ mrjob taskhistory 94851372-49b9-48fd-b5ca-b51b330561ab
    2014-01-17T19:22:03.024Z  jobinput   237d6925-75e4-4a0a-9186-cd214b510e6a
        /dap/stor/datasets/cmd/cmd/acct/acctwtmp.c
    
    2014-01-17T19:22:03.633Z  map task   4906e7b9-c65e-4655-95d0-5bdcbda62b39
                                         (attempt 1, host 26.cn.us-east.joyent.us)
        /dap/stor/datasets/cmd/cmd/acct/acctwtmp.c
    
    2014-01-17T19:22:04.164Z  taskoutput 41dd8fac-2923-42ec-a893-43318596472b
        /dap/jobs/ea784e03-a735-cd29-d913-b1b9cb5f0503/stor/dap/stor/datasets/cmd/cmd/acct/acctwtmp.c.0.4906e7b9-c65e-4655-95d0-5bdcbda62b39
    
    2014-01-17T19:22:07.380Z  map task   94851372-49b9-48fd-b5ca-b51b330561ab
                                         (attempt 1, host 293.cn.us-east.joyent.us)
    
    2014-01-17T19:22:07.604Z  taskoutput 5cbb34ab-a0f4-4144-9629-629d323cc1f0
        /dap/jobs/ea784e03-a735-cd29-d913-b1b9cb5f0503/stor/dap/stor/datasets/cmd/cmd/acct/acctwtmp.c.1.94851372-49b9-48fd-b5ca-b51b330561ab
    
    2014-01-17T19:22:02.447Z  reducer    660f7838-4055-49b3-8376-dec9648ec2a5
                                         (attempt 1, host 9.cn.us-east.joyent.us)
                                         input stream open
    
    2014-01-17T19:22:06.397Z  error      4263bcc0-3a5e-433b-9460-b5970044cb51
                                         InternalError
                                         internal error: agent timed out
    
    2014-01-17T19:22:12.279Z  taskinput  686236b8-1de2-4ef6-aa6b-e4f908c2497a
        /dap/jobs/ea784e03-a735-cd29-d913-b1b9cb5f0503/stor/dap/stor/datasets/cmd/cmd/acct/acctwtmp.c.1.94851372-49b9-48fd-b5ca-b51b330561ab
    
    2014-01-17T19:22:07.539Z  reducer    318b47e5-a234-4fa0-b11e-e80701cfc90d
                                         (attempt 2, host 9.cn.us-east.joyent.us)
                                         input stream open
    
    2014-01-17T19:22:11.437Z  error      681faf97-2ae5-4938-89a8-d422e7156d11
                                         InternalError
                                         internal error: agent timed out
    
    2014-01-17T19:22:12.689Z  reducer    721187a7-5df1-4ce1-9c3b-9e54ffd71cce
                                         (attempt 3, host 20.cn.us-east.joyent.us)
                                         6 inputs

## Using the Moray tools to fetch detailed state

Sometimes it's necessary to dig into the Moray state directly because "mrjob"
doesn't have a subcommand to fetch exactly what you want.  Please file tickets
for such things, but in the meantime, you can use the Moray client tools to
extract state directly.  The Moray client tools should be available on your
PATH if you've set up the Marlin tools.

## Figuring out which jobsupervisor is managing a job

You can find the jobsupervisor that's managing a job with:

    $ getobject marlin_jobs_v2 994703de-0c5c-49e8-ba98-8361d1624ae1 | \
        json value.worker
    4fdcffe8-30ef-476e-8266-279a241c76c0

where 994703de-0c5c-49e8-ba98-8361d1624ae1 is the jobid.

The returned value is the zonename of the jobsupervisor that *normally* manages
the job.  In most cases, this is also the jobsupervisor that's currently
managing the job.  However, it's possible for one jobsupervisor to take over for
another.  You can check this with:

    $ findobjects marlin_health_v2 instance=4fdcffe8-30ef-476e-8266-279a241c76c0 | \
        json value
    {
      "component": "worker",
      "instance": "4fdcffe8-30ef-476e-8266-279a241c76c0",
      "generation": "2013-07-02T16:23:36.810Z"
    }

This case indicates that the supervisor is functioning normally.  If one
supervisor takes over for another, you'll see an "operatedBy" field with the
uuid of the supervisor that's taken over for this supervisor.


# Debugging Marlin (common tasks): On the storage nodes

## Figuring out what's running

mrjob only shows overall system state.  Once a task is issued to a server, you
have to contact the agent running on that server to figure out the status.  The
dashboard shows running groups and streams, but you can get more information by
logging into the box and running the "mrgroups" and "mrzones" tools.
Continuing the above example, 25.cn.us-east.joyent.us corresponds to MS08214 in
us-east-1, so we can log into that box and run `mrgroups`:

    [root@MS08214 (us-east-1) ~]# mrgroups
    JOBID                                PH NTASKS RUN SHARE  LOGIN
    4ff04c4d-4540-483d-94ca-d515320d2b9d  0      0   1      1 dap
    946587d1-3ef6-46d5-b139-186d59813f9d  3      1   0      1 poseidon

"Groups" refer to a bunch of tasks from the same job and phase.  A two-phase
job will have two groups on each physical server where it's running.  Each
group may have multiple "streams" associated with it, each corresponding to a
compute zone where tasks are running.  We can see these with:

    [root@MS08214 (us-east-1) ~]# mrgroups -s
    JOBID       PH ZONE                                 LAST START              
    4ff04c4d...  0 1118c00b-4729-4c18-a289-f54afe2d9e9d 2013-07-02T22:16:17.746Z
    946587d1...  3 4836fd7b-d80d-4b2d-8519-8955ca5621ff 2013-07-02T22:20:30.068Z

In this simple case, each of the two jobs running on this box has one group,
and each group has one zone.  But in general:

* On a given box, you can have many groups for each job -- one for each phase.
* On a given box, for a given group, you can have many streams -- one for each
  zone that's been assigned to that group.

You can even see exactly what processes the user's job is running:

    [root@MS08214 (us-east-1) ~]# mrgroups -sv
    JOBID       PH ZONE                                 LAST START              
    4ff04c4d...  0 1118c00b-4729-4c18-a289-f54afe2d9e9d 2013-07-02T22:16:17.746Z
      90397 ./node lib/agent.js
        90403 /bin/bash --norc

This is useful when a customer complains of a hung job or something and you
want to go see exactly what it's doing.  The full procedure is:

* Use "mrjob" or "findobjects" to find the task of interest (usually, one of
  the only running tasks) and figure out which machine it's running on.
* ssh to the machine it's running on.
* Run `mrgroups -sv` to view running streams on that box and the processes
  they're running
* Use truss, DTrace, or whatever other tools you usually use for inspecting
  process state -- with the usual caveats about interfering with production
  systems.

The user task's stdout and stderr are saved to /var/tmp/marlin\_task inside the
zone.  These are the stdout and stderr files that will be saved to Manta when
the task completes, so do not modify or remove them!


## Figuring out what ran in the past

After the job has completed, all we save is the lackey log (see above).  That's
currently saved for a few days in the global zone, under
/var/smartdc/marlin/log/zones.  Files are named JOBID.PHASENUM.UUID.  The usual
way to find the right one is to find the taskId that you're debugging as
described above, and then:

    $ grep -l TASKID /var/smartdc/marlin/log/zones/JOBID.*

That usually returns pretty quickly, and you can view the file with bunyan(1).


## Viewing service logs

The marlin agent and jobsupervisor log to their SMF log files.  You can view the
logs with bunyan(1).  Both use a log level of "info" by default.  Recall that
bunyan(1) has a "-p" option that allows you to see log records at "debug" and
"trace" levels without restarting the process.

As you might expect:

* **fatal** errors are those that will cause the process terminate because they
  are unrecoverable
* **error** events are serious issues that generally indicate either a system
  failure of some kind or a bug in the software and which are probably inducing
  at least retries, if not errors in jobs
* **warn** events are less serious issues that may be seen under normal
  conditions, or may be unexpected events that are not impacting users' jobs

**Like the rest of Manta, Marlin components' logs are rotated hourly up to
Manta.**  You can find a given hour's logs in
/poseidon/stor/logs/{marlin-agent,jobsupervisor}/YYYY/MM/DD/HH.

The lackey log is not currently rotated up to Manta.  See "Figuring out what ran
in the past" for information on that log.


# Anticipated frequent issues (by symptom)

## Users want more information about job progress

The "mjob get" output for a job includes a count of tasks issued and tasks
completed.  This can be used to measure progress.  If you've got a 100-object
map job where each input produces one output, there will be 100 tasks issued,
and when they all complete, the job is done.  If you tack on a second phase to
that job with count=2 reducers, there will be 102 tasks, and so on.

In general, the user can figure out how many tasks a job *should* have, but
Manta can't necessarily infer this in many cases.  That's because each task can
emit any number of output objects.  So if you have a two-phase map-map job and
feed in 100 inputs, you could emit 1, 2, 5, or 100 outputs from *each* task in
the first phase, and Manta has know way to know.  That's why the only progress
Manta provides is number of tasks issued and number of tasks completed, from
which a user may be able to compute their own measure of progress.


## Users observe non-zero count of errors from "mjob get"

See "Fetch summary of errors for a job" above.  Most error messages should be
pretty self-explanatory, but some of the more complicated ones are described
below.


## Users observe non-zero count of retries from "mjob get"

Besides the "tasks issued", "tasks completed", and error counters, the "mjob
get" output for a job also shows a count of retries.  These aren't generally
actionable for users except to explain latency.

Retries represent internal failures that the system believes may be transient.
There are currently three causes:

* An agent crashed.  This is the most common.  When an agent crashes, all tasks
  that were running or queued on that system are retried, preferably on other
  systems.  There may be a latency hit for restarting or requeueing the task,
  but it shouldn't impact correctness.
* An agent failed to heartbeat for a full minute, possibly a result of a system
  panic, network partition, or excessive load.  The same thing happens as for an
  agent crash.
* The user hits a condition where Marlin allocated a large number of zones to
  the job, but later decided to forcibly take some of those zones back.

You can find out exactly what caused the retries using "mrerrors -j JOBID".  See
"Fetch summary of errors for a job" above.

## User observes job in "queued" state

Jobs enter the "queued" state when they're submitted, but should move to
"running" very quickly, even if Manta's compute resources are all tied up.  If
jobs remain in state "queued" for more than a minute, there are several things
to check:

* Run a simple no-op job, like "mjob create -m wc &lt; /dev/null".  That job
  should complete within a second or two without dispatching any tasks.  If it
  does, there's likely something invalid about the user's job that the system
  failed to handle properly.  Check the corresponding jobsupervisor's log (see
  "Figuring out which jobsupervisor is managing a job").
* If that test job remains queued for a minute, check that the jobsupervisor
  services are healthy (namely, that the SMF service is running, not in
  maintenance).  If they're maintenance, try clearing one.  If it comes up, it
  should pick up your test job and run it.
* If any jobsupervisors are healthy but all jobs are still queued, check the
  supervisor logs for recent errors (with "bunyan -lerror").  It's likely that
  they're not able to connect to the Marlin Moray shard, in which case the next
  step is to check whether Moray is working.


## Job hung (not making forward progress)

A job is making forward progress if any of the counters shown by "mjob get" are
incremented over time.  If the counters stay steady, there may be a Marlin
issue, but there may also be a user error or simply a long-running task.

* The first thing to check is whether the user has ended the input stream.
  "mjob get" will report "inputDone": true once the input stream has been ended.
  If the input stream is not ended, Marlin is waiting for the user to submit
  inputs.
* Check if there are any tasks outstanding.  See "List tasks not-yet-completed
  for a given job", and then investigate with "Figuring out what's running".
* If nothing's running, check that the jobsupervisor responsible for this job is
  healthy.  See "Figuring out which jobsupervisor is managing a job".


## Poor job performance

Alongside "mrjob" and "mrerrors" is a tool called "mrjobreport" which prints out
a summary of time spent processing a job.  This can be used to tell where time
was spent: dispatching tasks, queued behind other tasks, executing, or
committing the results.  It can also identify the slowest tasks.  You can then
look at the lackey log from those jobs to figure out why they took so long.

For smaller jobs, you can also use "mrjob log JOBID", which prints out a log of
everything that happened for a job.  You can use this to find long periods where
nothing happened.


## Error: User Task Error (for unknown reason)

User task errors usually indicate that the user's script either exited with a
non-zero status, or one of the user's processes dumped core.  The error message
should generally be pretty clear, and where appropriate should have a link to
the stderr and core file.  (Stderr is not saved for successful tasks.)

If a user can't figure out why their bash script exited with non-zero status,
check if they're running afoul of bash's pipeline exit semantics.  From bash(1):

    The  return  status of a pipeline is the exit status of the last com-
    mand, unless the pipefail option is enabled.  If pipefail is enabled,
    the  pipeline's  return  status  is the value of the last (rightmost)
    command to exit with a non-zero status, or zero if all commands  exit
    successfully.

Users likely want to be setting pipefail, which you can do using "set -o
pipefail" right in the bash script.


## Error: Task Init Error

There are several common causes of TaskInitErrors, and the error message should
generally be clear in each case:

* an asset failed to be downloaded (the HTTP status code will be included in
  the error message)
* the user had an "init" script which either returned a non-zero exit status or
  dumped core
* Requested image is not available: this is supposed to be emitted when the user
  asks for an image that is not supported using the "image" property of a job.
  If the user did not ask for any image, see below.
* Not enough memory available: the user asked for more memory than the default,
  and all of the memory on the system they were assigned to was spoken for.
  The task may succeed later.  We should be keeping track of these stats so
  that we can see if we need to allocate more memory to Marlin.
* Not enough disk available: same as memory, but for disk.

Note that the memory and disk errors are different from the similar
UserTaskErrors "user task ran out of memory" and "user task ran out of local
disk space", which mean that the task actually did run, but bumped up against
the requested limits.  The user should ask for more memory or disk,
respectively.

More esoteric errors indicate more serious Marlin issues:

* "Requested image is not available," and the user did not request any
  particular image.  We've seen this in cases where the Marlin agent was enabled
  on a node *other* than a Manta storage node (e.g., a CN hosting the metadata
  services).  Audit that the only instances of marlin-agent enabled in the whole
  fleet are the ones on the Manta storage nodes (shrimps).


## Error: Internal Error

Users should never see internal errors.  Transient ones are generally retried,
and persistent ones often represent serious issues.  Use "mrerrors -j JOBID" to
see the details, including the internal error message.

The most common cause is "lackey timeout".  This can be a result of user error
(e.g., if the user pstops the lackey, or kills it, possibly by trying to halt
the zone), or it can indicate that the lackey crashed.  Look for core files in
the zone and file a bug.  You can also look in the lackey log to see why it
crashed.

One error we've seen is ": not connected: ".  This is issued by a jobsupervisor
when attempting to locate an object, but when it fails to contact the
electric-moray service.  Check supervisor health, look for errors in the log,
and check whether electric-moray is online.


# Controlling Marlin

Occasionally it may be necessary or desirable to modify the running Marlin
system.

## Modifying Marlin state

As described above, Marlin's state lives in Moray.  **Extreme care should be
taken when modifying it directly.**

## Cancel a running job

Use `mrjob cancel JOBID`, which produces no output on success.  This is a last
resort option for jobs that have run amok.  Any time this is necessary, there's
likely an underlying software bug that needs to be filed and fixed.

## Deleting a job

In rare instances (always involving a serious Marlin bug), the presence of a job
may disrupt the system so much that cancelling it is not tenable or not
sufficient to stabilize the system.  In such cases, it's possible to delete the
job, which removes all records associated with the job from Moray.

If the job has already been completed and archived (has "timeArchiveDone" set),
deleting a job has only a small user-facing impact: the user will no longer be
able to fetch inputs, outputs, errors, and so on from the "live" API.  They will
be able to fetch them through the archived objects (in.txt, out.txt, and so on).
See the user-facing REST API documentation for details.  This transition happens
anyway for all jobs after a few hours; deleting an archived job only has the
effect of making this transition happen immediately.

**If the job has not been completed or has not been archived, deleting the job
will result in data loss for the user.**  In particular, the inputs, outputs,
errors, and so on will all be lost permanently.  **This can also have an
unpredictable impact on the rest of Marlin, which is not designed to have state
removed while a job is running.**  This operation should be used with extreme
care, and likely only by engineering.

To delete a job, use `mrjob delete JOBID`.  Be sure to have read the above
caveats.

## Quiescing a supervisor

It's possible to *quiesce* a supervisor, which causes that supervisor to
continue processing whatever jobs it's working on, but to avoid picking up new
jobs.  This is useful when the supervisor is being removed from service (as
during some upgrades), or in cases where the supervisor has become clogged due
to a bug but is still picking up new jobs (that are then becoming hung).

The "mrsup" command can be used to both quiesce and unquiesce a supervisor.
These commands produce no output on success, but cause entries to be written to
the supervisor log.  The quiesce state is not persistent, so if the supervisor
crashes or restarts for another reason, it will start picking up new jobs when
it comes up again.


# Reference

The information in this section may be useful once you're familiar with Marlin
internals.


## Internal state

As mentioned above, all of Marlin's state is stored using JSON records in Moray
using several buckets:

* **marlin\_jobs\_v2**: When the user creates a job, muskie creates a record in
  this bucket.  The global job state is stored in this record, and is
  periodically updated by the supervisor as execution progresses.
* **marlin\_jobinputs\_v2**: As users submit inputs, muskie creates records in
  this bucket.
* **marlin\_tasks\_v2**: Supervisors poll for new jobinputs, locate the
  corresponding objects, and assign work to agents by writing new records into
  this bucket.  Agents poll for new tasks assigned to them in this bucket and
  then execute them.
* **marlin\_taskinputs\_v2**: For reduce tasks, which operate on many objects,
  the supervisor writes a separate record in this bucket for each input object.
* **marlin\_taskoutputs\_v2**: As tasks emit outputs, the agent writes records
  into this bucket.  If the job has a subsequent phase, these outputs will
  become tasks or inputs for the next phase.  Otherwise, they'll be marked job
  outputs.
* **marlin\_errors\_v2**: When supervisors or agents emit errors (including
  retryable errors), they write records into this bucket.
* **marlin\_health\_v2**: Unlike the other buckets, where each record is
  associated with a particular job, this bucket is only used by supervisors and
  agents to report health.  There's one record per supervisor and per agent.
  See "Health checking" below.

This design may seem unnecessarily indirect in some cases, but it keeps each
record small so that we can support streaming arbitrary numbers of objects
through the system.

The schema for these buckets is not documented or stable, but you can find the
latest version (with comments) on
[mo](https://mo.joyent.com/marlin/blob/master/lib/schema.js).


## Heartbeats, failures, and health checking

Supervisors and agents heartbeat periodically by writing records into the
**marlin_health_v2** bucket.  Heartbeats are monitored by all supervisors.

The most common failure mode is a restart (crash).  Since the supervisor state
is entirely reconstructible from what's in Moray, supervisor restarts are pretty
straightforward.  For simplicity, agents always reset the world and start from a
clean state when they come up.  Supervisors detect agent restarts and abort and
re-issue all outstanding tasks for that agent.

The other main failure mode is a hang or partition, manifested as a failure to
heartbeat for an extended period.  If this happens to a supervisor, another
supervisor takes over the failed supervisor's jobs.  When the failed one comes
back, it coordinates to take over its work.  If an agent disappears for an
extended period, supervisors treat this much like a restart, by failing
outstanding tasks and re-issuing them on other servers.

Lackeys heartbeat periodically by making HTTP requests to the agent.  On
failure, the agent fails the task.

If wrasse fails, archiving is delayed, but this has little effect on users.
Wrasse recovers from restarts by rerunning any archives that were in progress.

Muskie instances are totally stateless.  Restarts are trivially recoverable, and
extended failures and partitions cause requests to be vectored to other
instances.


## Kang state

Jobsupervisors and agents export Kang entry points that describe their internal
state.  For agents, the HTTP server runs on port 9080, and you can get this
state with:

    $ curl -si http://GZ_IP_ADDRESS:9080/kang/snapshot | json

For jobsupervisors, you can get this with:

    $ curl -si http://ZONE_IP_ADDRESS/kang/snapshot | json

This API is undocumented and unstable.  It's the same one that feeds the
dashboard.


# General Manta tips

This section should be folded into a generic "Manta" document because they apply
to all Manta services.

## Finding a Manta service

The `manta-status` command can be run from the GZ of the headnode and shows all
Manta service instances.  You can view just the jobsupervisors with:


    [root@headnode (us-east-2) ~]# manta-status | egrep '^SERVICE|.*jobsupervisor'
    SERVICE         SHARD  DATACENTER  ZONE      ZONE IPS        SERVER IP  STORAGE ID                 COMPUTE ID
    jobsupervisor   1      us-east-3   9271ab00  -               -          -                          -
    jobsupervisor   1      us-east-2   eb2cf788  10.117.0.71     10.9.0.49  -                          -
    jobsupervisor   1      -           4fdcffe8  -               -          -                          -
    jobsupervisor   1      -           3b76a819  -               -          -                          -
    jobsupervisor   1      us-east-2   ad34c936  10.117.0.70     10.9.0.48  -                          -
    jobsupervisor   1      us-east-3   c958cb5b  -               -          -                          -
    
There's a known issue where the "datacenter" column shows "-" for instances in
the us-east-1 datacenter.

In a given datacenter, "manta-status" can only show the zone IP and server IP
for instances deployed in that datacenter.

You can log into one of these zones using `manta-login` from the GZ of the
headnode in the same datacenter.  You can give `manta-login` a zone short uuid
(e.g., ad34c936) or a role like "jobsupervisor", in which case it will print
this list and ask you which one you want to log into.


# .
