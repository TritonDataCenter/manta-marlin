---
title: Marlin
markdown2extras: wiki-tables, code-friendly
apisections: Task Control API
---

# Marlin

Marlin is a facility for running compute jobs on Manta storage nodes.  This
document describes the desired end state for version 1.


# Overview and Public API

For an overview of what Marlin is and how users interact with it, see the
documentation in muskie.git.  Really, go read that first.  This document assumes
familiarity with that, including how the rest of the Manta object store works.


# Design goals

Marlin facilitates the following use cases:

* MapReduce in a generic environment (e.g., log analysis)
* MapReduce in a node-based environment (i.e., make it particularly easy for
  node apps)
* Straight Map with no reduce (e.g., video transcoding)
* Allowing others to run compute on your data (e.g., for government
  organizations with lots of public data, e.g.,
  http://community.topcoder.com/pds-challenge/)
* Hadoop itself?

Of course, we want this to be easy to use, monitor, and debug.  A good
barometer is that word count, the MR equivalent of "hello world", should be
trivial to write.

Marlin defines two public APIs:

* Job API: allows end users to submit, cancel, and monitor compute jobs.  This
  will ultimately become part of the Manta API.
* Task Control API: used by user code running inside Marlin to coordinate
  execution and report progress.

All DateTimes in these APIs use W3C DTF in UTC ("YYYY-MM-DDThh:mm:ss.sZ").



# Task Control API

The Task Control API is used by tasks (user code running in compute zones on
Manta storage nodes) to retrieve parameters and report on progress.  The API is
serviced not by a public endpoint, but by a small server running inside the
task's own compute zone that's implicitly scoped to that zone's current task.
As a result, task identifiers are not actually necessary in the task API.

This API is *not* used to monitor task status; that's provided by the
public-facing Job API.

## GET /info

Retrieves information about the current job and task.

### Returns

This field returns the entire job info (as `GET /jobs/:jobid` does) plus the
following fields:

|| **Field** || **Type** || **Description** ||
|| taskId || String || Task identifier ||
|| taskInputKey || Array&nbsp;of&nbsp;Strings || An array with two fields: the object key, and the full path to the corresponding file. ||

Object metadata (e.g., user-specified headers) are not included in the task
info.  Users can retrieve this by querying Manta directly.
 

## POST /commit

Report that the given key has been successfully processed.  Tasks *must* report
when they finish processing each key.

This information is used for two purposes: first, it is recorded in the job
state so that end users can see what progress is being made while the job runs.
Second, any objects created since the last progress report are committed to the
job.  See `PUT /object/:key`.

When the key has been completed, the task may be terminated immediately,
potentially before this request even completes.  Any cleanup the task wishes to
complete should be done before the final call to this URI.

### Input

|| **Field** || **Type** || **Description** ||
|| key || String || Key for input object successfully completed ||

### Returns

Returns 204 (no content) on success.

### Example

Request:

    {
        "key": "logs/2012-03-11.log"
    }

Response:

    HTTP/1.1 204 No content
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:19:26 GMT
    X-Request-Id: 4bcf467e-4b88-4ab4-b7ab-65fad7464de9
    X-Response-Time: 32


## PUT /object/:key

Writes a new Manta object named "key" associated with this task.  The arguments,
semantics, and return value for this URI exactly match the Manta API for
creating a new object, with the following additions:

* Any object created using this URI since the last call to `POST /commit` will
  be discarded if the job fails.  Such objects will not be included in the
  job's output, but will be listed in the task state for *postmortem*
  debugging.
* Jobs using pipelined reducers may specify a "X-Marlin-Reducer" header whose
  value is an integer from 0 to N - 1, where N is the number of reducers in the
  next phase.  Any other input results in a 409 Conflict error.  This value
  indicates which of several reducers this output file should be fed to.  If
  this value is unspecified and there are multiple reducers in the next phase,
  output keys will be randomly assigned to reducers.

For examples, see the Manta documentation.

## POST /fail

Fail the current task.  Any objects created since the last call to `POST
/commit` will be discarded (but will be linked in the task state for
*postmortem* analysis).

### Input

The input should have at least the following fields:

|| **message** || String || human-readable error message ||

It's also recommended to include a "code" field for a machine-parseable error
code in string form.  Other fields may also be specified up to a maximum
request size.

### Returns

Returns 204 (no content) on success, but may not return at all, as the task may
be terminated immediately.

Callers can assume that this call will never fail.


### Example

Request:

    {
        "message": "corrupt log record"
    }

Response:

    HTTP/1.1 204 No content
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:19:26 GMT
    X-Request-Id: 4bcf467e-4b88-4ab4-b7ab-65fad7464de9
    X-Response-Time: 12

## POST /abort

Exactly like `POST /fail`, except that additional *postmortem* state will be
kept around and linked to the job state.  Core dumps of currently running user
processes and the entire filesystem contents will be preserved.

# Implementing use cases

## Built-in facilities

We'll provide several facilities for dealing with common tasks in MR jobs.

### dpipe: output to the next job pipeline

dpipe [&lt;manta_key&gt;] [reducer]

Reads content from stdin and outputs it from this task.  If &lt;manta\_key&gt;
is not given, a new name will be created.

This is exactly equivalent to:

    $ curl -X PUT -T- http://localhost/object/$manta_key

If "reducer" is specified, it must be an integer from 0 to N - 1, where N is the
number of reducers in the next stage of the job pipeline.  This indicates that
this object should be routed to the given reducer.


### aggr: aggregate rows of numbers

aggr &lt;sum|avg|pctile99...&gt;

Reads a set of "key: value" pairs on stdin, aggregates them key-wise according
to the given function, and emits the result.

Example input:

    $ cat data.txt
    manta: 15
    marlin: 12.8
    manta: 38
    moray: 3

Example output:

    $ aggr sum < data.txt
    manta: 53
    marlin: 12.8
    moray: 3

The "key:" bit may also be omitted.


## Total word count

Input: a bunch of text files

Output: a count of the total number of words in all files

    {
        "jobName": "total word count"
        "inputKeys": [ ... ],
        "phases": [ {
            "exec": "wc -w"
        }, {
            "type": "reduce",
            "exec": "aggr sum"
        } ]
    }

## Specific word count

Input: a bunch of text files and a word "baseball"

Output: the total number of occurences of "baseball" in all files

    {
        "jobName": "'baseball' count"
        "inputKeys": [ ... ],
        "phases": [ {
            "exec": "grep -c baseball"
        }, {
            "type": "reduce",
            "exec": "aggr sum"
        } ]
    }

## Word frequency count (and using a custom asset)

Input: a bunch of files

Output: a text table for the frequency count of each word; e.g.:

    marlin: 527
    manta: 328
    moray: 356

For this case, you'll need a script that computes this for a given text file.
This will do:

    $ cat wordfreq
    #!/usr/bin/perl -W
    
    $counts = {};
    
    while (<>) {
        foreach(split) {
            s/^\W+//;
            s/\W+$//;
            next unless $_;
            $counts->{lc($\_)}++ 
        }
    }
    
    while (($word, $count) = each(%$counts)) {
        printf("%s: %d\n", $word, $count);
    }

You'll first need to PUT this to manta, say at /dap/bin/wordfreq.  Then the job
looks like this:

    {
        "jobName": "word frequency count"
        "inputKeys": [ ... ],
        "phases": [ {
            "assets": [ "/dap/bin/wordfreq" ],
            "exec": "/dap/bin/wordfreq"
        }, {
            "type": "reduce",
            "exec": "aggr sum",
        } ]
    }

If you wanted, you could rewrite "wordfreq" as an awk one-liner and include it
directly in the "exec" property.

## Pipelined reducers

Sometimes it's necessary to pipeline the reduce phase so that instead of
processing all input keys in one pass, the input keys are processed in chunks
across multiple passes.  For example, you may want to process half of the input
keys in each of two parallel tasks, and then process the output of that, to
avoid having to load the entire data set in memory at once.

We already support multiple "reduce" phases in a job, but to support this you
also need to parallelize a single reduce phase.  You can do this with the
"count" property:

    "phases": [
        ... /* any number of map phases */
        , {
            "type": "reduce",
            "exec": ...,
            "count": 2
        }, {
            "type": "reduce",
            "exec": ...
        }
    ]

In this example, some number of map phases are followed by a phase with two
reducers running in parallel, followed by a final reduce stage.  The two
parallel reducers are identical except for their input and output keys.  By
default, input keys are randomly assigned to one of the reducers.  If a more
sophisticated assignment is necessary, the previous map phase may specify to
which reducer a given output object will be assigned using the X-Marlin-Reducer
header, which must be an integer between 0 and N - 1, where N is the total
number of reducers in the next phase.


## Node, Python, Ruby, etc. interface

The Jobs API is built in terms of Unix shell commands for generality.  This
makes it easy to run simple jobs like "grep" and "wc".  For slightly more
complex cases, custom utilities can be built in the Unix style and composed with
existing system utilities without writing significant new code.  For more
complex cases you want entirely custom code, you can simply run your program via
the "exec" string.

We'll also provide a higher-level interface for Node.js, so that people that
want to build jobs with custom code can use Node easily.  Likely we'll have
consumers implement an interface like:

    function processObject(file, callback) { ... }

and we'll provide convenience routines for API operations (like "taskParams()",
"commit()", "abort()", and so on).

Without fleshing this out completely, we know such an interface can be
implemented without changes to the Jobs API.  When people want to use the Node
API, we tell them to write phases like this:

    "phases": [ {
        "assets": [ "my/npm/package" ],
        "exec": "mnode /assets/my/npm/package"
    } ]

"mnode" is an executable Node.js program included with the dataset that installs
the given npm package (and therefore its dependencies, too) and then invokes our
Marlin interface in that package.

If we care to, we can do something similar for Python, Ruby, R, or any other
environment.  It would be just as easy for third parties to build their own.
Users would only need to reference a third party asset:

    "phases": [ {
        "assets": [
            "third-party/framework/start.sh"
            "my/script"
        ],
        "exec": "/assets/third-party/framework/start.sh /assets/my/script" 
    } ]

## Log analysis (MapReduce)

TBD.

## Image conversion / video transcoding (Straight Map)

This job uses the "convert" command that's part of the ImageMagick suite.  It
converts image files between types based on file extension.

    {
        "jobName": "convert GIFs to PNGs",
        "inputKeys": [ "file1.gif", "file2.gif", ... ]
        "phases": [ {
            "exec": "output=$(basename $mc_input_file .gif).png; convert $mc_input_file $output && dpipe < $output"
        }
    }


## Node interface

TBD.

## Hadoop proper

TBD.

## Customer records, join

TBD.

## Build system

TBD.


# Marlin implementation

## Marlin network architecture

Marlin makes use of two generic Manta components:
* muskie, the Manta web tier, which handles the Jobs API (requests to POST new
  jobs and GET job state).
* moray, where all Marlin state is stored.

On top of these, Marlin adds two components:

* Job worker tier, which locates new and abandoned jobs in Moray, assigns task
  groups to individual storage and compute nodes, and monitors job progress.
* Compute node agents, which execute and monitor tasks and report progress and
  completion.

There's a cluster of web tier workers and job workers in each datacenter, but
there's no real DC-awareness among these components.

**Job state**

To keep the service resilient to partitions and individual node failures, **all
job state is stored in Moray** (i.e., all components are essentially
stateless), and **all communication happens via Moray**.  Rather than talking
to each other directly, each component drops state changes into Moray and polls
for whatever other state changes it needs to know about.  This model makes it
relatively easy to deal with individual component failure and to scale
horizontally.  See "Invariants and failure modes" below for details.

Recall that Moray spans datacenters, is eventually consistent, and remains
available in the case of minority DC partitions.  Using Moray in this way also
implies that Moray becoming unavailable makes Marlin unavailable, and that
Moray must be able to keep up with the Marlin workload.  See "Scalability"
below.

Jobs are stored inside a single Moray bucket called "marlinJobs", with indexed
fields that allow the web tier, job workers, and agents to poll on the subset of
jobs that they care about.


**Web tier**

muskie retrieves current job state by querying "marlinJobs" for the job with the
given jobId.

When a new job is submitted, muskie allocates a new jobId and drops the job
definition into "marlinJobs".


**Job workers**

To synchronize changes to job state, each job is assigned to one worker at a
time.  This assignment is stored in the job object in Moray.  An assignment is
valid as long as the job's modification time (mtime) is within the last
WORKER\_TIMEOUT seconds.

This works as follows: each job worker periodically queries Moray for jobs
whose state is not "done" and either (a) the "worker" field is undefined or (b)
the modification time is more than WORKER\_TIMEOUT seconds ago.  (This is a
relatively simple query on indexed fields.)  For each job the worker finds, it
uses a test-and-set operation to assign the job to itself.  In this way,
workers pick up both new jobs and jobs whose assigned workers appear to have
failed.  A worker must update each job every WORKER\_TIMEOUT seconds, even if
there's no meaningful progress to report, to indicate that the worker is still
handling that job.  If the worker sees that WORKER\_TIMEOUT seconds has elapsed
since it last updated the job, it must discard any job state it has and stop
working on the job to avoid conflicting with another worker who picks up the
job.  (In practice, the best way to implement this may simply be to have all
state updates use a test-and-set that checks whether worker == me.)

When a worker picks up a newly submitted job, the first thing it has to do is
query Moray to find out where all the input keys are.  Moray itself has many
independent partitions and uses a consistent hash ring to map a given key to
the Moray partition that holds the metadata for that key.  The worker takes all
the input keys, executes the consistent hash function to group them by Moray
partition, and performs a batch request to each partition to determine which
storage nodes each key is actually stored on.

The worker than divides the first phase of the job into **task groups**, each of
which will run on one storage node and process whichever input keys are stored
on that node.  These task assignments are written out to a Moray bucket called
"marlinTaskGroups" with a field identifying the hostname of the node on which
the task will run.  The agents running on each node poll Moray for tasks
assigned to them, process each one, and update the task object.  The job worker
polls on these task updates, updates the global job state with job progress (so
the web tier can see it) and advances the job when each phase completes.

When a phase completes (as determined by the worker when all of its tasks have
completed), the worker begins processing the next phase in the same way it did
the first one: by locating the objects within Manta and then assigning tasks to
individual nodes.  When all phases complete, the job is marked done.

**Compute node agents**

As described above, compute node agents monitor tasks assigned to them in
"marlinTaskGroups".  As their tasks make progress, they update the task record,
which is also monitored by a job worker.

**Invariants and failure modes**

Invariants:

* All state is stored in Moray, so all components are stateless.
* At most one worker can ever "own" a job.  Only the owner of a job can write
  task assignments or update the job record in "marlinJobs".
* To a first approximation, a component is partitioned iff it cannot reach
  Moray.  This is an oversimplification because Moray itself is sharded, so it's
  theoretically possible to be partitioned from one shard but not another.
  This case is very unlikely, since each shard has a presence in each DC, but
  the system will handle this.  Most importantly, the only partitioning that
  matters is that between a component and a given Moray shard.

If a user's code fails within a task (e.g., dumps core), and the on-node retry
policy has been exhausted, the compute agent reports the task as failed, and
that will become visible in the final job state.  It's the user's problem at
that point.

If a compute node agent fails transiently (e.g., dumps core and restarts), it
should be able to pick up where it left off.  This should be invisible to the
user's code and the rest of the system, modulo slightly increased task
completion time.  (The agent must synchronously write state to local disk to
facilitate this.  If this storage fails, that's equivalent to the compute node
agent failing persistently.)

If a compute node agent either fails for an extended period *or* becomes
partitioned from its Moray shard (which are indistinguishable to the outside
world), the job worker redispatches any uncompleted work to other nodes that
have the same data.  It does not cancel the task on the failed node.  If both
tasks eventually complete successfully, the worker will necessarily see one's
results first and discard the other's.  (That's why the state in
"marlinTaskGroups" is not authoritative job state, and why the worker must
"commit" this state to the job record in "marlinJobs" for the web tier to see
it, rather than having the web tier scan "marlinTaskGroups" for itself.)

If a job worker fails transiently, it should be able to pick up where it left
off, since all state is stored in Moray.  This should be invisible to the rest
of the system, modulo slightly increased job completion time.

If a job worker fails for an extended period, another job worker will pick up
its jobs.  This becomes exactly like the case where a worker fails transiently:
the state in Moray is sufficient for the new worker to pick up where the old one
left off.

If a job worker becomes partitioned from one or more Moray shards, then it will
be unable to process task updates from compute nodes reporting to those shards.
It may end up trying to redispatch tasks assigned to those nodes to other nodes.
If it can talk to enough shards to complete the phase, the phase will complete;
otherwise, it will stall until connectivity is restored, at which point the
worker will see the completed task state and finish the phase.

If a job worker becomes partitioned from the Moray shard storing the job state
for WORKER\_TIMEOUT seconds, this will be treated as an extended worker failure.
Another worker will pick up the job where the first one left off.  The first
worker won't clobber the job state after someone else has picked up the job
because it also knows that WORKER\_TIMEOUT has elapsed.

Web tier nodes are completely stateless.  Any failure results in failing any
in-flight requests.  Persistent failures have no additional impact as long as
enough nodes are in service to handle load.  Partitions between the web node and
the Moray shard storing job records would result in failed requests.

If an entire datacenter becomes partitioned from the rest, then its web tier
nodes certainly won't be able to service any requests.  All of its job workers
become partitioned (see above -- the work should eventually be picked up by
workers in other datacenters).  All of its compute node agents effectively also
become partitioned, and their work *may* be duplicated in other datacenters.
Importantly, jobs submitted to the majority partition should always be able to
complete successfully in this state, since there's always enough copies of the
data to process the job.

**Scalability**

Being stateless, web tier nodes can be scaled arbitrarily horizontally.  If load
exceeds capacity, requests will queue up and potentially eventually fail, but
alarms should ring long before that happens.

Similarly, job workers can be scaled arbitrarily horizontally, and capacity
monitoring should allow administrators to ensure that sufficient capacity is
always available.  The failure mode for excess load is simply increased job
completion time.  Task updates may accumulate in Moray faster than the worker
can process them, but the total space used in Moray is the same regardless of
how fast the job workers process them, so there's no cascading failure.  If job
workers cannot update their locks in Moray, the system may thrash as workers try
to take each others' jobs, but this would be very extreme.

Compute nodes rely on the OS to properly balance the workload within that box.
They may queue tasks after some very large number of concurrent tasks, but it's
not anticipated that we would ever reach that point in practice.  If this
became a problem, job workers could conceivably redispatch the task to another
node with the same data available.  If this truly became a serious problem, we
could support compute agents that download data, rather than operating locally
(just as reducers already do).

Given that there's enough capacity in each of the Marlin tiers, the remaining
question is Moray.  Moray will be sharded, and we will say that each compute
node reports its tasks to exactly one (uniformly distributed) shard.  All job
state will be stored on a single shard, since the number of updates is
relatively minimal.  We should be able to reshard to scale this out.

Additionally, each compute node can batch its Moray updates from all tasks for
up to N seconds, sharply reducing the number of requests required.  Similarly,
job workers can batch job state updates for many jobs up to N seconds.  Batching
of "location" GET requests is discussed above, under "job workers".  Thus, the
total number of Moray-wide requests should be boundable to something like (# of
workers \* # of compute zone agents) / (N seconds), where N is how frequently
each agent updates Moray.  These requests should mostly be distributed across
all shards.

Batching requests for the same record over multiple seconds reduces both network
throughput and database operations, as multiple state changes can be combined at
the source.

Batching requests over multiple records helps network throughput, but not the
total number of database operations, since each record must be operated on
separately.  The total number of database operations could still be around (# of
active tasks \* # of agents \* # of active jobs \* # of job workers).  Again,
these are distributed across many Moray shards, which can be scaled
horizontally.

## Objects

There are two types of objects stored in Moray: jobs and task group
assignments.  For the details, see the schema in lib/schema.js in marlin.git.

Web tier:

* POST /jobs (create new job)
* GET /jobs/jobId (get job status)
* PUT /jobs/jobId (rare case: use T&amp;S to set cancelled = true)

Workers:

* GET /jobs where worker == null or mtime &lt; ... (pick up new jobs)
* PUT /jobs/jobId (update job state)
* PUT /taskgroups/taskGroupId (create/update assignment)
* GET /taskgroups/taskGroupId (check task group state)

Agents:

* GET /taskgroups where host == me (pick up new task groups)
* GET /taskgroups/taskGroupId (check task group state)
* PUT /taskgroups/taskGroupId (post task group results)


## Cancellation

To cancel a job, the web tier writes "cancelled: true" to the job record.  At
this point, we could say that the job state is immutable and all future state
requests only show whatever state has been completed up to this point.

Asynchronously, the job worker will need to be polling on the job state
occasionally to notice the cancelled flag.  It propagates this to individual
task assignments, where the compute node agents will notice that the job has
been cancelled and abort them.  Eventually, all work on the job will complete,
though this may take some time to propagate through the system.


## CN agent

As described above, compute node agents receive task assignments via Moray and
report progress back to Moray.  Such progress includes keys processed and state
changes, including that a task is queued, loaded, running, timed out, failed, or
done.)

The agent maintains a pool of compute zones that are either "uninitialized",
"ready", or "busy".  Uninitialized zones are those currently in an arbitrary
state.  Those are asynchronously transitioned to "ready" by the following steps:

1. Halt the zone, if it's not already halted.
2. "zfs rollback" to the zone's origin snapshot
3. Boot the zone.
4. Sets up an agent inside the zone that listens for local HTTP requests to the
   Task Control API and forwards them via a zsock server to the CN agent.

When a new task is dispatched, if there are no "ready" zones available, tasks
are enqueued for later.  Otherwise, the agent picks a "ready" zone and does the
following:

1. Downloads the job's bundle's assets into the zone.
2. Creates a hyprlofs mount for the files.
3. Finally, invokes the bundle's exec script.

When the job completes, successfully or otherwise, the CN agent reclaims the
zone by marking it "uninitialized" again and asynchronously transitioning it to
"ready" again to be used for another task.

To implement the Task Control API:

`/info` is simple: it just retrieves data from the agent's persistent state,
which already has to exist to manage the job.

`/objects/:key` makes a note in the task state that this object is being
written, and then (first approach) proxies the request directly to manta. But
see "open questions" below.

`/commit` updates the persistent state for the task to indicate that all
objects written since the last checkpoing have been committed.

`/fail` also updates the persistent state, but indicates that new objects should
be discarded.  They won't actually be deleted, but marked as discarded in the
job state so users can look at them later.  After that, this request terminates
the zone.

`/abort` is like `/fail`, except that before terminating it gcores all processes
in the user's contract and does an incremental "zfs send" of the zone to a new
manta file.  (We'll want some automated mechanism to reconstruct the user's zone
based on this, presumably by "zfs receive" the dataset and then "zfs receive -i"
the stream associated with this task.)

In order to survive transient failure, the agent must store state on disk.  The
current plan is to use sqlite for this because it already has usable Node
bindings.

## Open questions

Roughly in order of importance:

* When user code wants to write files to manta, how exactly does that work? Do
  we have a way to tell Manta to write one copy on the local box, and if so, do
  we avoid sending it across the network twice?
* How do we monitor the user code's contract? Presumably we'll need an agent
  in the zone that bootstraps their code and watches the contract (but then we
  need to figure out what to do if that guy dies -- adopt the contract?)
* What metrics do we want to provide via CA?  How will we implement these?
  What other changes must be made to CA to support Marlin and Manta?
* Should the job log be a manta object so that user's can add their own entries?

## Management API?

cancel tasks, take compute zones and servers in and out of service
initial setup: creating compute zones
tuning # of compute zones

## Rough roadmap

The basic idea is to get a prototype up and running relatively quickly and
build that out as needed.  This is a *very* rough draft of how to get there.

Today we have:

* a Moray implementation that supports GET/DEL/PUT (with test-and-set) and
  search, as well as a client-side library that supports throttling.
* a mock Moray implementation, at least for testing the job worker
* an agent that takes task group specifications and executes each task in the
  group, including support for assets and both "generic" and "map" tasks, using
  hyprlofs to map files in.
* a job worker that picks up unassigned jobs in Moray, executes them, and
  can pick up partially-completed jobs.  Also times out idle task groups.
* a very simple web tier that just drops jobs into Moray and reads state back.

Remaining work not necessarily required for the end-to-end demo includes:

* moray: search by bucket change number
* moray client: support for consistent hashing
* agent: automated tests
* agent: implement "abort" (which saves all zone state)
* agent/worker: implement explicit fail
* agent/worker: implement retry
* all: Moray record versioning
* all: implement cancel
* all: implement tools for observing global system state

plus work covered by existing MANTA tickets.
