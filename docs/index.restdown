---
title: Marlin
markdown2extras: wiki-tables, code-friendly
apisections: Task Control API
---

# Marlin

Marlin is a facility for running compute jobs on Manta storage nodes.  This
document describes the desired end state for version 1.

**For an overview of what Marlin is and how users interact with it, see the
Manta documentation in muskie.git.  Really, go read that first.  This document
assumes familiarity with that, including how the rest of the Manta object store
works.**


# First principles

Marlin provides two main functions:

* a non-interactive Unix shell environment for doing "work" on Manta objects as
  local files
* a framework for distributing such work to the right physical servers, tracking
  which pieces are complete, capturing the output, and repeating the whole
  process to facilitate multi-phase computation on objects at rest

Example use cases include:

* log processing: grepping through logs, reformatting log entries, or generating
  summary reports
* image processing: converting formats, generating thumbnails
* video processing: transcoding, extracting segments, resizing
* "hardcore" data analysis, using NumPy, SciPy, R, or the like
* SQL-like queries over structured data (similar to what Hive provides for
  Hadoop)

Since the primitive abstraction is the Unix shell, lots of these operations can
be done with built-in tools like `awk`, `grep`, `sort`, `json`, `bunyan`, and so
on.  We also provide a rich suite of software like ImageMagic, ffmpeg, and so
on, as well as runtime environments like Perl, Python, Java, R, and so on so
that users can run their own software, too.

Besides that, using Unix as the basic abstraction makes it possible to develop
software locally and then run it in Marlin with little to no changes required.

With rich access controls in Manta, it will be possible to run compute on other
users' data that's been made available to you.  This may be especially useful
for government organizations with lots of public data (e.g.,
http://community.topcoder.com/pds-challenge/).  In version 1, the only access
controls available in Manta are "private to my account" and "public to the
internet".

Marlin must be easy to use, monitor, and debug.  As a benchmark, "Word frequency
count", the MR equivalent of "hello world", should be trivial to run from the
command line.


# Examples

There are lots more examples in the public (muskie) documentation.

## XXX: Finding active customers (with a join)

## XXX: Hadoop

## Build system

Marlin could be an effective distributed build system, at least for Joyent
internally.  Using Marlin this way guarantees that each repo is built in a clean
zone with whichever dataset it requires, the builds themselves are automatically
distributed for parallelism, and the artifacts end up in Manta where we probably
want them anyway.

The main challenge is that git repos themselves would be hard to store in Manta,
since they're typically a whole tree of files presumably updated with partial
writes.  But git repos can be bundled into a single file using "git bundle", and
the resulting bundle stored in Manta.  If we add a post-commit hook to our git
repos that save a "git bundle" into Manta, then the distributed build system is
a pretty simple map job:

    "phases": [ {
        "dataset": "smartos-1.8.4",
        "exec": "git clone $mr_input_file repo && cd repo && make &&
            mpipe < build/dist/latest.tar.bz2"
    } ]

The input objects would be the git bundle for each repo in Manta.  If different
repos needed to be built with different datasets, those would have to be
separate jobs.


# Marlin implementation

The following sections are somewhat out-of-date and subject to change.  When in
doubt, check the source.


## APIs

End users manage compute jobs through the "jobs" entry points under the main
Manta API.  See the Manta API documentation for details.

Internally, Marlin worker processes (which are responsible for managing the
distributed execution of jobs) communicate with agents (which actually execute
user commands) through small JSON records stored in Moray, the Manta metadata
tier.  This mechanism is documented in the worker source code.

On each physical server where jobs are executed, the Marlin agent communicates
with a lackey running inside each compute zone using a private HTTP API which is
documented in the agent source code.  This API is a superset of the public Manta
API, allowing allowing users to securely perform the usual Manta operations from
within jobs.  The non-public entry points in this API are private implementation
details, though future versions may stabilize these for consumption by
higher-level frameworks.

All DateTimes in all APIs use W3C DTF in UTC ("YYYY-MM-DDThh:mm:ss.sZ").


## Network architecture

Marlin makes use of two generic Manta components:

* muskie, the Manta web tier, which handles the Jobs API (requests to POST new
  jobs and GET job state).
* moray, where all Marlin state is stored.

On top of these, Marlin adds two components:

* Job worker tier, which locates new and abandoned jobs in Moray, assigns task
  groups to individual storage and compute nodes, and monitors job progress.
* Compute node agents, which execute and monitor tasks and report progress and
  completion.

There's a cluster of web tier workers and job workers in each datacenter, but
there's no real DC-awareness among these components.

**Job state**

To keep the service resilient to partitions and individual node failures, **all
job state is stored in Moray** (i.e., all components are essentially
stateless), and **all communication happens via Moray**.  Rather than talking
to each other directly, each component drops state changes into Moray and polls
for whatever other state changes it needs to know about.  This model makes it
relatively easy to deal with individual component failure and to scale
horizontally.  See "Invariants and failure modes" below for details.

Recall that Moray spans datacenters, is eventually consistent, and remains
available in the case of minority DC partitions.  Using Moray in this way also
implies that Moray becoming unavailable makes Marlin unavailable, and that
Moray must be able to keep up with the Marlin workload.  See "Scalability"
below.

Job state is stored across seeral Moray buckets with indexed fields that allow
the web tier, job workers, and agents to poll on the subset of jobs and job
state that they care about.  For details on these buckets and the records
contained therein, see the documentation in lib/schema.js.  A design constraint
is that the size of every JSON record in the system must be bounded (and should
be fairly small).  This applies to records received from and sent back to users
as well as records used internally.


**Web tier**

muskie retrieves current job state by querying "marlinJobs" for the job with the
given jobId.  When a new job is submitted, muskie drops the job definition into
the jobs bucket.


**Job workers**

To synchronize changes to job state, each job is assigned to one worker at a
time.  This assignment is stored in the job object in Moray.  An assignment is
valid as long as the job's modification time (mtime) is within the last
WORKER\_TIMEOUT seconds.  Workers save their jobs periodically to indicate
liveness.  They also poll periodically for unassigned jobs (to pick them up) and
jobs that have not been updated within the timeout period (to handle worker
failure).

Managing the streaming, distributed execution of jobs is a complex process.  The
basic idea is that workers query Moray for state changes needing attention (new
jobs, new job inputs, and completed tasks) and process them by issuing new
tasks, updating job state, and so on.  Each input is processed by locating the
corresponding object and writing out either a task assignment (for map tasks) or
a taskinput record (for reduce tasks).  For details, see the worker source.

**Compute node agents**

Like workers, the per-storage-node agents periodically poll Moray for state
changes needing attention (newly assigned tasks, cancelled tasks, and so on) and
process them by scheduling execution.  Agents are responsible for assigning
tasks to zones, managing the lifecycle of those zones, keeping track of output,
and proxying front-door requests between zones and Manta.  As tasks complete
(successfully or otherwise), the agent writes new updates to Moray that will be
processed by the worker.


**Analysis of distributed system failure modes**

Invariants:

* All state is stored in Moray, so all components are stateless.
* At most one worker can ever "own" a job.  Only the owner of a job can write
  task assignments or update the job record in "marlinJobs".
* To a first approximation, a component is partitioned iff it cannot reach
  Moray.  This is an oversimplification because Moray itself is sharded, so it's
  theoretically possible to be partitioned from one shard but not another.
  This case is very unlikely, since each shard has a presence in each DC, but
  the system will handle this.  Most importantly, the only partitioning that
  matters is that between a component and a given Moray shard.

If a user's code fails within a task (e.g., dumps core), and the on-node retry
policy has been exhausted, the lackey reports the task as failed, and that will
become visible in the final job state.  It's the user's problem at that point.

If a compute node agent fails transiently (e.g., dumps core and restarts), fails
for an extended period of time, or becomes partitioned from its Moray shard, any
running tasks are aborted by the worker.  The worker will retry aborted tasks up
to once.  This retry should be invisible to the user's code and the rest of the
system, modulo increased task completion time.  Agents ignore tasks dispatched
before they began running, which covers the case where the agent actually
crashes.  To cover the case where the agent simply becomes partitioned, the
worker explicitly cancels tasks that it has retried elsewhere.

If a job worker fails transiently, it can pick up exactly where it left off,
since all state is stored in Moray.  This should be invisible to the rest of the
system, modulo slightly increased job completion time.  If a job worker fails
for an extended period, another job worker will pick up its jobs.  This becomes
exactly like the case where a worker fails transiently: the state in Moray is
sufficient for the new worker to pick up where the old one left off.

If a job worker becomes partitioned from one or more Moray shards, it will stop
processing updates (since it won't see them).  After the job timeout elapses,
the worker will stop trying to save the record and another worker will pick up
the job.  This looks just like a worker failure.

Web tier nodes are completely stateless.  Any failure results in failing any
in-flight requests.  Persistent failures have no additional impact as long as
enough nodes are in service to handle load.  Partitions between the web node and
the Moray shard storing job records would result in failed requests.

If an entire datacenter becomes partitioned from the rest, then its web tier
nodes certainly won't be able to service any requests.  All of its job workers
become partitioned (see above -- the work should eventually be picked up by
workers in other datacenters).  All of its compute node agents effectively also
become partitioned, and their work *may* be duplicated in other datacenters.
Importantly, jobs submitted to the majority partition should always be able to
complete successfully in this state, since there's always enough copies of the
data to process the job.

**Scalability analysis**

Being stateless, web tier nodes can be scaled arbitrarily horizontally.  If load
exceeds capacity, requests will queue up and potentially eventually fail, but
alarms should ring long before that happens.

Similarly, job workers can be scaled arbitrarily horizontally, and capacity
monitoring should allow administrators to ensure that sufficient capacity is
always available.  The failure mode for excess load is increased job completion
time.  Task updates may accumulate in Moray faster than the worker can process
them, but the total space used in Moray is the same regardless of how fast the
job workers process them, so there's no cascading failure.  If job workers
cannot update their locks in Moray, the system may thrash as workers try to take
each others' jobs, but this would be very extreme.

Compute nodes rely on the OS to properly balance the workload within that box.
They may queue tasks after some very large number of concurrent tasks, but it's
not anticipated that we would ever reach that point in practice.  If this
becomes a problem, job workers could conceivably redispatch the task to another
node with the same data available.  If this truly became a serious problem, we
could support compute agents that download data, rather than operating locally
(just as reducers already do).

Given that there's enough capacity in each of the Marlin tiers, the remaining
question is Moray.  Moray will be sharded, and we will say that each compute
node reports its tasks to exactly one (uniformly distributed) shard.  All job
state will be stored on a single shard, since the number of updates is
relatively minimal.  We should be able to reshard to scale this out.

***Future ideas***

Each compute node can batch its Moray updates from all tasks for up to N
seconds, sharply reducing the number of requests required.  Similarly, job
workers can batch job state updates for many jobs up to N seconds.  Batching of
"location" GET requests is discussed above, under "job workers".  Thus, the
total number of Moray-wide requests should be boundable to something like (# of
workers \* # of compute zone lackeys) / (N seconds), where N is how frequently
each agent updates Moray.  These requests should mostly be distributed across
all shards.

Batching requests for the same record over multiple seconds reduces both network
throughput and database operations, as multiple state changes can be combined at
the source.

Batching requests over multiple records helps network throughput, but not the
total number of database operations, since each record must be operated on
separately.  The total number of database operations could still be around (# of
active tasks \* # of agents \* # of active jobs \* # of job workers).  Again,
these are distributed across many Moray shards, which can be scaled
horizontally.


## Cancellation

To cancel a job, the web tier writes an update to the job record.
Asynchronously, the job worker will need to be polling on the job state
occasionally to notice the cancelled flag.  It propagates this to individual
task assignments, where the compute node agents will notice that the job has
been cancelled and abort them.  Eventually, all work on the job will complete,
though this may take some time to propagate through the system.


# Future work

* flesh out maggr
* apache2json?
* Crazy idea #1: eliminate the worker by writing a library to lookup input
  objects and dispatch them to the next phase.  Have muskie do this for phase 0,
  and agents do this for subsequent phases.  (Should this be a service instead
  of a library?)
* Crazy idea #2: make things pushed based for the common case, resorting to
  polling only as a catch-all.  Once polling is cheap, eliminate "state" for
  jobs entirely -- there's just (have all inputs submitted so far been
  processed?)

## Node, Python, Ruby, etc. framework

The Jobs API is built in terms of Unix shell commands for generality.  This
makes it easy to run simple jobs like "grep" and "wc".  For slightly more
complex cases, custom utilities can be built in the Unix style and composed with
existing system utilities without writing significant new code.  For more
complex cases you want entirely custom code, you can simply run your program via
the "exec" string.

We may also provide a higher-level interface for Node.js, so that people that
want to build jobs with custom code can use Node easily.  We could have
consumers implement an interface like:

    function processObject(file, callback) { ... }

and we'd provide convenience routines for API operations (like "taskParams()",
"commit()", "abort()", and so on).

Without fleshing this out completely, we know such an interface can be
implemented without changes to the Jobs API.  When people want to use the Node
API, we tell them to write phases like this:

    "phases": [ {
        "assets": [ "my/npm/package" ],
        "exec": "mnode /assets/my/npm/package"
    } ]

"mnode" is an executable Node.js program included with the zone that installs
the given npm package (and therefore its dependencies, too) and then invokes our
Marlin interface in that package.

If we care to, we can do something similar for Python, Ruby, R, or any other
environment.  It would be just as easy for third parties to build their own.
Users would only need to reference a third party asset:

    "phases": [ {
        "assets": [
            "/third-party/public/framework/start.sh"
            "/dap/stor/script"
        ],
        "exec": "/assets/third-party/public/framework/start.sh /assets/dap/stor/script"
    } ]
