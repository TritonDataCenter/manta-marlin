---
title: Marlin
markdown2extras: wiki-tables, code-friendly
apisections: Job API, Task Control API
---

# Marlin

Marlin is a facility for running compute jobs on Manta storage nodes.  This
document describes the desired end state for version 1.


# Design goals

Marlin facilitates the following use cases:

* MapReduce in a generic environment (e.g., log analysis)
* MapReduce in a node-based environment (i.e., make it particularly easy for
  node apps)
* Straight Map with no reduce (e.g., video transcoding)
* Allowing others to run compute on your data (e.g., for government
  organizations with lots of public data)
* Hadoop itself?

Of course, we want this to be easy to use, monitor, and debug.  A good
barometer is that word count, the MR equivalent of "hello world", should be
trivial to write.  

Marlin defines two public APIs:

* Job API: allows end users to submit, manage, and monitor compute jobs
* Task Control API: used by user code running inside Marlin to coordinate
  execution and report progress.

All DateTimes in these APIs use W3C DTF in UTC ("YYYY-MM-DDThh:mm:ss.sZ").

This document assumes familiarity with the Manta object store.


# Overview by example

To explain Marlin, we'll outline how a user would implement distributed word
count algorithm.  This is a distributed version of "wc -w".  On a single
system, that might look like this:

    $ wc -w /usr/share/dict/*
          69 /usr/share/dict/README
         150 /usr/share/dict/connectives
        1323 /usr/share/dict/propernames
      234936 /usr/share/dict/web2
      121847 /usr/share/dict/web2a
      234936 /usr/share/dict/words
      593261 total

The idea of the distributed version is that you'd like to do something like:

    $ wc -w file1 file2 ...

where file1, file2, ... may be stored on completely different systems, and get
back summary output:

    593261

Let's assume the input files are already stored in Manta:

* file1, file2, file4 on storage node S1
* file3, file5 on storage node S2

though the user doesn't know which files are on which nodes.

To execute the word count, the user submits a Marlin **job** that includes the
following information:

    "jobName": "distributed word count example",
    "inputKeys": [ "file1", "file2", "file3", "file4", "file5" ],
    "phases": [ {
        "exec": "wc -w $mc_input_keys | mpipe",
    }, {
        "type": "reduce",
        "exec": "awk '{sum += $1} END{ print sum; } $mc_input_keys' | mpipe"
    } ]

"jobName" and "inputKeys" are pretty self-explanatory.  "phases" describes what
to actually do with the data.

Because the first phase operates on all five files, but these are stored on
different storage nodes, Marlin splits this phase into two **tasks**, one for
each node containing input files.  The first task runs runs "wc -w file1
file2 file4 | mpipe" on storage node S1, and the other task runs "wc -w file3
file5 | mpipe" on node S2.  "mpipe" is a built-in command that reads data on
stdin, buffers it until EOF, and outputs a uniquely named temporary object on
Manta as the output of the current task.  So by piping the "wc" output to
"mpipe", we make it the output of this task, and that output gets stored on
Manta.

The second phase is a **reduce phase** that takes the outputs from the previous
phases (the temporary objects created by "mpipe") and runs the given awk script,
producing a single total that's the sum of all words in all files.  Since
there's only one of these and it combines data from many different nodes, it
runs on general-purpose compute nodes, not Manta storage nodes.  Using "mpipe"
again, the output of this phase is saved as a new Manta object which becomes the
output of the job.

With this example we've covered:

* *jobs*, which users submit to do work with data
* *phases*, which describe individual processing steps of a job
* *map phases*, which run on individual nodes where input objects are stored and
  process one object at a time
* *reduce phases*, which run on separate compute nodes and combine outputs from
  previous phases
* *tasks*, which are individual pieces of a map or reduce phase
* *compute zones*, which are the environments in which tasks run


# Job API

The Job API is a public-facing API that sits alongside the public Manta endpoint
and lets end users submit, manage, and monitor compute jobs.


## POST /jobs

Submit a new job to be run as soon as possible under Marlin.

Each job defines a set of input Manta objects to process and a set of
**phases**, each describing what to do with each of the input objects.  Each
phase may output any number of manta objects for input to the next phase.  The
output from the last phase is the output of the job.

Each phase of the job is divided into discrete tasks that run one piece of the
phase.  While the job is running, users can retrieve the status of the job to
see which tasks have completed, which tasks are running, which keys have been
processed, and which keys are left.

When all tasks complete, the job itself completes.  Failed tasks are not
retried (though retry options may be provided in a future version of this API).
By default, if a task fails, other tasks are unaffected.  The system continues
running tasks until all of them either fail or complete in order to make as
much progress as possible.  This behavior can be controlled with the "failFast"
job option.


### Inputs

|| **Field** || **Type** || **Description** ||
|| jobName || String || Human-readable job label (not necessarily unique). ||
|| phases || Array&nbsp;of&nbsp;Objects || Array of compute phases.  See below.  ||
|| phases[i].type (optional) || String: "map"&nbsp;or&nbsp;"reduce" || Map phases (the default) process individual objects and are executed on the physical system where those objects are stored.  Reduce phases process many objects at once and are generally executed on separate compute nodes. ||
|| phases[i].count (optional) || Integer || For reduce jobs only, indicates how many reducers to use. See "Pipelined Reducers" below. ||
|| phases[i].assets (optional) || Array of Strings || List of Manta objects that will be made available in the compute zone.  These may be scripts, binaries, libraries, or other objects used to actually run the job, not inputs to the job. ||
|| phases[i].exec || String || Shell command to launch the task.  This are passed directly to "bash -c". ||
|| phases[i].args (optional) || Object || User-defined task arguments.  This is just an arbitrary JSON object that will be available to each task.  Many jobs won't need this because they use shell arguments directly in the "exec" line, but complex tasks may find it easier to process a structured JSON argument. ||
|| phases[i].datasetVersion (optional) || String || Version of compute zone dataset to use.  By default, the latest stable dataset is used. ||
|| inputKeys || Array&nbsp;of&nbsp;Strings || List of Manta objects to process. ||
|| failFast (optional) || Boolean || Cancel all tasks when any task fails.  Otherwise, tasks continue running even after one of them fails (in attempt to make as much progress as possible). ||

Jobs can have any number of map or reduce phases.  For examples:

* N &gt; 1 map phases, 0 reduce phases: file conversion (e.g., video
  transcoding, image rendering)
* N &gt; 1 map phases, 1 reduce phase: traditional map-reduce (e.g., word count)
* N &gt; 1 map phases, M &gt; 1 reduce phases: MR with pipelined reducers
* 0 map phases, N reduce phases: in-place compute

To process a map phase, the system divides the phase into tasks, where each task
runs a piece of the overall job on a Manta node where some of the input objects
are stored.  Reduce phases are also divided into tasks, with one task per
reducer.  (But most jobs will have at most 1 reducer.)

Both map and reduce tasks must report when they have succeeded or failed, and
should also report progress as they go.  The system may time out map tasks that
are taking too long.

You specify the code that runs in each phase using the "assets", "exec", and
"args" properties.  Any Manta keys listed in "phases[i].assets" will be made
available under "/assets" in the compute zone before the task is started.  This
can be used to distribute applications and libraries that will actually process
the input to each compute zone.  "assets" should *not* be used to specify input
keys.  Use "inputKeys" for that.

The task is started by invoking "phases[i].exec".  This may be any shell-style
command, including environment variables, shell arguments, and pipelines.
The shell variable "$mc_input_files" may be used to refer to input key files.

Additional structured arguments may be stored in "args", which can be retrieved
by the task's code using the Task Control API.

### Returns

On success, returns 201 with the URI for the job in a Location header and the
state of the job as returned by `GET /jobs/:jobId`.

### Examples

Here's an example job that uses a script stored on Manta for the body of the
task:

    {
        "jobName": "monthly log summary",
        "phases": [ {
            "assets": [ "bin/log_rollup" ],
            "exec": "/assets/bin/log_rollup",
            "args": { "reducer": "example.com/reducer" }
        } ],
        "inputKeys": [
            "logs/2012-03-01.log",
            "logs/2012-03-02.log",
            "logs/2012-03-03.log",
            ...
            "logs/2012-03-31.log"
        ]
    }

See use cases below for more examples.


## GET /jobs

Returns the list of a user's jobs.

### Inputs

|| **Field** || **Type** || **Description** ||
|| state || Array of Strings || Returns only jobs in one of the given states. ||

### Returns

On success, returns 200 with an array of objects, each with the following
fields:

|| **Field** || **Type** || **Description** ||
|| jobId || String || See "Inputs" to `POST /jobs`. ||
|| jobName  || String || See "Inputs" to `POST /jobs`. ||
|| state || String || See "Returns" for `GET /jobs/:jobId`. ||

### Example

Example input:

    GET /jobs?state=running

Example return value:

    [ {
        "jobId": "acfec1f0-7779-11e1-81f9-979db30d72bf",
        "jobName": monthly log summary",
        "state": "running",
    } ]


## POST /jobs/:jobId/cancel

Asynchronously cancels the given job, if the job has not yet completed.  If the
job has completed, this operation does nothing.  It may take several minutes to
cancel a job.

### Returns

Returns 204 (no content) on success.


## GET /jobs/:jobId

Returns basic information about a submitted job.  For detailed runtime status,
see `GET /jobs/:jobId/status`.


### Returns

Returns 200 with at least the following fields:

|| **Job&nbsp;field** || **Type** || **Description** ||
|| jobId || String || Unique identifier for this job. ||
|| jobName || String || See "Inputs" to `POST /jobs`. ||
|| phases || Array of Objects || See "Inputs" to `POST /jobs`. ||
|| inputKeys || Array&nbsp;of&nbsp;Strings || See "Inputs" to `POST /jobs`. ||
|| createTime || DateTime&nbsp;String || Job submission time. ||
|| state || String&nbsp;(see&nbsp;below) || Describes whether the job is queued, running, or done.  See below. ||

The job **state** is one of:

|| **Job&nbsp;state** || **Meaning** ||
|| queued || The job has been created but it's not yet running. That is, no tasks have been instantiated to execute the job yet. ||
|| running || The job is currently running on at least one storage node. That is, one or more tasks have been instantiated to execute the job. Some or all of those tasks may have already completed, but not all tasks have been instantiated and completed. ||
|| done || All of the job's tasks have completed or the job has been aborted. ||

For jobs in the "done" state, these additional fields will be present:

|| finishTime || DateTime&nbsp;String || Job completion time. ||
|| result || "success" or "failure" || Describes whether the job completed successfully. ||
|| reason || String&nbsp;(see&nbsp;below) || If the job result is "failed", this describes why the job failed. ||

For failed jobs, the **reason** describes why the job failed:

|| **Job&nbsp;error&nbsp;code** || **Meaning** ||
|| jobCancelled || the job was explicitly cancelled ||
|| taskFailed || one or more of the job's tasks failed ||


### Example

Here's example state for a job which has not yet started running:

    {
        "jobId": "acfec1f0-7779-11e1-81f9-979db30d72bf",
        "jobName": monthly log summary",
        "phases": [ {
            "assets": "bin/log_rollup",
            "exec": "/assets/bin/log_rollup"
        } ],
        "inputKeys": [
            "logs/2012-03-01.log",
            "logs/2012-03-02.log",
            "logs/2012-03-03.log",
            ...
            "logs/2012-03-31.log"
        ],
        "createTime": "2012-03-20T17:19:26.732Z",
        "state": "queued"
    }

Here's an example state for the same job after it has finished successfully:

    {
        "jobId": "acfec1f0-7779-11e1-81f9-979db30d72bf",
        "jobName": monthly log summary",
        "phases": [ {
            "assets": "bin/log_rollup",
            "exec": "/assets/bin/log_rollup",
            "args": { "reducer": "example.com/reducer" }
        } ],
        "inputKeys": [
            "logs/2012-03-01.log",
            "logs/2012-03-02.log",
            "logs/2012-03-03.log",
            ...
            "logs/2012-03-31.log"
        ],
        "createTime": "2012-03-20T17:19:26.732Z",
        "state": "done",
        "result": "success",
        "finishTime": "2012-03-20T18:37:52.182Z"
    }

Here's an example where the job failed:

    {
        "jobId": "acfec1f0-7779-11e1-81f9-979db30d72bf",
        "jobName": monthly log summary",
        "phases": [ {
            "assets": "bin/log_rollup",
            "exec": "/assets/bin/log_rollup"
        } ],
        "inputKeys": [
            "logs/2012-03-01.log",
            "logs/2012-03-02.log",
            "logs/2012-03-03.log",
            ...
            "logs/2012-03-31.log"
        ],
        "createTime": "2012-03-20T17:19:26.732Z",
        "state": "done",
        "result": "failed",
        "reason": "taskFailed",
        "finishTime": "2012-03-20T18:37:52.182Z"
    }


## GET /jobs/:jobId/status

Retrieves detailed status information about a job.  The system makes a best
effort to keep this information up to date, but any of the runtime state may be
outdated or inconsistent while a job is in the "running" state.  

### Returns

This URI returns all of the fields returned by `GET /jobs/:jobId`, plus these
extra fields:

|| **Detailed job fields** || **Type** || **Description** ||
|| doneKeys || Array&nbsp;of&nbsp;Strings || Manta keys that have been processed successfully. ||
|| outputKeys || Array&nbsp;of&nbsp;Strings || Manta keys output by this job. ||
|| jobLog || Array&nbsp;of&nbsp;Objects || Log of job activity (see below). ||
|| tasks || Array&nbsp;of&nbsp;Objects || Individual task states (see below). ||

#### Job log

The **jobLog** field includes entries for all state changes for the job itself
as well as all tasks associated with the job.  This is useful for understanding
job failures and performance *postmortem*.  Each entry has the following
fields:

|| **Job&nbsp;log&nbsp;entry&nbsp;field** || **Type** || **Description** ||
|| date || DateTime&nbsp;String || Time of log entry ||
|| taskId&nbsp;(task&nbsp;entries&nbsp;only) || String || Identifier for a given task ||
|| event || String || Either "stateChange" or "message" ||
|| message || String || Human-readable message ||
|| newState&nbsp;(state&nbsp;change&nbsp;entries&nbsp;only) || String || New state name ||

#### Job progress

Callers can see precisely which keys have been completed by examining
"doneKeys".  For a rough measure of progress, callers can compare
"doneKeys.length" to "inputKeys.length".  These mechanisms rely on tasks
to report progress as they process each object, though tasks are not required
to report progress until they finish processing all of their input.

The **tasks** field shows more detailed information about how the job was
distributed to various servers and the status of each task on each server.  Each
entry of this array describes an individual task, including:

|| **Task&nbsp;field** || **Type** || **Meaning** ||
|| taskId || String || Unique identifier for this task within this job. ||
|| host || String || Unique identifier for the physical server on which this task ran. ||
|| machine || String || Unique identifier for the compute zone in which the task ran. ||
|| inputKeys || Array&nbsp;of&nbsp;Strings || List of keys assigned to this task. ||
|| startTime || DateTime&nbsp;String || When this task was started. ||
|| state || String || Describes the current state of the task (see below). ||
|| outputKeys || Array&nbsp;of&nbsp;Strings || List of keys output by this task. ||
|| endTime || DateTime&nbsp;String || The time when the task finished (done tasks only). ||
|| result || "result" or "failure" || Describes whether the task completed successfully (done tasks only). ||
|| partialKeys || List&nbsp;of&nbsp;Strings || List of Manta objects created by the task but not committed because the task failed or is still running. ||

The task **state** is one of:

|| **Task&nbsp;state** || **Meaning** ||
|| queued || The task has been planned, but has not started running yet. ||
|| loading || Marlin is preparing the compute zone to run the task. This step includes downloading and extracting the job's assets. ||
|| running || The task is currently running. ||
|| done || The task is no longer running. ||

For failed tasks, the **reason** indicates why the task failed:

|| **Error&nbsp;code** || **Meaning** ||
|| abnormalExit || The task exited with non-zero status, or exited without indicating that it had finished processing. ||
|| failed || The task explicitly failed. ||
|| timeout || The task did not complete within the alloted time interval. ||


### Examples

The response for a successfully completed job looks like this:

    {
        "jobId": "4bcf467e-4b88-4ab4-b7ab-65fad7464de9",
        "jobName": "weekly log summary",
        "phases": [ {
            "assets": "bin/log_rollup",
            "exec": "/assets/bin/log_rollup"
        } ],
        "inputKeys": [
            "logs/2012-03-11.log",
            "logs/2012-03-12.log",
            ...
            "logs/2012-03-17.log"
        ],
        "createTime": "2012-03-20T17:19:26.732Z",
        "state": "done",
        "finishTime": "2012-03-20T17:22:35.246Z",
        "result": "success",
        "doneKeys": [
            "logs/2012-03-11.log",
            "logs/2012-03-12.log",
            ...
            "logs/2012-03-17.log"
        ],
        "tasks": [ {
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "host": "2F16",
            "machine": "e7b35c46-c36d-4e02-8cde-6fdf2695af15", 
            "inputKeys": [
                "logs/2012-03-11.log",
                "logs/2012-03-12.log",
                "logs/2012-03-16.log"
            ],
            "outputKeys": [
                "log_summary/2012-03-11.txt",
                "log_summary/2012-03-12.txt",
                "log_summary/2012-03-16.txt"
            ],
            "startTime": "2012-03-20T17:19:30.102Z",
            "state": "done",
            "endTime": "2012-03-20T17:21:25.327Z",
            "result": "success"
        }, {
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "host": "2F20",
            "machine": "b01698b6-7447-11e1-8f74-fbabac25f69a",
            "inputKeys": [
                "logs/2012-03-13.log",
                "logs/2012-03-14.log",
                "logs/2012-03-15.log",
                "logs/2012-03-17.log"
            ],
            "outputKeys": [
                "log_summary/2012-03-13.txt",
                "log_summary/2012-03-14.txt",
                "log_summary/2012-03-15.txt",
                "log_summary/2012-03-17.txt"
            ],
            "startTime": "2012-03-20T17:19:33.302Z",
            "state": "done",
            "endTime": "2012-03-20T17:22:35.246Z",
            "result": "success"
        } ],
        "outputKeys": [
            "log_summary/2012-03-11.txt",
            "log_summary/2012-03-12.txt",
            "log_summary/2012-03-13.txt",
            "log_summary/2012-03-14.txt",
            "log_summary/2012-03-15.txt",
            "log_summary/2012-03-16.txt",
            "log_summary/2012-03-17.txt"
        ],
        "jobLog": [ {
            "date": "2012-03-20T17:19:26.732Z",
            "event": "stateChange",
            "newState": "queued",
            "message": "job created and queued"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "event": "stateChange",
            "newState": "running",
            "message": "job started running"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:33.302Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:40.102Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:19:41.502Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:21:25.327Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "done",
            "message": "task completed"
        }, {
            "date": "2012-03-20T17:22:35.246Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "done",
            "message": "task completed"
        }, {
            "date": "2012-03-20T17:22:35.246Z",
            "event": "stateChange",
            "newState": "done",
            "message": "job completed"
        } ]
    }

In this example, the job was distributed to two nodes, 2F16 and 2F20, where one
task was run on each node.  Each task got a few of the 7 input objects.  Both
tasks have completed processing all of their objects.

Here's what a failed job's state looks like:

    {
        "jobId": "4bcf467e-4b88-4ab4-b7ab-65fad7464de9",
        "jobName": "weekly log summary",
        "phases": [ {
            "assets": "bin/log_rollup",
            "exec": "/assets/bin/log_rollup"
        } ],
        "inputKeys": [
            "logs/2012-03-11.log",
            "logs/2012-03-12.log",
            "logs/2012-03-13.log",
            "logs/2012-03-14.log",
            "logs/2012-03-15.log",
            "logs/2012-03-16.log",
            "logs/2012-03-17.log"
        ],
        "createTime": "2012-03-20T17:19:26.732Z",
        "state": "done",
        "finishTime": "2012-03-20T17:22:35.246Z",
        "result": "failed",
        "reason": "taskFailed",
        "doneKeys": [
                "logs/2012-03-11.log",
                "logs/2012-03-12.log",
                "logs/2012-03-13.log",
                "logs/2012-03-14.log",
                "logs/2012-03-16.log",
                "logs/2012-03-17.log"
        ],
        "tasks": [ {
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "host": "2F16",
            "machine": "e7b35c46-c36d-4e02-8cde-6fdf2695af15", 
            "inputKeys": [
                "logs/2012-03-11.log",
                "logs/2012-03-12.log",
                "logs/2012-03-16.log"
            ],
            "outputKeys": [
                "log_summary/2012-03-11.txt",
                "log_summary/2012-03-12.txt",
                "log_summary/2012-03-16.txt"
            ],
            "startTime": "2012-03-20T17:19:30.102Z",
            "state": "done",
            "endTime": "2012-03-20T17:21:25.327Z",
            "result": "success"
        }, {
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "host": "2F20",
            "machine": "b01698b6-7447-11e1-8f74-fbabac25f69a",
            "inputKeys": [
                "logs/2012-03-13.log",
                "logs/2012-03-14.log",
                "logs/2012-03-17.log"
            ],
            "outputKeys": [
                "log_summary/2012-03-13.txt",
                "log_summary/2012-03-14.txt",
                "log_summary/2012-03-17.txt"
            ],
            "partialKeys": [
                "log_summary/2012-03-15.txt"
            ]
            "startTime": "2012-03-20T17:19:33.302Z",
            "state": "done",
            "endTime": "2012-03-20T17:22:35.246Z",
            "result": "failed",
            "reason": "failed",
            "error": {
                "message": "corrupt log record"
            }
        } ],
        "outputKeys": [
            "log_summary/2012-03-11.txt",
            "log_summary/2012-03-12.txt",
            "log_summary/2012-03-13.txt",
            "log_summary/2012-03-14.txt",
            "log_summary/2012-03-16.txt",
            "log_summary/2012-03-17.txt"
        ],
        "jobLog": [ {
            "date": "2012-03-20T17:19:26.732Z",
            "event": "stateChange",
            "newState": "queued",
            "message": "job created and queued"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "event": "stateChange",
            "newState": "running",
            "message": "job started running"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:33.302Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:40.102Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:19:41.502Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:21:25.327Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "done",
            "message": "task completed"
        }, {
            "date": "2012-03-20T17:22:35.246Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "done",
            "message": "task failed"
        }, {
            "date": "2012-03-20T17:22:35.246Z",
            "event": "stateChange",
            "newState": "done",
            "message": "job completed"
        } ]
    }

In this example, we can see that the second task failed partway through.  We
see this from both the task state and the job log.  Since we see that it had
started writing the summary for log_summary/2012-03-15.txt, we know it failed
on this file, and we could next look to see how far it got.  We also have the
error message reported by the task: "corrupt log record".  The more information
the task reports, the better the chance of root causing the failure from the
first occurrence.


# Task Control API

The Task Control API is used by tasks (user code running in compute zones on
Manta storage nodes) to retrieve parameters and report on progress.  The API is
serviced not by a public endpoint, but by a small server running inside the
task's own compute zone that's implicitly scoped to that zone's current task.
As a result, task identifiers are not actually necessary in the task API.

This API is *not* used to monitor task status; that's provided by the
public-facing Job API.

## GET /info

Retrieves information about the current job and task.

### Returns

This field returns the entire job info (as `GET /jobs/:jobid` does) plus the
following fields:

|| **Field** || **Type** || **Description** ||
|| taskId || String || Task identifier ||
|| taskInputKeys || Array&nbsp;of&nbsp;Array&nbsp;of&nbsp;Strings || List of Manta objects this task is expected to process. Each entry of this array is an array with two fields: the object key, and the full path to the corresponding file. ||

Object metadata (e.g., user-specified headers) are not included in the task
info.  Users can retrieve this by querying Manta directly.
 

## POST /commit

Report that the given keys have been successfully processed.  Tasks *must*
report when they finish processing all of their keys, and *may* also report
piecemeal progress.

This information is used for two purposes: first, it is recorded in the job
state so that end users can see what progress is being made while the job runs.
Second, any objects created since the last progress report are committed to the
job.  See `PUT /object/:key`.

When the last keys have been completed, the task may be terminated immediately,
potentially before this request even completes.  Any cleanup the task wishes to
complete should be done before the final call to this URI.

### Input

|| **Field** || **Type** || **Description** ||
|| keys || Array&nbsp;of&nbsp;Strings || List of objects successfully completed ||

### Returns

Returns 204 (no content) on success.

### Example

Request:

    {
        "keys": [ "logs/2012-03-11.log" ]
    }

Response:

    HTTP/1.1 204 No content
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:19:26 GMT
    X-Request-Id: 4bcf467e-4b88-4ab4-b7ab-65fad7464de9
    X-Response-Time: 32


## PUT /object/:key

Writes a new Manta object named "key" associated with this task.  The arguments,
semantics, and return value for this URI exactly match the Manta API for
creating a new object, with the following additions:

* Any object created using this URI since the last call to `POST /commit` will
  be discarded if the job fails.  Such objects will not be included in the
  job's output, but will be listed in the task state for *postmortem* analysis.
* Jobs using pipelined reducers may specify a "X-Marlin-Reducer" header whose
  value is an integer from 0 to N - 1, where N is the number of reducers in the
  next phase.  Any other input results in a 409 Conflict error.  This value
  indicates which of several reducers this output file should be fed to.  If
  this value is unspecified and there are multiple reducers in the next phase,
  output keys will be randomly assigned to reducers.

For examples, see the Manta documentation.

## POST /fail

Fail the current task.  Any objects created since the last call to `POST
/commit` will be discarded (but will be linked in the task state for
*postmortem* analysis).

### Input

The input should have at least the following fields:

|| **message** || String || human-readable error message ||

It's also recommended to include a "code" field for a machine-parseable error
code in string form.  Other fields may also be specified up to a maximum
request size.

### Returns

Returns 204 (no content) on success, but may not return at all, as the task may
be terminated immediately.

Callers can assume that this call will never fail.


### Example

Request:

    {
        "message": "corrupt log record"
    }

Response:

    HTTP/1.1 204 No content
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:19:26 GMT
    X-Request-Id: 4bcf467e-4b88-4ab4-b7ab-65fad7464de9
    X-Response-Time: 12

## POST /abort

Exactly like `POST /fail`, except that additional *postmortem* state will be
kept around and linked to the job state.  Core dumps of currently running user
processes and the entire filesystem contents will be preserved.

# Implementing use cases

## Built-in facilities

We'll provide several facilities for dealing with common tasks in MR jobs.

### mpipe: output to the next job pipeline

mpipe [&lt;manta_key&gt;] [reducer]

Reads content from stdin and outputs it from this task.  If &lt;manta_key&gt; is not
given, a new name will be created.

This is exactly equivalent to:

    $ curl -X PUT -T- http://localhost/object/$manta_key

If "reducer" is specified, it must be an integer from 0 to N - 1, where N is the
number of reducers in the next stage of the job pipeline.  This indicates that
this object should be routed to the given reducer.


### aggr: aggregate rows of numbers

aggr &lt;sum|avg|pctile99...&gt;

Reads a set of "key: value" pairs on stdin, aggregates them key-wise according
to the given function, and emits the result.

Example input:

    $ cat data.txt
    manta: 15
    marlin: 12.8
    manta: 38
    moray: 3

Example output:

    $ aggr sum < data.txt
    manta: 53
    marlin: 12.8
    moray: 3

The "key:" bit may also be omitted.


## Total word count

Input: a bunch of text files

Output: a count of the total number of words in all files

    {
        "jobName": "total word count"
        "inputKeys": [ ... ],
        "phases": [ {
            "exec": "wc -w < $mc_input_files | mpipe"
        }, {
            "type": "reduce",
            "exec": "aggr sum | mpipe"
        } ]
    }

## Specific word count

Input: a bunch of text files and a word "baseball"

Output: the total number of occurences of "baseball" in all files

    {
        "jobName": "'baseball' count"
        "inputKeys": [ ... ],
        "phases": [ {
            "exec": "grep -c baseball $mc_input_files | mpipe"
        }, {
            "type": "reduce",
            "exec": "aggr sum | mpipe"
        } ]
    }

## Word frequency count (and using a custom asset)

Input: a bunch of files

Output: a text table for the frequency count of each word; e.g.:

    marlin: 527
    manta: 328
    moray: 356

For this case, you'll need a script that computes this for a given text file.
This will do:

    $ cat wordfreq
    #!/usr/bin/perl -W
    
    $counts = {};
    
    while (<>) {
        foreach(split) {
            s/^\W+//;
            s/\W+$//;
            next unless $_;
            $counts->{lc($_)}++ 
        }
    }
    
    while (($word, $count) = each(%$counts)) {
        printf("%s: %d\n", $word, $count);
    }

You'll first need to PUT this to manta, say at /dap/bin/wordfreq.  Then the job
looks like this:

    {
        "jobName": "word frequency count"
        "inputKeys": [ ... ],
        "phases": [ {
            "assets": [ "/dap/bin/wordfreq" ],
            "exec": "/dap/bin/wordfreq $mc_input_files | mpipe"
        }, {
            "type": "reduce",
            "exec": "aggr sum | mpipe",
        } ]
    }

If you wanted, you could rewrite "wordfreq" as a one-liner and include it
directly in the "exec" property, but this would be a bit unwieldy.

## Pipelined reducers

Sometimes it's necessary to pipeline the reduce phase so that instead of
processing all input keys in one pass, the input keys are processed in chunks
across multiple passes.  For example, you may want to process half of the input
keys in each of two parallel tasks, and then process the output of that, to
avoid having to load the entire data set in memory at once.

We already support multiple "reduce" phases in a job, but to support this you
also need to parallelize a single reduce phase.  You can do this with the
"count" property:

    "phases": [
        ... /* any number of map phases */
        , {
            "type": "reduce",
            "exec": ...,
            "count": 2
        }, {
            "type": "reduce",
            "exec": ...
        }
    ]

In this example, some number of map phases are followed by a phase with two
reducers running in parallel, followed by a final reduce stage.  The two
parallel reducers are identical except for their input and output keys.  By
default, input keys are randomly assigned to one of the reducers.  If a more
sophisticated assignment is necessary, the previous map phase may specify to
which reducer a given output object will be assigned using the X-Marlin-Reducer
header, which must be an integer between 0 and N - 1, where N is the total
number of reducers in the next phase.


## Node, Python, Ruby, etc. interface

The Jobs API is built in terms of Unix shell commands for generality.  This
makes it easy to run simple jobs like "grep" and "wc".  For slightly more
complex cases, custom utilities can be built in the Unix style and composed with
existing system utilities without writing significant new code.  For more
complex cases you want entirely custom code, you can simply run your program via
the "exec" string.

We'll also provide a higher-level interface for Node.js, so that people that
want to build jobs with custom code can use Node easily.  Likely we'll have
consumers implement an interface like:

    function processObject(file, callback) { ... }

and we'll provide convenience routines for API operations (like "taskParams()",
"commit()", "abort()", and so on).

Without fleshing this out completely, we know such an interface can be
implemented without changes to the Jobs API.  When people want to use the Node
API, we tell them to write phases like this:

    "phases": [ {
        "assets": [ "my/npm/package" ],
        "exec": "mnode /assets/my/npm/package"
    } ]

"mnode" is an executable Node.js program included with the dataset that installs
the given npm package (and therefore its dependencies, too) and then invokes our
Marlin interface in that package.

If we care to, we can do something similar for Python, Ruby, R, or any other
environment.  It would be just as easy for third parties to build their own.
Users would only need to reference a third party asset:

    "phases": [ {
        "assets": [
            "third-party/framework/start.sh"
            "my/script"
        ],
        "exec": "/assets/third-party/framework/start.sh /assets/my/script" 
    } ]

## Log analysis (MapReduce)

## Image conversion / video transcoding (Straight Map)

This job uses the "convert" command that's part of the ImageMagick suite.  It
converts image files between types based on file extension.

    {
        "jobName": "convert GIFs to PNGs",
        "inputKeys": [ "file1.gif", "file2.gif", ... ]
        "phases": [ {
            "exec": "for gif in $mc_input_files; do convert $gif $(basename $gif .gif).png"
        } ]
    }


## Node interface

TBD.

## Hadoop proper

TBD.


# Marlin implementation

There are at least three types of services in the Marlin architecture:

* Web tier, which handles requests to GET job state and POST new jobs. 
* Job workers, which assign tasks to individual storage and compute nodes and
  monitor task progress.
* Compute node agents, which execute and monitor tasks and report progress and
  completion.

To keep the service resilient to partitions and individual node failures, **all
job state is stored in Moray**.  Recall that Moray spans datacenters, is
eventually consistent, and remains available in the case of minority DC
partitions.

There's a cluster of web tier workers and job workers in each datacenter, but
there's no real DC-awareness in these workers.  These workers are mostly
stateless, too, though see below for details.  They can be scaled horizontally,
and the system is resilient to failures in individual nodes.

Below we describe several possible approaches to managing the flow of jobs
through the system.


## Approach 1: stateful worker assignment, polling, no (magical) queue

In this model, each job is assigned to one job worker at a time.  This means a
given job worker only needs to monitor the state of a subset of jobs, and this
also makes it easier to synchronize changes to the job state.

**Job state**

Job state is stored in Moray under the following buckets:


|| **File** || **Purpose** ||
|| /marlin/jobs/all/JOBID || job definition (immutable properties: name, input, phases, etc.) and *committed* job state (which keys have been processed) ||
|| /marlin/jobs/unassigned/JOBID || job assignment records (for unassigned jobs) ||
|| /marlin/workers/WORKER/JOBID || jobs assignment record (for jobs assigned to this worker) ||
|| /marlin/task_assignments/COMPUTE_NODE/TASKID || task definition (immutable properties: input keys, task type, assets, etc.) ||
|| /marlin/task_state/JOBID/TASKID || task progress ||

**Web tier**

Workers retrieve current job state by fetching the job in /marlin/jobs/all.

When a new job is submitted, the web tier allocates a new id and drops an empty
object into /marlin/jobs/unassigned/JOBID, then drops the actual job
definition in /marlin/jobs/all/JOBID.  The user request is completed when
both of these records are in place (and we'll probably want some asynchronous
process to cleanup assignment records that have no corresponding job -- in case
of web tier crashes).

**Job workers**

Job workers periodically scan /marlin/jobs/unassigned and pick up new jobs.
They assign themselves new-job records using a test-and-set overwrite of the
"unassigned" file with a (worker, timestamp) tuple.  The worker "owns" this job
for a WORKER_TIMEOUT period from the saved timestamp.  After writing this
record, the worker immediately moves the file into
/marlin/workers/WORKER/JOBID.  Until the job is complete, the worker updates
the assignment record at least every WORKER_TIMEOUT.  If this update ever fails,
the worker immediately discards any job state it has and stops working on the
job.

Workers are allowed to pick up empty records (indicating new jobs) as well as
assigned jobs with an expired timestamp (indicating that a worker crashed,
possibly in between writing the above record and moving it).

When the job is first submitted, the worker queries Moray to find out where all
the input keys are and creates **tasks** to process the job.  These tasks are
written into /marlin/task_assignments/COMPUTE_NODE, which is monitored by
the corresponding compute node agent (see below).

The worker monitors the files in /marlin/task_state/JOBID.  As tasks complete,
the worker schedules the set of tasks for the next job, drops those into
/marling/task_assignments, and so on.  (This may happen even before a phase is
complete, once it's known where at least some of the output keys are.)

As tasks make progress, the worker updates the job state in /marlin/jobs/all so
that the web tier can see the results.

**Compute node agents**

Compute node agents monitor their directory in /marlin/task_assignments for new
tasks.  As their tasks make progress, they drop state updates into
/marlin/task_state/JOBID/TASKID, which is monitored by the corresponding job
worker.

**Health monitor**

This design requires an additional set of nodes to monitor workers, but the
operation is relatively simple: they sweep over all outstanding job assignments
(/marlin/workers/*) looking for expired timestamps.  They test-and-set rename
them into /marlin/jobs/unassigned.  This functionality could even be built into
job workers themselves (that they periodically scan other workers' queues for
outdated records).

**Invariants and failure modes**

Invariants:

* At most one worker can ever "own" a job.  Only the owner of a job can write
  task assignments or update the job record in /marlin/jobs/all.

If a user's code fails within a task (e.g., dumps core), and any retry policy
has been exhausted, the compute agent reports it as failed, and that will be
visible in the final job state.  It's the user's problem at that point.

If a compute node agent fails transiently (e.g., dumps core and restarts), it
should be able to pick up where it left off.  This should be invisible to the
user's code and the rest of the system, modulo slightly increased task
completion time.

If a compute node agent either fails for an extended period *or* becomes
partitioned from the rest of the system (which are indistinguishable to the
outside world), the job worker redispatches any uncompleted work to other nodes
that have the same data.  It does not cancel the task on the failed node.
If both eventually complete successfully, the worker will necessarily see one's
results first and discard the other.  (That's why the state in
/marlin/task_state is not authoritative job state, and why the worker must
"commit" this state to the actual job record for the web tier rather than have
the web tier scan /marlin/task_state.)

If a job worker fails transiently, it should be able to pick up where it left
off, since all state is stored in Moray.  This should be invisible to the rest
of the system, modulo slightly increased job completion time.

If a job worker fails for an extended period, the health monitor will unassign
the job and another worker will pick it up.  This becomes exactly like the case
where a worker fails transiently: the state in Moray is sufficient for the new
worker to pick up where the old one left off.

If a job worker becomes partitioned, the rest of the system treats this as an
extended failure once WORKER_TIMEOUT has elapsed.  The worker itself won't try
to clobber the job state after someone else has picked it up because it also
knows that WORKER_TIMEOUT has elapsed.

Health monitor nodes are completely stateless.  Partitioning, transient failure,
and persistent failure have no impact as long as enough nodes are in service to
handle load.

Web tier nodes are also completely stateless.  Any failure results in failing
any in-flight requests.  Persistent failures have no additional impact as long
as enough nodes are in service to handle load.  Partitions between the web node
and Moray would result in failed requests.

If an entire datacenter becomes partitioned from the rest, then its web tier
nodes certainly won't be able to service any requests.  All of its job workers
become partitioned (see above -- the work should eventually be picked up by
workers in other datacenters).  All of its compute node agents effectively also
become partitioned, and their work *may* be duplicated in other datacenters.


## CN agent

When the agent receives the request for a new job, it figures out how many tasks
to split that request into (probably always 1 to start with) and reports that
back to the MC.  After that, it reports all task state changes back to the MC
(task queued, loaded, running, timed out/failed/succeeded).

If there are no compute zones available at the moment, tasks are enqueued for
later.  Otherwise, the agent picks CZs to run each of the tasks and does the
following:

1. Halts the zone, if it's not already halted.
2. "zfs rollback" to the zone's origin snapshot
3. Boots the zone.
4. Creates a hyprlofs mount for the files.
5. Sets up the zsock server in the zone. (This could actually be baked into the
   zone's dataset, in which case this step is skipped.)
6. Downloads the job's bundle's assets into the zone.
7. Finally, invokes the bundle's exec script.

Steps 5, 6, and 7 have to be done in the context of the zone. We'll probably
need an agent inside the zone in order to monitor the contract anyway, so this
agent can be responsible bootstrapping the user's code: downloading the assets
and invoking the bundle.

The agent also has to implement the Task Control API. Users will invoke that
API via calls to an HTTP server on localhost (see step 5 above) which proxies
all requests via a zsock to the CN agent.  The CN agent knows which zone each
request is coming from, so it knows which task and job the request is
associated with.

`/info` is simple: it just retrieves data from the agent's persistent state,
which already has to exist to manage the job.

`/objects/:key` makes a note in the task state that this object is being
written, and then (first approach) proxies the request directly to manta. But
see "open questions" below.

`/commit` updates the persistent state for the task to indicate that all
objects written since the last checkpoing have been committed.

`/fail` also updates the persistent state, but indicates that new objects should
be discarded.  They won't actually be deleted, but marked as discarded in the
job state so users can look at them later.  After that, this request terminates
the zone.

`/abort` is like `/fail`, except that before terminating it gcores all processes
in the user's contract and does an incremental "zfs send" of the zone to a new
manta file.  (We'll want some automated mechanism to reconstruct the user's zone
based on this, presumably by "zfs receive" the dataset and then "zfs receive -i"
the stream associated with this task.)

To terminate a zone, the CN agent halts it, does a "zfs rollback" to the origin,
and updates the MC.


## Job state

* The MC always owns the immutable state for a job.
* While a job is running, the run state of the task is comprised of the run
  state of its tasks, which lives authoritatively on the corresponding CNs.
  Agents update the MC with task progress, and the MC caches this task state to
  service job status requests from the user and to determine when the job has
  completed.
* CN agents only keep state about tasks that are currently running or queued to
  run on that node.  They must keep state about already-run tasks until the MC
  has acked their report that the job completed.
* Once the job has completed, all of its state is immutable.  The MC keeps this
  information for future state requests.

## Open questions:

Roughly in order of importance:

* Need to examine all kinds of failures: agent, CN, network, user code, etc.
* What does the MC actually look like? Presumably it must be HA.
* How do the components communicate? AMQP or HTTP?
* [How] can we manage the hyprlofs mount from the global zone?
* How does the agent manage persistent state? It needs to survive agent failure,
  but it's less clear that it must (or even should) survive CN failure.  It
  certainly doesn't have to be available when the CN fails, so it may as well be
  local storage.
* When user code wants to write files to manta, how exactly does that work? Do
  we have a way to tell Manta to write one copy on the local box, and if so, do
  we avoid sending it across the network twice?
* How do we monitor the user code's contract? Presumably we'll need an agent
  in the zone that bootstraps their code and watches the contract (but then we
  need to figure out what to do if that guy dies -- adopt the contract?)
* What's the algorithm for assigning tasks to compute nodes? A pretty dumb one
  will probably suffice for a while, but it might be nice if the MC kept track
  of what everyone was working on to better distribute work.
* Should the job log be a manta object so that user's can add their own entries?

## Compute zones

Sizing
Create, reset
Tuning

## Management API?

cancel tasks, take compute zones and servers in and out of service
