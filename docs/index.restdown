---
title: Marlin
markdown2extras: wiki-tables, code-friendly
apisections: Job API, Task Control API
---

# Marlin

Marlin is a facility for running compute jobs on Manta storage nodes.  This
document describes the desired end state for version 1.


# Design goals

Marlin facilitates the following use cases:

* MapReduce in a generic environment (e.g., log analysis)
* MapReduce in a node-based environment (i.e., make it particularly easy for
  node apps)
* Straight Map with no reduce (e.g., video transcoding)
* Allowing others to run compute on your data
* Hadoop itself?

Of course, we want this to be easy to use, monitor, and debug.  A good
barometer is that word count, the MR equivalent of "hello world", should be
trivial to write.  

Marlin defines two public APIs:

* Job API: allows end users to submit, manage, and monitor compute jobs
* Task Control API: used by user code running inside Marlin to coordinate
  execution and report progress.

All DateTimes in these APIs use W3C DTF in UTC ("YYYY-MM-DDThh:mm:ss.sZ").


# Job API

The Job API is a public-facing API that sits alongside the public Manta endpoint
and lets end users submit, manage, and monitor compute jobs.


## POST /jobs

Submit a new **job** to be run immediately under Marlin.

Each job defines a set of input Manta objects on which to operate and code to
process each of these objects.  When the job is submitted, the system divides it
into several tasks and schedules each task to run on a single Manta storage node
on whatever input keys are stored on that node.  Each task runs inside a
**compute zone** on that node.  Tasks must report when they have succeeded or
failed, and may also report progress as they go.  The system may time out tasks
that are taking too long.

While the job is running, users can retrieve the status of the job to see which
tasks have completed, which tasks are running, which keys have been processed,
and which keys are left.

When all tasks complete, the job itself completes.  The job succeeds if all
tasks succeed, and the job fails otherwise.  Failed tasks are not retried.


### Inputs

|| **Field** || **Type** || **Description** ||
|| jobName || String || Human-readable job label (not necessarily unique). ||
|| bundle || Object || Describes the code that implements the body of each task. ||
|| bundle.assets || Array of Strings || List of Manta objects (identified by key) that will be made available in the compute zone.  These should be binaries, libraries, or other shared objects used to actually run the job, not inputs to the job. ||
|| bundle.exec || String || Script to run to actually launch the task. ||
|| bundle.args || Object || User-defined task arguments.  This is just an arbitrary JSON object that will be available to each task, like an array of reducer nodes. ||
|| inputKeys || Array&nbsp;of&nbsp;Strings || List of Manta objects to process. ||

You specify the code that runs in each task with the "bundle" object.  Any Manta
keys listed in "bundle.assets" will be made available under "/assets" in the
compute zone before the task is started.  The task is started by invoking
"bundle.exec".

### Returns

On success, returns 201 with the URI for the job in a Location header and the
state of the job as returned by `GET /jobs/:jobId`.

### Example

Here's an example job that uses a script stored on Manta for the body of the
task:

    {
        "jobName": "monthly log summary",
        "bundle": {
            "assets": "bin/log_rollup",
            "exec": "/assets/bin/log_rollup",
            "args": { "reducer": "example.com/reducer" }
        },
        "inputKeys": [
            "logs/2012-03-01.log",
            "logs/2012-03-02.log",
            "logs/2012-03-03.log",
            ...
            "logs/2012-03-31.log"
        ]
    }


## GET /jobs

Returns the list of a user's jobs.

### Inputs

|| **Field** || **Type** || **Description** ||
|| state || Array of Strings || Returns only jobs in one of the given states. ||

### Returns

On success, returns 200 with an array of objects, each with the following
fields:

|| **Field** || **Type** || **Description** ||
|| jobId || String || See "Inputs" to `POST /jobs`. ||
|| jobName  || String || See "Inputs" to `POST /jobs`. ||
|| state || String || See "Returns" for `GET /jobs/:jobId`. ||

### Example

Example input:

    GET /jobs?state=running

Example return value:

    [ {
        "jobId": "acfec1f0-7779-11e1-81f9-979db30d72bf",
        "jobName": monthly log summary",
        "state": "running",
    } ]


## POST /jobs/:jobId/cancel

Asynchronously cancels the given job, if the job has not yet completed.  If the
job has completed, this operation does nothing.  It may take several minutes to
cancel a job.

### Returns

Returns 204 (no content) on success.


## GET /jobs/:jobId

Returns basic information about a submitted job.  For detailed runtime status,
see `GET /jobs/:jobId/status`.


### Returns

Returns 200 with at least the following fields:

|| **Job&nbsp;field** || **Type** || **Description** ||
|| jobId || String || Unique identifier for this job. ||
|| jobName || String || See "Inputs" to `POST /jobs`. ||
|| bundle || Object || See "Inputs" to `POST /jobs`. ||
|| inputKeys || Array&nbsp;of&nbsp;Strings || See "Inputs" to `POST /jobs`. ||
|| createTime || DateTime&nbsp;String || Job submission time. ||
|| state || String&nbsp;(see&nbsp;below) || Describes whether the job is queued, running, or done.  See below. ||

For jobs in the "done" state, these additional fields will be present:

|| finishTime || DateTime&nbsp;String || Job completion time. ||
|| result || "success" or "failure" || Describes whether the job completed successfully. ||
|| reason || String&nbsp;(see&nbsp;below) || If the job result is "failed", this describes why the job failed. ||

The job **state** is one of:

|| **Job&nbsp;state** || **Meaning** ||
|| queued || The job has been created but it's not yet running. That is, no tasks have been instantiated to execute the job yet. ||
|| running || The job is currently running on at least one storage node. That is, one or more tasks have been instantiated to execute the job. Some or all of those tasks may have already completed, but not all tasks have been instantiated and completed. ||
|| done || All of the job's tasks have completed or the job has been aborted. ||

For failed jobs, the **reason** describes why the job failed:

|| **Job&nbsp;error&nbsp;code** || **Meaning** ||
|| jobCancelled || the job was explicitly cancelled ||
|| taskFailed || one or more of the job's tasks failed ||

### Example

Here's example state for a job which has not yet started running:

    {
        "jobId": "acfec1f0-7779-11e1-81f9-979db30d72bf",
        "jobName": monthly log summary",
        "bundle": {
            "assets": "bin/log_rollup",
            "exec": "/assets/bin/log_rollup",
            "args": { "reducer": "example.com/reducer" }
        },
        "inputKeys": [
            "logs/2012-03-01.log",
            "logs/2012-03-02.log",
            "logs/2012-03-03.log",
            ...
            "logs/2012-03-31.log"
        ],
        "createTime": "2012-03-20T17:19:26.732Z",
        "state": "queued"
    }

Here's an example state for the same job after it has finished successfully:

    {
        "jobId": "acfec1f0-7779-11e1-81f9-979db30d72bf",
        "jobName": monthly log summary",
        "bundle": {
            "assets": "bin/log_rollup",
            "exec": "/assets/bin/log_rollup",
            "args": { "reducer": "example.com/reducer" }
        },
        "inputKeys": [
            "logs/2012-03-01.log",
            "logs/2012-03-02.log",
            "logs/2012-03-03.log",
            ...
            "logs/2012-03-31.log"
        ],
        "createTime": "2012-03-20T17:19:26.732Z",
        "state": "done",
        "result": "success",
        "finishTime": "2012-03-20T18:37:52.182Z"
    }

Here's an example where the job failed:

    {
        "jobId": "acfec1f0-7779-11e1-81f9-979db30d72bf",
        "jobName": monthly log summary",
        "bundle": {
            "assets": "bin/log_rollup",
            "exec": "/assets/bin/log_rollup",
            "args": { "reducer": "example.com/reducer" }
        },
        "inputKeys": [
            "logs/2012-03-01.log",
            "logs/2012-03-02.log",
            "logs/2012-03-03.log",
            ...
            "logs/2012-03-31.log"
        ],
        "createTime": "2012-03-20T17:19:26.732Z",
        "state": "done",
        "result": "failed",
        "reason": "taskFailed",
        "finishTime": "2012-03-20T18:37:52.182Z"
    }


## GET /jobs/:jobId/status

Retrieves detailed status information about a job.  The system makes a best
effort to keep this information up to date, but any of the runtime state may be
outdated or inconsistent while a job is in the "running" state.  

### Returns

This URI returns all of the fields returned by `GET /jobs/:jobId`, plus these
extra fields:

|| **Detailed job fields** || **Type** || **Description** ||
|| doneKeys || Array&nbsp;of&nbsp;Strings || Manta keys that have been processed successfully. ||
|| outputKeys || Array&nbsp;of&nbsp;Strings || Manta keys output by this job. ||
|| jobLog || Array&nbsp;of&nbsp;Objects || Log of job activity (see below). ||
|| tasks || Array&nbsp;of&nbsp;Objects || Individual task states (see below). ||

#### Job log

The **jobLog** field includes entries for all state changes for the job itself
as well as all tasks associated with the job.  This is useful for understanding
job failures and performance *postmortem*.  Each entry has the following
fields:

|| **Job&nbsp;log&nbsp;entry&nbsp;field** || **Type** || **Description** ||
|| date || DateTime&nbsp;String || Time of log entry ||
|| taskId&nbsp;(task&nbsp;entries&nbsp;only) || String || Identifier for a given task ||
|| event || String || Either "stateChange" or "message" ||
|| message || String || Human-readable message ||
|| newState&nbsp;(state&nbsp;change&nbsp;entries&nbsp;only) || String || New state name ||

#### Job progress

Callers can see precisely which keys have been completed by examining
"doneKeys".  For a rough measure of progress, callers can compare
"doneKeys.length" to "inputKeys.length".  These mechanisms rely on tasks
to report progress as they process each object, though tasks are not required
to report progress until they finish processing all of their input.

The **tasks** field shows more detailed information about how the job was
distributed to various servers and the status of each task on each server.  Each
entry of this array describes an individual task, including:

|| **Task&nbsp;field** || **Type** || **Meaning** ||
|| taskId || String || Unique identifier for this task within this job. ||
|| host || String || Unique identifier for the physical server on which this task ran. ||
|| machine || String || Unique identifier for the compute zone in which the task ran. ||
|| inputKeys || Array&nbsp;of&nbsp;Strings || List of keys assigned to this task. ||
|| startTime || DateTime&nbsp;String || When this task was started. ||
|| state || String || Describes the current state of the task (see below). ||
|| outputKeys || Array&nbsp;of&nbsp;Strings || List of keys output by this task. ||
|| endTime || DateTime&nbsp;String || The time when the task finished (done tasks only). ||
|| result || "result" or "failure" || Describes whether the task completed successfully (done tasks only). ||
|| partialKeys || List&nbsp;of&nbsp;Strings || List of Manta objects created by the task but not committed because the task failed or is still running. ||

The task **state** is one of:

|| **Task&nbsp;state** || **Meaning** ||
|| queued || The task has been planned, but has not started running yet. ||
|| loading || Marlin is preparing the compute zone to run the task. This step includes downloading and extracting the job's bundle. ||
|| running || The task is currently running. ||
|| done || The task is no longer running. ||

For failed tasks, the **reason** indicates why the task failed:

|| **Error&nbsp;code** || **Meaning** ||
|| abnormalExit || The task exited with non-zero status, or exited without indicating that it had finished processing. ||
|| failed || The task explicitly failed. ||
|| timeout || The task did not complete within the alloted time interval. ||


### Examples

The response for a successfully completed job looks like this:

    {
        "jobId": "4bcf467e-4b88-4ab4-b7ab-65fad7464de9",
        "jobName": "weekly log summary",
        "bundle": {
            "assets": "bin/log_rollup",
            "exec": "/assets/bin/log_rollup",
            "args": { "reducer": "example.com/reducer" }
        },
        "inputKeys": [
            "logs/2012-03-11.log",
            "logs/2012-03-12.log",
            ...
            "logs/2012-03-17.log"
        ],
        "createTime": "2012-03-20T17:19:26.732Z",
        "state": "done",
        "finishTime": "2012-03-20T17:22:35.246Z",
        "result": "success",
        "doneKeys": [
            "logs/2012-03-11.log",
            "logs/2012-03-12.log",
            ...
            "logs/2012-03-17.log"
        ],
        "tasks": [ {
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "host": "2F16",
            "machine": "e7b35c46-c36d-4e02-8cde-6fdf2695af15", 
            "inputKeys": [
                "logs/2012-03-11.log",
                "logs/2012-03-12.log",
                "logs/2012-03-16.log"
            ],
            "outputKeys": [
                "log_summary/2012-03-11.txt",
                "log_summary/2012-03-12.txt",
                "log_summary/2012-03-16.txt"
            ],
            "startTime": "2012-03-20T17:19:30.102Z",
            "state": "done",
            "endTime": "2012-03-20T17:21:25.327Z",
            "result": "success"
        }, {
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "host": "2F20",
            "machine": "b01698b6-7447-11e1-8f74-fbabac25f69a",
            "inputKeys": [
                "logs/2012-03-13.log",
                "logs/2012-03-14.log",
                "logs/2012-03-15.log",
                "logs/2012-03-17.log"
            ],
            "outputKeys": [
                "log_summary/2012-03-13.txt",
                "log_summary/2012-03-14.txt",
                "log_summary/2012-03-15.txt",
                "log_summary/2012-03-17.txt"
            ],
            "startTime": "2012-03-20T17:19:33.302Z",
            "state": "done",
            "endTime": "2012-03-20T17:22:35.246Z",
            "result": "success"
        } ],
        "outputKeys": [
            "log_summary/2012-03-11.txt",
            "log_summary/2012-03-12.txt",
            "log_summary/2012-03-13.txt",
            "log_summary/2012-03-14.txt",
            "log_summary/2012-03-15.txt",
            "log_summary/2012-03-16.txt",
            "log_summary/2012-03-17.txt"
        ],
        "jobLog": [ {
            "date": "2012-03-20T17:19:26.732Z",
            "event": "stateChange",
            "newState": "queued",
            "message": "job created and queued"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "event": "stateChange",
            "newState": "running",
            "message": "job started running"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:33.302Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:40.102Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:19:41.502Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:21:25.327Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "done",
            "message": "task completed"
        }, {
            "date": "2012-03-20T17:22:35.246Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "done",
            "message": "task completed"
        }, {
            "date": "2012-03-20T17:22:35.246Z",
            "event": "stateChange",
            "newState": "done",
            "message": "job completed"
        } ]
    }

In this example, the job was distributed to two nodes, 2F16 and 2F20, where one
task was run on each node.  Each task got a few of the 7 input objects.  Both
tasks have completed processing all of their objects.

Here's what a failed job's state looks like:

    {
        "jobId": "4bcf467e-4b88-4ab4-b7ab-65fad7464de9",
        "jobName": "weekly log summary",
        "bundle": {
            "assets": "bin/log_rollup",
            "exec": "/assets/bin/log_rollup",
            "args": { "reducer": "example.com/reducer" }
        },
        "inputKeys": [
            "logs/2012-03-11.log",
            "logs/2012-03-12.log",
            "logs/2012-03-13.log",
            "logs/2012-03-14.log",
            "logs/2012-03-15.log",
            "logs/2012-03-16.log",
            "logs/2012-03-17.log"
        ],
        "createTime": "2012-03-20T17:19:26.732Z",
        "state": "done",
        "finishTime": "2012-03-20T17:22:35.246Z",
        "result": "failed",
        "reason": "taskFailed",
        "doneKeys": [
                "logs/2012-03-11.log",
                "logs/2012-03-12.log",
                "logs/2012-03-13.log",
                "logs/2012-03-14.log",
                "logs/2012-03-16.log",
                "logs/2012-03-17.log"
        ],
        "tasks": [ {
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "host": "2F16",
            "machine": "e7b35c46-c36d-4e02-8cde-6fdf2695af15", 
            "inputKeys": [
                "logs/2012-03-11.log",
                "logs/2012-03-12.log",
                "logs/2012-03-16.log"
            ],
            "outputKeys": [
                "log_summary/2012-03-11.txt",
                "log_summary/2012-03-12.txt",
                "log_summary/2012-03-16.txt"
            ],
            "startTime": "2012-03-20T17:19:30.102Z",
            "state": "done",
            "endTime": "2012-03-20T17:21:25.327Z",
            "result": "success"
        }, {
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "host": "2F20",
            "machine": "b01698b6-7447-11e1-8f74-fbabac25f69a",
            "inputKeys": [
                "logs/2012-03-13.log",
                "logs/2012-03-14.log",
                "logs/2012-03-17.log"
            ],
            "outputKeys": [
                "log_summary/2012-03-13.txt",
                "log_summary/2012-03-14.txt",
                "log_summary/2012-03-17.txt"
            ],
            "partialKeys": [
                "log_summary/2012-03-15.txt"
            ]
            "startTime": "2012-03-20T17:19:33.302Z",
            "state": "done",
            "endTime": "2012-03-20T17:22:35.246Z",
            "result": "failed",
            "reason": "failed",
            "error": {
                "message": "corrupt log record"
            }
        } ],
        "outputKeys": [
            "log_summary/2012-03-11.txt",
            "log_summary/2012-03-12.txt",
            "log_summary/2012-03-13.txt",
            "log_summary/2012-03-14.txt",
            "log_summary/2012-03-16.txt",
            "log_summary/2012-03-17.txt"
        ],
        "jobLog": [ {
            "date": "2012-03-20T17:19:26.732Z",
            "event": "stateChange",
            "newState": "queued",
            "message": "job created and queued"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "event": "stateChange",
            "newState": "running",
            "message": "job started running"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:33.302Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:40.102Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:19:41.502Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:21:25.327Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "done",
            "message": "task completed"
        }, {
            "date": "2012-03-20T17:22:35.246Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "done",
            "message": "task failed"
        }, {
            "date": "2012-03-20T17:22:35.246Z",
            "event": "stateChange",
            "newState": "done",
            "message": "job completed"
        } ]
    }

In this example, we can see that the second task failed partway through.  We
see this from both the task state and the job log.  Since we see that it had
started writing the summary for log_summary/2012-03-15.txt, we know it failed
on this file, and we could next look to see how far it got.  We also have the
error message reported by the task: "corrupt log record".  The more information
the task reports, the better the chance of root causing the failure from the
first occurrence.


# Task Control API

The Task Control API is used by tasks (user code running in compute zones on
Manta storage nodes) to retrieve parameters and report on progress.  The API is
serviced not by a public endpoint, but by a small server running inside the
task's own compute zone that's implicitly scoped to that zone's current task.
As a result, task identifiers are not actually necessary in the task API.

This API is *not* used to monitor task status; that's provided by the
public-facing Job API.

## GET /info

Retrieves information about the current job and task.

### Returns

|| **Field** || **Type** || **Description** ||
|| jobId || String || See `POST /jobs`. ||
|| jobName || String || See `POST /jobs`. ||
|| bundle || Object || See `POST /jobs`. ||
|| taskId || String || Task identifier ||
|| inputKeys || Array&nbsp;of&nbsp;Array&nbsp;of&nbsp;Strings || List of Manta objects this task is expected to process. Each entry of this array is an array with two fields: the object key, and the full path to the corresponding file. ||

Object metadata (e.g., user-specified headers) are not included in the task
info.  Users can retrieve this by querying Manta directly.

### Example

Here's an example response:

    {
        "jobId": "5e42cd1e-34bb-402f-8796-bf5a2cae47db",
        "jobName": "weekly log rollup",
        "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
        "inputKeys": [
            [ "logs/2012-03-11.log", "/keys/logs/2012-03-11.log" ],
            [ "logs/2012-03-12.log", "/keys/logs/2012-03-12.log" ],
            [ "logs/2012-03-16.log", "/keys/logs/2012-03-16.log" ]
        ]
    }
 

## POST /checkpoint

Report that the given keys have been successfully processed.  Tasks *must*
report when they finish processing all of their keys, and *may* also report
piecemeal progress.

This information is used for two purposes: first, it is recorded in the job
state so that end users can see what progress is being made while the job runs.
Second, any objects created since the last progress report are committed to the
job.  See `PUT /object/:key`.

When the last keys have been completed, the task may be terminated immediately,
potentially before this request even completes.  Any cleanup the task wishes to
complete should be done before the final call to this URI.

### Input

|| **Field** || **Type** || **Description** ||
|| keys || Array&nbsp;of&nbsp;Strings || List of objects successfully completed ||

### Returns

Returns 204 (no content) on success.

### Example

Request:

    POST /checkpoint HTTP/1.1
    Authorization: ...
    Host: api.example.com
    Accept: application/json
    Content-Type: application/json

    {
        "keys": [ "logs/2012-03-11.log" ]
    }

Response:

    HTTP/1.1 204 No content
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:19:26 GMT
    X-Request-Id: 4bcf467e-4b88-4ab4-b7ab-65fad7464de9
    X-Response-Time: 32


## PUT /object/:key

Writes a new Manta object named "key" associated with this task.  The arguments,
semantics, and return value for this URI exactly match the Manta API for
creating a new object, except that any object created using this URI since the
last call to `POST /checkpoint` will be discarded if the job fails.  Such
objects will not be included in the job's output, but will be listed in the task
state for *postmortem* analysis.

For examples, see the Manta documentation.

## POST /fail

Fail the current task.  Any objects created since the last call to `POST
/checkpoint` will be discarded (but will be linked in the task state for
*postmortem* analysis).

### Input

The input should have at least the following fields:

|| **message** || String || human-readable error message ||

It's also recommended to include a "code" field for a machine-parseable error
code in string form.

### Returns

Returns 204 (no content) on success, but may not return at all, as the task may
be terminated immediately.

Callers can assume that this call can never fail.


### Example

Request:

    POST /fail HTTP/1.1
    Authorization: ...
    Host: api.example.com
    Accept: application/json
    Content-Type: application/json

    {
        "message": "corrupt log record"
    }

Response:

    HTTP/1.1 204 No content
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:19:26 GMT
    X-Request-Id: 4bcf467e-4b88-4ab4-b7ab-65fad7464de9
    X-Response-Time: 12

## POST /abort

Exactly like `POST /fail`, except that additional *postmortem* state will be
kept around and linked to the job state.  Core dumps of currently running user
processes and the entire filesystem contents will be preserved.

# Implementing use cases

## Log analysis (MapReduce)
## Video transcoding (Straight Map)
## Node interface
## Hadoop

# Implementation

## Overview of a job

Requests for the Jobs API come in through the Manta web API and get proxied to
a Marlin Controller (MC).  The immutable properties (assigned id, job name, job
params, etc.) are stored persistently, then the user's request returns
immediately.

Sometime later (possibly immediately, but asynchronously), the MC queries Manta
(Moray, specifically) to find out where each of the job's input keys are
located.  Using this information the MC figures out which compute nodes should
process which keys and submits requests to agents running on each of the
assigned compute nodes.  These requests include the job parameters and the set
of keys that the target CN must process.  Each CN responds with one or more
tasks that will process the job on that node.

At this point, the MC's main job is to receive progress updates from each of the
compute nodes, update its internal and persistent state, and service job status
requests for this information.  When the MC determines that all tasks for this
job have completed, it marks the job "done" and records that persistently.  At
that point, all of the job's state is forever immutable, and it only exists at
the MC.

## CN agent

When the agent receives the request for a new job, it figures out how many tasks
to split that request into (probably always 1 to start with) and reports that
back to the MC.  After that, it reports all task state changes back to the MC
(task queued, loaded, running, timed out/failed/succeeded).

If there are no compute zones available at the moment, tasks are enqueued for
later.  Otherwise, the agent picks CZs to run each of the tasks and does the
following:

1. Halts the zone, if it's not already halted.
2. "zfs rollback" to the zone's origin snapshot
3. Boots the zone.
4. Creates a hyprlofs mount for the files.
5. Sets up the zsock server in the zone. (This could actually be baked into the
   zone's dataset, in which case this step is skipped.)
6. Downloads the job's bundle's assets into the zone.
7. Finally, invokes the bundle's exec script.

Steps 5, 6, and 7 have to be done in the context of the zone. We'll probably
need an agent inside the zone in order to monitor the contract anyway, so this
agent can be responsible bootstrapping the user's code: downloading the assets
and invoking the bundle.

The agent also has to implement the Task Control API. Users will invoke that
API via calls to an HTTP server on localhost (see step 5 above) which proxies
all requests via a zsock to the CN agent.  The CN agent knows which zone each
request is coming from, so it knows which task and job the request is
associated with.

`/info` is simple: it just retrieves data from the agent's persistent state,
which already has to exist to manage the job.

`/objects/:key` makes a note in the task state that this object is being
written, and then (first approach) proxies the request directly to manta. But
see "open questions" below.

`/checkpoint` updates the persistent state for the task to indicate that all
objects written since the last checkpoing have been committed.

`/fail` also updates the persistent state, but indicates that new objects should
be discarded.  They won't actually be deleted, but marked as discarded in the
job state so users can look at them later.  After that, this request terminates
the zone.

`/abort` is like `/fail`, except that before terminating it gcores all processes
in the user's contract and does an incremental "zfs send" of the zone to a new
manta file.  (We'll want some automated mechanism to reconstruct the user's zone
based on this, presumably by "zfs receive" the dataset and then "zfs receive -i"
the stream associated with this task.)

To terminate a zone, the CN agent halts it, does a "zfs rollback" to the origin,
and updates the MC.


## Job state

* The MC always owns the immutable state for a job.
* While a job is running, the run state of the task is comprised of the run
  state of its tasks, which lives authoritatively on the corresponding CNs.
  Agents update the MC with task progress, and the MC caches this task state to
  service job status requests from the user and to determine when the job has
  completed.
* CN agents only keep state about tasks that are currently running or queued to
  run on that node.  They must keep state about already-run tasks until the MC
  has acked their report that the job completed.
* Once the job has completed, all of its state is immutable.  The MC keeps this
  information for future state requests.

## Open questions:

Roughly in order of importance:

* Need to examine all kinds of failures: agent, CN, network, user code, etc.
* What does the MC actually look like? Presumably it must be HA.
* How do the components communicate? AMQP or HTTP?
* [How] can we manage the hyprlofs mount from the global zone?
* How does the agent manage persistent state? It needs to survive agent failure,
  but it's less clear that it must (or even should) survive CN failure.  It
  certainly doesn't have to be available when the CN fails, so it may as well be
  local storage.
* When user code wants to write files to manta, how exactly does that work? Do
  we have a way to tell Manta to write one copy on the local box, and if so, do
  we avoid sending it across the network twice?
* How do we monitor the user code's contract? Presumably we'll need an agent
  in the zone that bootstraps their code and watches the contract (but then we
  need to figure out what to do if that guy dies -- adopt the contract?)
* What's the algorithm for assigning tasks to compute nodes? A pretty dumb one
  will probably suffice for a while, but it might be nice if the MC kept track
  of what everyone was working on to better distribute work.
* Should the job log be a manta object so that user's can add their own entries?

## Compute zones

Sizing
Create, reset
Tuning

## Management API?

cancel tasks, take compute zones and servers in and out of service
