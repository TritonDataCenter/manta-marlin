---
title: Marlin
markdown2extras: wiki-tables, code-friendly
apisections: Task Control API
---

# Marlin

Marlin is a facility for running compute jobs on Manta storage nodes.  This
document describes the desired end state for version 1.


# Overview and Public API

For an overview of what Marlin is and how users interact with it, see the
documentation in muskie.git.  Really, go read that first.  This document assumes
familiarity with that, including how the rest of the Manta object store works.


# Design goals

Marlin facilitates the following use cases:

* MapReduce in a generic environment (e.g., log analysis)
* MapReduce in a node-based environment (i.e., make it particularly easy for
  node apps)
* Straight Map with no reduce (e.g., video transcoding)
* Allowing others to run compute on your data (e.g., for government
  organizations with lots of public data, e.g.,
  http://community.topcoder.com/pds-challenge/)
* Hadoop itself?

Of course, we want this to be easy to use, monitor, and debug.  A good
barometer is that word count, the MR equivalent of "hello world", should be
trivial to write.

Marlin defines two public APIs:

* Job API: allows end users to submit, cancel, and monitor compute jobs.  This
  will ultimately become part of the Manta API.
* Task Control API: used by user code running inside Marlin to coordinate
  execution and report progress.

All DateTimes in these APIs use W3C DTF in UTC ("YYYY-MM-DDThh:mm:ss.sZ").



# Streaming API

This section describes a modified API based around streams, both internally and
externally.

In this model, a job's input, output, and discarded objects are represented as
streams rather than the "inputKeys", "outputKeys", and "discardedKeys"
properties.  We now have the following resources:

* `POST /:userid/jobs`: submit a new job (JSON)
* `GET /:userid/jobs`: list jobs (JSON or text/plain)
* `GET /:userid/jobs/:jobid`: retrieve current job state (JSON or text/plain?)
* `PUT /:userid/jobs/:jobid/input`: submit input keys for a job (text/plain)
* `GET /:userid/jobs/:jobid/output`: retrieve list of output keys for a job
  (text/plain)
* `GET /:userid/jobs/:jobid/phases/:i/results`: retrieves list of results for a
  given phase. (JSON object describing each input key to the phase, the result
  (success or failure), and any keys output or discarded.)

Below are example implementations of several use cases.  In all of these
examples, it's assumed that the input is streamed in and the output is streamed
out.  The sequence for each of these examples is:

    $ curl -i -X POST -d @input.json '-Hcontent-type: application/json' \
        manta.joyent.us/:userid/jobs
    
    # extract new job URL from Location header

    $ curl -X PUT -d @input_keys.txt manta.joyent.us/:userid/jobs/:jobid/input

    $ curl manta.joyent.us/:userid/jobs/:jobid

    # repeat above until state == "done"

    $ curl -o output_keys.txt manta.joyent.us/:userid/jobs/:jobid/output

Alternatively, jobs may be pipelined, so that the input keys from one job come
from the output keys of a previous job.  To do this, use the "inputStream"
property on the job itself, as in:

    {
        "jobName": "part two"
        "phases": [ ... ],
        "inputStream": "/:userid/jobs/:jobid/output"
    }

"inputStream" may refer to any other job's output stream, or any Manta object.
The object must have type text/plain.

There's no corresponding "outputStream" property, since every job has its own
output stream at `/:userid/jobs/:jobid/output` that can be used anywhere an
input stream or Manta file is required.

The following examples include whitespace for readability, but JSON does not
allow whitespace within strings.  To use these examples, either remove the
newlines or save the phase contents as a file in Manta and use that file as an
asset.


## Total word count

Input: a list of plain text objects (e.g., plaintext mailbox files, source code)

Output: a single object with one line containing the total number of characters,
words, and lines in all files (e.g., "wc" output)

    {
        "jobName": "total word count",
        "phases": [ {
            "exec": "wc"
        }, {
            "type": "reduce",
            "exec": "awk '{ l += $1; w += $2; c += $3 } END { print l, w, c }'"
        } ]
    }

Example output (exactly the same format as "wc"):

      122919  380672 3416475


## Word frequency count

Input: a list of plain text objects.

Output: a single object with one line *per whitespace-delimited* word in the
input objects identifying how many times that word occurred in all of the input
files.  The result should be sorted by count.

    {
        "jobName": "word frequency count",
        "phases": [ {
            "exec": "awk '
                {
                    for (i = 1; i < NF; i++) {
                        counts[$i]++
                    }
                }
                END {
                    for (i in counts) {
                        print i, counts[i]
                    }
                }'
            "
        }, {
            "type": "reduce",
            "exec": "awk '
                {
                    byword[$1] += $2
                }
                END {
                    for (i in byword) {
                        print i, byword[i]
                    }
                }' | sort -k2,2 -n"
        } ]
    }

Example output:

    aardvark: 37
    broomstick: 318
    mÃ©tier: 1

## Line count by file extension

Input: a list of text files (e.g., source code)

Output: a total count of lines by file extension

    {
        "jobName": "total lines by file extension",
        "phases": [ {
            "exec": "wc -l $mr_input_key | 
                (read count file; echo $count ${file##*.}",
        }, {
            "type": "reduce",
            "exec": "awk '
                {
                    byword[$2] += $1
                }
                END {
                    for (word in byword) {
                        print byword[word], word
                    }
                }'
            "
        } ]
    }

Note: we use $mr\_input\_key rather than reading from stdin so that "wc" prints
out the filename after the word count.

Example output:

    12992 py
    15773 md
    32782 h
    122919 js
    186279 c

## Word index

Input: a list of text files (e.g., plaintext emails)

Output: a file with one word per line indicating which files and lines the word
appears in the input files.

Example output:
aardvark mail0100.txt:12,13
curmudgeon file1.txt:30 file2.txt:10,12,14 file3.txt:15

    {
        "jobName": "word index",
        "phases": [ {
            "exec": "awk '
                {
                    for (i = 1; i <= NF; i++) {
                        if ($i in indx) {
                            indx[$i] = indx[$i] "," NR
                        } else {
                            indx[$i] = FILENAME ":" NR
                        }   
                    }   
                }
                END {
                    for (word in indx) {
                        print word, indx[word]
                    }   
                }'
            "
        }, {
            "type": "reduce",
            "exec": "awk '
                {
                    for (i = 2; i <= NF; i++) {
                        indx[$1] = indx[$1] " " $i
                    }
                }
                END {
                    for (word in indx) {
                        print word, indx[word]
                    }
                }'
            "
        } ]
    }

Example output:

    And  macbeth.txt:6,10 hamlet.txt:4,25,28,30,32
    Is  hamlet.txt:29
    Ophelia!  hamlet.txt:33
    Signifying  macbeth.txt:12
    by  macbeth.txt:11 hamlet.txt:4,5
    for  macbeth.txt:2
    regard  hamlet.txt:31
    shadow,  macbeth.txt:8
    such  macbeth.txt:2
    there's  hamlet.txt:9

## Image conversion

Input: list of arbitrary image files

Output: list of same images converted to PNGs

Assumes: ImageMagick package installed in the zone

    {
        "jobName": "convert images to PNGs",
        "phases": [ {
            "exec": "convert $mr_input_file ${mr_input_file##*.}.png &&
                dpipe < ${mr_input_file##*.}.png"
        } ]
    }

The output here is a list of keys of the converted images, as in:

    img001.png
    img002.png

## Video transcoding

Input: list of arbitrary video files

Output: list of same videos encoded with webm using default ffmpeg presets

Assumes: ffmpeg package installed in the zone

    {
        "jobName": "transcode video to webm",
        "phases": [ {
            "exec": "ffmpeg -i $mr_input_file ${mr_input_file##*.}.webm &&
                dpipe < ${mr_input_file##*.}.webm"
        } ]
    }

The output here is a list of keys of the transcoded videos, as in:

    video001.webm
    video002.webm

## Ad-hoc Apache log query

Input: list of logs in Apache common log format

Output: count of all queries decomposed by URI

Assumes: apache2json tool, which converts each Apache common log record into a
    JSON object with named fields

    {
        "jobName": "count HTTP requests by URI"
        "phases": [ {
            "exec": "apache2json | json -a url"
        }, {
            "type": "reduce",
            "exec": "sort | uniq -c | sort -n"
        } ]
    }

Example output:

       3 /apache_pb.gif
       4 /kart.htm
      13 /index.htm

## Data warehousing with an intermediate database

Input: list of logs in Apache common log format

Output: SQL database describing all of the HTTP requests

    {
        "jobName": "aggregate HTTP requests into a SQL database",
        "phases": [ {
            "exec": "awk '{
                print \"INSERT INTO Requests (SourceIP, User, URI, Status, Size)
                  VALUES (\" $1 \",\" $3 \",\" $7 \",\" $9 \",\" $10 \");\" }'"
        }, {
            "type": "reduce",
            "exec": "echo 'CREATE TABLE Requests (
                SourceIP VARCHAR(20),
                User VARCHAR(20),
                URI VARCHAR(256),
                Status VARCHAR(4),
                Size INT(7)
            );'; cat"
        } ]
    }

Note: this assumes URIs have no spaces in them.  A more robust solution would
use an actual Apache log parser in place of awk here, but the idea would be the
same.

Example output:

    CREATE TABLE Requests (
       SourceIP VARCHAR(20),
       User VARCHAR(20),
       URI VARCHAR(256),
       Status VARCHAR(4),
       Size INT(7)
    );

    INSERT INTO Requests (SourceIP, User, URI, Status, Size) VALUES
        ("127.0.0.1", "frank", "index.htm", 200, 136);
    INSERT INTO Requests (SourceIP, User, URI, Status, Size) VALUES
        ("192.168.0.1",  "bob", "index.htm", 200, 136);
    INSERT INTO Requests (SourceIP, User, URI, Status, Size) VALUES
        ("127.0.0.1", "frank", "kart.htm", 200, 15378);

The resulting file could be imported into a SQL database.


## Finding active customers (with a join)

XXX

## Hadoop

XXX

## Build system

Marlin could be an effective distributed build system, at least for Joyent
internally.  Using Marlin this way guarantees that each repo is built in a clean
zone with whichever dataset it requires, the builds themselves are automatically
distributed for parallelism, and the artifacts end up in Manta where we probably
want them anyway.

The main challenge is that git repos themselves would be hard to store in Manta,
since they're typically a whole tree of files presumably updated with partial
writes.  But git repos can be bundled into a single file using "git bundle", and
the resulting bundle stored in Manta.  If we add a post-commit hook to our git
repos that save a "git bundle" into Manta, then the distributed build system is
a pretty simple map job:

    {
        "jobName": "build SDC",
        "phases": [ {
            "dataset": "smartos-1.6.3",
            "exec": "git clone $mr_input_file repo && cd repo && make &&
                dpipe < build/dist/latest.tar.bz2"
        } ]
    }

The input keys would be the keys identifying the git bundle for each repo in
Manta.  If different repos needed to be built with different datasets, those
would have to be separate jobs.


# Task Control API

The Task Control API is used by tasks (user code running in compute zones on
Manta storage nodes) to retrieve parameters and report on progress.  The API is
serviced not by a public endpoint, but by a small server running inside the
task's own compute zone that's implicitly scoped to that zone's current task.
As a result, task identifiers are not actually necessary in the task API.

This API is *not* used to monitor task status; that's provided by the
public-facing Job API.

## GET /info

Retrieves information about the current job and task.

### Returns

This field returns the entire job info (as `GET /jobs/:jobid` does) plus the
following fields:

|| **Field** || **Type** || **Description** ||
|| taskId || String || Task identifier ||
|| taskInputKey || Array&nbsp;of&nbsp;Strings || An array with two fields: the object key, and the full path to the corresponding file. ||

Object metadata (e.g., user-specified headers) are not included in the task
info.  Users can retrieve this by querying Manta directly.
 

## POST /commit

Report that the given key has been successfully processed.  Tasks *must* report
when they finish processing each key.

This information is used for two purposes: first, it is recorded in the job
state so that end users can see what progress is being made while the job runs.
Second, any objects created since the last progress report are committed to the
job.  See `PUT /object/:key`.

When the key has been completed, the task may be terminated immediately,
potentially before this request even completes.  Any cleanup the task wishes to
complete should be done before the final call to this URI.

### Input

|| **Field** || **Type** || **Description** ||
|| key || String || Key for input object successfully completed ||

### Returns

Returns 204 (no content) on success.

### Example

Request:

    {
        "key": "logs/2012-03-11.log"
    }

Response:

    HTTP/1.1 204 No content
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:19:26 GMT
    X-Request-Id: 4bcf467e-4b88-4ab4-b7ab-65fad7464de9
    X-Response-Time: 32


## PUT /object/:key

Writes a new Manta object named "key" associated with this task.  The arguments,
semantics, and return value for this URI exactly match the Manta API for
creating a new object, with the following additions:

* Any object created using this URI since the last call to `POST /commit` will
  be discarded if the job fails.  Such objects will not be included in the
  job's output, but will be listed in the task state for *postmortem*
  debugging.
* Jobs using pipelined reducers may specify a "X-Marlin-Reducer" header whose
  value is an integer from 0 to N - 1, where N is the number of reducers in the
  next phase.  Any other input results in a 409 Conflict error.  This value
  indicates which of several reducers this output file should be fed to.  If
  this value is unspecified and there are multiple reducers in the next phase,
  output keys will be randomly assigned to reducers.

For examples, see the Manta documentation.

## POST /fail

Fail the current task.  Any objects created since the last call to `POST
/commit` will be discarded (but will be linked in the task state for
*postmortem* analysis).

### Input

The input should have at least the following fields:

|| **message** || String || human-readable error message ||

It's also recommended to include a "code" field for a machine-parseable error
code in string form.  Other fields may also be specified up to a maximum
request size.

### Returns

Returns 204 (no content) on success, but may not return at all, as the task may
be terminated immediately.

Callers can assume that this call will never fail.


### Example

Request:

    {
        "message": "corrupt log record"
    }

Response:

    HTTP/1.1 204 No content
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:19:26 GMT
    X-Request-Id: 4bcf467e-4b88-4ab4-b7ab-65fad7464de9
    X-Response-Time: 12

## POST /abort

Exactly like `POST /fail`, except that additional *postmortem* state will be
kept around and linked to the job state.  Core dumps of currently running user
processes and the entire filesystem contents will be preserved.


# Implementing use cases

## Built-in facilities

We'll provide several facilities for dealing with common tasks in MR jobs.

### dpipe: output to the next job pipeline

dpipe [&lt;manta_key&gt;] [reducer]

Reads content from stdin and outputs it from this task.  If &lt;manta\_key&gt;
is not given, a new name will be created.

This is exactly equivalent to:

    $ curl -X PUT -T- http://localhost/object/$manta_key

If "reducer" is specified, it must be an integer from 0 to N - 1, where N is the
number of reducers in the next stage of the job pipeline.  This indicates that
this object should be routed to the given reducer.


### aggr: aggregate rows of numbers

aggr &lt;sum|avg|pctile99...&gt;

Reads a set of "key: value" pairs on stdin, aggregates them key-wise according
to the given function, and emits the result.

Example input:

    $ cat data.txt
    manta: 15
    marlin: 12.8
    manta: 38
    moray: 3

Example output:

    $ aggr sum < data.txt
    manta: 53
    marlin: 12.8
    moray: 3

The "key:" bit may also be omitted.


## Pipelined reducers

Sometimes it's necessary to pipeline the reduce phase so that instead of
processing all input keys in one pass, the input keys are processed in chunks
across multiple passes.  For example, you may want to process half of the input
keys in each of two parallel tasks, and then process the output of that, to
avoid having to load the entire data set in memory at once.

We already support multiple "reduce" phases in a job, but to support this you
also need to parallelize a single reduce phase.  You can do this with the
"count" property:

    "phases": [
        ... /* any number of map phases */
        , {
            "type": "reduce",
            "exec": ...,
            "count": 2
        }, {
            "type": "reduce",
            "exec": ...
        }
    ]

In this example, some number of map phases are followed by a phase with two
reducers running in parallel, followed by a final reduce stage.  The two
parallel reducers are identical except for their input and output keys.  By
default, input keys are randomly assigned to one of the reducers.  If a more
sophisticated assignment is necessary, the previous map phase may specify to
which reducer a given output object will be assigned using the X-Marlin-Reducer
header, which must be an integer between 0 and N - 1, where N is the total
number of reducers in the next phase.


## Node, Python, Ruby, etc. interface

The Jobs API is built in terms of Unix shell commands for generality.  This
makes it easy to run simple jobs like "grep" and "wc".  For slightly more
complex cases, custom utilities can be built in the Unix style and composed with
existing system utilities without writing significant new code.  For more
complex cases you want entirely custom code, you can simply run your program via
the "exec" string.

We'll also provide a higher-level interface for Node.js, so that people that
want to build jobs with custom code can use Node easily.  Likely we'll have
consumers implement an interface like:

    function processObject(file, callback) { ... }

and we'll provide convenience routines for API operations (like "taskParams()",
"commit()", "abort()", and so on).

Without fleshing this out completely, we know such an interface can be
implemented without changes to the Jobs API.  When people want to use the Node
API, we tell them to write phases like this:

    "phases": [ {
        "assets": [ "my/npm/package" ],
        "exec": "mnode /assets/my/npm/package"
    } ]

"mnode" is an executable Node.js program included with the dataset that installs
the given npm package (and therefore its dependencies, too) and then invokes our
Marlin interface in that package.

If we care to, we can do something similar for Python, Ruby, R, or any other
environment.  It would be just as easy for third parties to build their own.
Users would only need to reference a third party asset:

    "phases": [ {
        "assets": [
            "third-party/framework/start.sh"
            "my/script"
        ],
        "exec": "/assets/third-party/framework/start.sh /assets/my/script" 
    } ]


# Marlin implementation

## Network architecture

Marlin makes use of two generic Manta components:
* muskie, the Manta web tier, which handles the Jobs API (requests to POST new
  jobs and GET job state).
* moray, where all Marlin state is stored.

On top of these, Marlin adds two components:

* Job worker tier, which locates new and abandoned jobs in Moray, assigns task
  groups to individual storage and compute nodes, and monitors job progress.
* Compute node agents, which execute and monitor tasks and report progress and
  completion.

There's a cluster of web tier workers and job workers in each datacenter, but
there's no real DC-awareness among these components.

**Job state**

To keep the service resilient to partitions and individual node failures, **all
job state is stored in Moray** (i.e., all components are essentially
stateless), and **all communication happens via Moray**.  Rather than talking
to each other directly, each component drops state changes into Moray and polls
for whatever other state changes it needs to know about.  This model makes it
relatively easy to deal with individual component failure and to scale
horizontally.  See "Invariants and failure modes" below for details.

Recall that Moray spans datacenters, is eventually consistent, and remains
available in the case of minority DC partitions.  Using Moray in this way also
implies that Moray becoming unavailable makes Marlin unavailable, and that
Moray must be able to keep up with the Marlin workload.  See "Scalability"
below.

Jobs are stored inside a single Moray bucket called "marlinJobs", with indexed
fields that allow the web tier, job workers, and agents to poll on the subset of
jobs that they care about.


**Web tier**

muskie retrieves current job state by querying "marlinJobs" for the job with the
given jobId.

When a new job is submitted, muskie allocates a new jobId and drops the job
definition into "marlinJobs".


**Job workers**

To synchronize changes to job state, each job is assigned to one worker at a
time.  This assignment is stored in the job object in Moray.  An assignment is
valid as long as the job's modification time (mtime) is within the last
WORKER\_TIMEOUT seconds.

This works as follows: each job worker periodically queries Moray for jobs
whose state is not "done" and either (a) the "worker" field is undefined or (b)
the modification time is more than WORKER\_TIMEOUT seconds ago.  (This is a
relatively simple query on indexed fields.)  For each job the worker finds, it
uses a test-and-set operation to assign the job to itself.  In this way,
workers pick up both new jobs and jobs whose assigned workers appear to have
failed.  A worker must update each job every WORKER\_TIMEOUT seconds, even if
there's no meaningful progress to report, to indicate that the worker is still
handling that job.  If the worker sees that WORKER\_TIMEOUT seconds has elapsed
since it last updated the job, it must discard any job state it has and stop
working on the job to avoid conflicting with another worker who picks up the
job.  (In practice, the best way to implement this may simply be to have all
state updates use a test-and-set that checks whether worker == me.)

When a worker picks up a newly submitted job, the first thing it has to do is
query Moray to find out where all the input keys are.  Moray itself has many
independent partitions and uses a consistent hash ring to map a given key to
the Moray partition that holds the metadata for that key.  The worker takes all
the input keys, executes the consistent hash function to group them by Moray
partition, and performs a batch request to each partition to determine which
storage nodes each key is actually stored on.

The worker than divides the first phase of the job into **task groups**, each of
which will run on one storage node and process whichever input keys are stored
on that node.  These task assignments are written out to a Moray bucket called
"marlinTaskGroups" with a field identifying the hostname of the node on which
the task will run.  The agents running on each node poll Moray for tasks
assigned to them, process each one, and update the task object.  The job worker
polls on these task updates, updates the global job state with job progress (so
the web tier can see it) and advances the job when each phase completes.

When a phase completes (as determined by the worker when all of its tasks have
completed), the worker begins processing the next phase in the same way it did
the first one: by locating the objects within Manta and then assigning tasks to
individual nodes.  When all phases complete, the job is marked done.

**Compute node agents**

As described above, compute node agents monitor tasks assigned to them in
"marlinTaskGroups".  As their tasks make progress, they update the task record,
which is also monitored by a job worker.

**Invariants and failure modes**

Invariants:

* All state is stored in Moray, so all components are stateless.
* At most one worker can ever "own" a job.  Only the owner of a job can write
  task assignments or update the job record in "marlinJobs".
* To a first approximation, a component is partitioned iff it cannot reach
  Moray.  This is an oversimplification because Moray itself is sharded, so it's
  theoretically possible to be partitioned from one shard but not another.
  This case is very unlikely, since each shard has a presence in each DC, but
  the system will handle this.  Most importantly, the only partitioning that
  matters is that between a component and a given Moray shard.

If a user's code fails within a task (e.g., dumps core), and the on-node retry
policy has been exhausted, the compute agent reports the task as failed, and
that will become visible in the final job state.  It's the user's problem at
that point.

If a compute node agent fails transiently (e.g., dumps core and restarts), it
should be able to pick up where it left off.  This should be invisible to the
user's code and the rest of the system, modulo slightly increased task
completion time.  (The agent must synchronously write state to local disk to
facilitate this.  If this storage fails, that's equivalent to the compute node
agent failing persistently.)

If a compute node agent either fails for an extended period *or* becomes
partitioned from its Moray shard (which are indistinguishable to the outside
world), the job worker redispatches any uncompleted work to other nodes that
have the same data.  It does not cancel the task on the failed node.  If both
tasks eventually complete successfully, the worker will necessarily see one's
results first and discard the other's.  (That's why the state in
"marlinTaskGroups" is not authoritative job state, and why the worker must
"commit" this state to the job record in "marlinJobs" for the web tier to see
it, rather than having the web tier scan "marlinTaskGroups" for itself.)

If a job worker fails transiently, it should be able to pick up where it left
off, since all state is stored in Moray.  This should be invisible to the rest
of the system, modulo slightly increased job completion time.

If a job worker fails for an extended period, another job worker will pick up
its jobs.  This becomes exactly like the case where a worker fails transiently:
the state in Moray is sufficient for the new worker to pick up where the old one
left off.

If a job worker becomes partitioned from one or more Moray shards, then it will
be unable to process task updates from compute nodes reporting to those shards.
It may end up trying to redispatch tasks assigned to those nodes to other nodes.
If it can talk to enough shards to complete the phase, the phase will complete;
otherwise, it will stall until connectivity is restored, at which point the
worker will see the completed task state and finish the phase.

If a job worker becomes partitioned from the Moray shard storing the job state
for WORKER\_TIMEOUT seconds, this will be treated as an extended worker failure.
Another worker will pick up the job where the first one left off.  The first
worker won't clobber the job state after someone else has picked up the job
because it also knows that WORKER\_TIMEOUT has elapsed.

Web tier nodes are completely stateless.  Any failure results in failing any
in-flight requests.  Persistent failures have no additional impact as long as
enough nodes are in service to handle load.  Partitions between the web node and
the Moray shard storing job records would result in failed requests.

If an entire datacenter becomes partitioned from the rest, then its web tier
nodes certainly won't be able to service any requests.  All of its job workers
become partitioned (see above -- the work should eventually be picked up by
workers in other datacenters).  All of its compute node agents effectively also
become partitioned, and their work *may* be duplicated in other datacenters.
Importantly, jobs submitted to the majority partition should always be able to
complete successfully in this state, since there's always enough copies of the
data to process the job.

**Scalability**

Being stateless, web tier nodes can be scaled arbitrarily horizontally.  If load
exceeds capacity, requests will queue up and potentially eventually fail, but
alarms should ring long before that happens.

Similarly, job workers can be scaled arbitrarily horizontally, and capacity
monitoring should allow administrators to ensure that sufficient capacity is
always available.  The failure mode for excess load is simply increased job
completion time.  Task updates may accumulate in Moray faster than the worker
can process them, but the total space used in Moray is the same regardless of
how fast the job workers process them, so there's no cascading failure.  If job
workers cannot update their locks in Moray, the system may thrash as workers try
to take each others' jobs, but this would be very extreme.

Compute nodes rely on the OS to properly balance the workload within that box.
They may queue tasks after some very large number of concurrent tasks, but it's
not anticipated that we would ever reach that point in practice.  If this
became a problem, job workers could conceivably redispatch the task to another
node with the same data available.  If this truly became a serious problem, we
could support compute agents that download data, rather than operating locally
(just as reducers already do).

Given that there's enough capacity in each of the Marlin tiers, the remaining
question is Moray.  Moray will be sharded, and we will say that each compute
node reports its tasks to exactly one (uniformly distributed) shard.  All job
state will be stored on a single shard, since the number of updates is
relatively minimal.  We should be able to reshard to scale this out.

Additionally, each compute node can batch its Moray updates from all tasks for
up to N seconds, sharply reducing the number of requests required.  Similarly,
job workers can batch job state updates for many jobs up to N seconds.  Batching
of "location" GET requests is discussed above, under "job workers".  Thus, the
total number of Moray-wide requests should be boundable to something like (# of
workers \* # of compute zone agents) / (N seconds), where N is how frequently
each agent updates Moray.  These requests should mostly be distributed across
all shards.

Batching requests for the same record over multiple seconds reduces both network
throughput and database operations, as multiple state changes can be combined at
the source.

Batching requests over multiple records helps network throughput, but not the
total number of database operations, since each record must be operated on
separately.  The total number of database operations could still be around (# of
active tasks \* # of agents \* # of active jobs \* # of job workers).  Again,
these are distributed across many Moray shards, which can be scaled
horizontally.

## Objects

There are two types of objects stored in Moray: jobs and task group
assignments.  For the details, see the schema in lib/schema.js in marlin.git.

Web tier:

* POST /jobs (create new job)
* GET /jobs/jobId (get job status)
* PUT /jobs/jobId (rare case: use T&amp;S to set cancelled = true)

Workers:

* GET /jobs where worker == null or mtime &lt; ... (pick up new jobs)
* PUT /jobs/jobId (update job state)
* PUT /taskgroups/taskGroupId (create/update assignment)
* GET /taskgroups/taskGroupId (check task group state)

Agents:

* GET /taskgroups where host == me (pick up new task groups)
* GET /taskgroups/taskGroupId (check task group state)
* PUT /taskgroups/taskGroupId (post task group results)


## Cancellation

To cancel a job, the web tier writes "cancelled: true" to the job record.  At
this point, we could say that the job state is immutable and all future state
requests only show whatever state has been completed up to this point.

Asynchronously, the job worker will need to be polling on the job state
occasionally to notice the cancelled flag.  It propagates this to individual
task assignments, where the compute node agents will notice that the job has
been cancelled and abort them.  Eventually, all work on the job will complete,
though this may take some time to propagate through the system.


## CN agent

As described above, compute node agents receive task assignments via Moray and
report progress back to Moray.  Such progress includes keys processed and state
changes, including that a task is queued, loaded, running, timed out, failed, or
done.)

The agent maintains a pool of compute zones that are either "uninitialized",
"ready", or "busy".  Uninitialized zones are those currently in an arbitrary
state.  Those are asynchronously transitioned to "ready" by the following steps:

1. Halt the zone, if it's not already halted.
2. "zfs rollback" to the zone's origin snapshot
3. Boot the zone.
4. Sets up an agent inside the zone that listens for local HTTP requests to the
   Task Control API and forwards them via a zsock server to the CN agent.

When a new task is dispatched, if there are no "ready" zones available, tasks
are enqueued for later.  Otherwise, the agent picks a "ready" zone and does the
following:

1. Downloads the job's bundle's assets into the zone.
2. Creates a hyprlofs mount for the files.
3. Finally, invokes the bundle's exec script.

When the job completes, successfully or otherwise, the CN agent reclaims the
zone by marking it "uninitialized" again and asynchronously transitioning it to
"ready" again to be used for another task.

To implement the Task Control API:

`/info` is simple: it just retrieves data from the agent's persistent state,
which already has to exist to manage the job.

`/objects/:key` makes a note in the task state that this object is being
written, and then (first approach) proxies the request directly to manta. But
see "open questions" below.

`/commit` updates the persistent state for the task to indicate that all
objects written since the last checkpoing have been committed.

`/fail` also updates the persistent state, but indicates that new objects should
be discarded.  They won't actually be deleted, but marked as discarded in the
job state so users can look at them later.  After that, this request terminates
the zone.

`/abort` is like `/fail`, except that before terminating it gcores all processes
in the user's contract and does an incremental "zfs send" of the zone to a new
manta file.  (We'll want some automated mechanism to reconstruct the user's zone
based on this, presumably by "zfs receive" the dataset and then "zfs receive -i"
the stream associated with this task.)

In order to survive transient failure, the agent must store state on disk.  The
current plan is to use sqlite for this because it already has usable Node
bindings.

## Open questions

Roughly in order of importance:

* When user code wants to write files to manta, how exactly does that work? Do
  we have a way to tell Manta to write one copy on the local box, and if so, do
  we avoid sending it across the network twice?
* What metrics do we want to provide via CA?  How will we implement these?
  What other changes must be made to CA to support Marlin and Manta?
* Should the job log be a manta object so that user's can add their own entries?

## Management API?

cancel tasks, take compute zones and servers in and out of service
initial setup: creating compute zones
tuning # of compute zones

## Rough roadmap

The basic idea is to get a prototype up and running relatively quickly and
build that out as needed.  This is a *very* rough draft of how to get there.

Today we have:

* a Moray implementation that supports GET/DEL/PUT (with test-and-set) and
  search, as well as a client-side library that supports throttling.
* a mock Moray implementation, at least for testing the job worker
* an agent that takes task group specifications and executes each task in the
  group, including support for assets and both "generic" and "map" tasks, using
  hyprlofs to map files in.
* a job worker that picks up unassigned jobs in Moray, executes them, and
  can pick up partially-completed jobs.  Also times out idle task groups.
* a very simple web tier that just drops jobs into Moray and reads state back.

Remaining work not includes:

* moray: search by bucket change number
* moray client: support for consistent hashing
* agent: automated tests
* agent: implement "abort" (which saves all zone state)
* agent/worker: implement explicit fail
* agent/worker: implement retry
* agent/worker: reducers (see below)
* all: Moray record versioning
* all: implement cancel

plus work covered by existing MANTA tickets.

## Reducers

Reducers are mainly different from mappers in three ways:

1. Reduce tasks operate on multiple keys at once, not just one.
2. Reduce nodes operate on data stored elsewhere in Manta, which means it must
   be downloaded first.  (We'd actually like to be able to do this for mappers
   too.)
3. Reduce tasks can run in any compute zone, which means we need a mechanism for
   discovering and choosing compute zones to run reduce tasks.

Here's the current plan:

* For testing purposes: have the worker assign reduce tasks to any host that it
  knows about, and by writing the same taskgroup record it would use for "map",
  except that there's a type field indicating "reduce".  Doing this first lets
  us use the rest of Marlin to iterate on the agent, where the bulk of the
  changes will be made.
* Modify the agent to support "reduce" tasks by processing them just as it would
  "generic" tasks except that all keys are processed in one invocation.
* Modify the agent to support downloading input files rather than mapping them
  in.  This should be supported for regular map tasks, too.  (This comes after
  adding "reduce" tasks because the implementation for map tasks alone may not
  be generalizable for reduce tasks.)
* Once this works, modify the agent to not specify input keys on the command
  line for reduce tasks.  (See below.)
* Implement a real zone scheduling algorithm.
* Rework agent/worker communication to use something other than the existing
  task group interface for assigning work, then stress test with large numbers
  of keys.


### Multiple keys at once

In order to process multiple keys at once, several pieces of both the global
zone agent and in-zone agent must be generalized because they assume only one
input key at a time, and that a sequence of input keys (tasks) make up a
complete task group.  These changes are straightforward.

There are some more serious scalability issues in processing up to millions of
keys at once:

1. Interface: It will be difficult to support a file-based (as opposed to
   stdin-based) interface for actually invoking the reducer, since command lines
   cannot support millions of arguments.  If we want to support a file-based
   interface, we may have to specify the keys on stdin or some other file that's
   passed as a command line argument.  There are other problems with this,
   though; see below.
2. Architecture: Today, the task group is the main unit of work inside the
   agent, and there's no way to share state across task groups.  But task groups
   are passed between worker and agent via Moray as JSON objects that must be
   parsed and stringified on both ends, stored in memory, and updated in
   entirety.  That's not going to scale well with millions of keys.


### File access

In order to deal with very large numbers of very large files, we'll need to
stream data when possible.  In the worst case, reducers may not be able to
physically store all of the data they will need to process.  But even in less
severe cases it may be a grossly inefficient use of both memory (valuable ARC
space) and disk to store all of the data at once when it could just as well be
streamed.

Streaming is relatively straightforward: the agent downloads input files and
pipes them over stdin to the reducer, which is invoked with no actual command
line arguments.  The agent may as well download the files sequentially, only
parallelizing to start the next file when the current one is near completion.

People may want access to files by name.  Lots of custom programs take
filenames and don't work on stdin/stdout.  And it's not hard to imagine reduce
tasks that by nature require random access to all of the input files.  This is
hard, since we must wait to start the reducer until all of the input file(s)
have been completely downloaded.  We can probably punt on this in the short
term.  We can always add new types of tasks in the future (e.g.,
"reduce-byfile") with different semantics, since such an abstraction is clearly
pretty different from streaming reduce.


### Scheduling compute zones

One obvious approach is to use CNAPI to provision and deprovision compute zones
on the general cloud as we need them.  But since we don't assume Marlin is
available on all nodes, this would require a way to specify that we only want to
provision on Marlin zones.  (XXX is that part of server features?)
Additionally, it also means that the job worker has to tell the agent about a
new zone that it didn't know about, that it should set it up as a Marlin zone,
and then run a given task group.  This feels like a gross conflation of
concerns, but on the other hand, we probably want the ability to add and remove
capacity dynamically anyway, so maybe those are two separate operations: add
capacity in the form of newly created zone Z, and dispatch a task to zone Z.
This still involves the worker knowing what resources are available to the agent
and picking among them, which seems to conflate concerns.

Alternatively, we say that reducer zones are provisioned just like map zones:
they're provisioned externally, and the agent maintains a pool of zones it knows
it can use.  Much like the way storage zones report capacity, each agent
maintains a record in Moray describing its *total* capacity broken down by
dataset (e.g., smartos-1.6.3) and whatever other immutable zone characteristics
users may express constraints on.  Then workers assign tasks to agents that
appear to have capacity, and agents can resize zones as needed to make the best
use of their available zones.


### Scaling records

Instead of using taskgroups, we may need to switch to more fine-grained records.

To start with, the records that we call task groups today would be narrowed in
scope to cover exactly one input key.  Everything else is the same: they specify
the phase details (possibly as job + phase number for efficiency), destination
host, state ("dispatched", "running", "cancelled", "done"), assigned machine,
result ("ok" or "fail"), timing information, and error details.

The result of processing each input key is written back to the same record, but
each emitted output key is written out to a new record containing an identifier
for the input record (which implicitly specifies a job, phase, host, and so on)
and a single output key.

Semantically, everything works the same as it does today, but streaming lists of
these things from the new Moray instead processing monolithic records.
