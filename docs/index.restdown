---
title: Marlin
markdown2extras: wiki-tables, code-friendly
apisections: Task Control API
---

# Marlin

Marlin is a facility for running compute jobs on Manta storage nodes.  This
document describes the desired end state for version 1.


# Overview and Public API

For an overview of what Marlin is and how users interact with it, see the
documentation in muskie.git.  Really, go read that first.  This document assumes
familiarity with that, including how the rest of the Manta object store works.


# Design goals

Marlin is designed to facilitate the following use cases:

* MapReduce in a Unix-based environment (e.g., log analysis using awk)
* MapReduce in a node-based environment (i.e., make it easy for node apps)
* Straight Map with no reduce (e.g., video transcoding)
* Allowing others to run compute on your data (e.g., for government
  organizations with lots of public data, e.g.,
  http://community.topcoder.com/pds-challenge/)
* Hadoop itself?

Marlin must be easy to use, monitor, and debug.  A good barometer is that "word
count", the MR equivalent of "hello world", should be trivial to run from the
command line.

# APIs

The **Job API** is what end users use to submit, cancel, and monitor compute
jobs.  This is actually part of the Manta API, and is documented with the rest
of Manta.  The rest of this document assumes familiarity with this API.

The **Task Control API** is used by user code running inside Marlin to
coordinate execution and report progress.  It's not available externally,
since it's scoped to a particular zone.  This is discussed in detail below.

All DateTimes in these APIs use W3C DTF in UTC ("YYYY-MM-DDThh:mm:ss.sZ").


# Example jobs

The following examples include whitespace for readability, but JSON does not
allow whitespace within strings.  To use these examples, either remove the
newlines or save the phase contents as a file in Manta and use that file as an
asset.

## Total word count

Input: a list of plain text objects (e.g., plaintext mailbox files, source code)

Output: a single object with one line containing the total number of characters,
words, and lines in all files (e.g., "wc" output)

    {
        "jobName": "total word count",
        "phases": [ {
            "exec": "wc"
        }, {
            "type": "reduce",
            "exec": "awk '{ l += $1; w += $2; c += $3 } END { print l, w, c }'"
        } ]
    }

Example output (exactly the same format as "wc"):

      122919  380672 3416475


## Word frequency count

Input: a list of plain text objects.

Output: a single object with one line *per whitespace-delimited* word in the
input objects identifying how many times that word occurred in all of the input
files.  The result should be sorted by count.

    {
        "jobName": "word frequency count",
        "phases": [ {
            "exec": "awk '
                {
                    for (i = 1; i < NF; i++) {
                        counts[$i]++
                    }
                }
                END {
                    for (i in counts) {
                        print i, counts[i]
                    }
                }'
            "
        }, {
            "type": "reduce",
            "exec": "awk '
                {
                    byword[$1] += $2
                }
                END {
                    for (i in byword) {
                        print i, byword[i]
                    }
                }' | sort -k2,2 -n"
        } ]
    }

Example output:

    aardvark: 37
    broomstick: 318
    mÃ©tier: 1

## Line count by file extension

Input: a list of text files (e.g., source code)

Output: a total count of lines by file extension

    {
        "jobName": "total lines by file extension",
        "phases": [ {
            "exec": "wc -l $mr_input_key |
                (read count file; echo $count ${file##*.}",
        }, {
            "type": "reduce",
            "exec": "awk '
                {
                    byword[$2] += $1
                }
                END {
                    for (word in byword) {
                        print byword[word], word
                    }
                }'
            "
        } ]
    }

Note: we use $mr\_input\_key rather than reading from stdin so that "wc" prints
out the filename after the word count.

Example output:

    12992 py
    15773 md
    32782 h
    122919 js
    186279 c

## Word index

Input: a list of text files (e.g., plaintext emails)

Output: a file with one word per line indicating which files and lines the word
appears in the input files.

Example output:
aardvark mail0100.txt:12,13
curmudgeon file1.txt:30 file2.txt:10,12,14 file3.txt:15

    {
        "jobName": "word index",
        "phases": [ {
            "exec": "awk '
                {
                    for (i = 1; i <= NF; i++) {
                        if ($i in indx) {
                            indx[$i] = indx[$i] "," NR
                        } else {
                            indx[$i] = FILENAME ":" NR
                        }
                    }
                }
                END {
                    for (word in indx) {
                        print word, indx[word]
                    }
                }'
            "
        }, {
            "type": "reduce",
            "exec": "awk '
                {
                    for (i = 2; i <= NF; i++) {
                        indx[$1] = indx[$1] " " $i
                    }
                }
                END {
                    for (word in indx) {
                        print word, indx[word]
                    }
                }'
            "
        } ]
    }

Example output:

    And  macbeth.txt:6,10 hamlet.txt:4,25,28,30,32
    Is  hamlet.txt:29
    Ophelia!  hamlet.txt:33
    Signifying  macbeth.txt:12
    by  macbeth.txt:11 hamlet.txt:4,5
    for  macbeth.txt:2
    regard  hamlet.txt:31
    shadow,  macbeth.txt:8
    such  macbeth.txt:2
    there's  hamlet.txt:9

## Image conversion

Input: list of arbitrary image files

Output: list of same images converted to PNGs

Assumes: ImageMagick package installed in the zone

    {
        "jobName": "convert images to PNGs",
        "phases": [ {
            "exec": "convert $mr_input_file ${mr_input_file##*.}.png &&
                mpipe < ${mr_input_file##*.}.png"
        } ]
    }

The output here is a list of keys of the converted images, as in:

    img001.png
    img002.png

## Video transcoding

Input: list of arbitrary video files

Output: list of same videos encoded with webm using default ffmpeg presets

Assumes: ffmpeg package installed in the zone

    {
        "jobName": "transcode video to webm",
        "phases": [ {
            "exec": "ffmpeg -i $mr_input_file ${mr_input_file##*.}.webm &&
                mpipe < ${mr_input_file##*.}.webm"
        } ]
    }

The output here is a list of keys of the transcoded videos, as in:

    video001.webm
    video002.webm

## Ad-hoc Apache log query

Input: list of logs in Apache common log format

Output: count of all queries decomposed by URI

Assumes: apache2json tool, which converts each Apache common log record into a
    JSON object with named fields

    {
        "jobName": "count HTTP requests by URI"
        "phases": [ {
            "exec": "apache2json | json -a url"
        }, {
            "type": "reduce",
            "exec": "sort | uniq -c | sort -n"
        } ]
    }

Example output:

       3 /apache_pb.gif
       4 /kart.htm
      13 /index.htm

## Data warehousing with an intermediate database

Input: list of logs in Apache common log format

Output: SQL database describing all of the HTTP requests

    {
        "jobName": "aggregate HTTP requests into a SQL database",
        "phases": [ {
            "exec": "awk '{
                print \"INSERT INTO Requests (SourceIP, User, URI, Status, Size)
                  VALUES (\" $1 \",\" $3 \",\" $7 \",\" $9 \",\" $10 \");\" }'"
        }, {
            "type": "reduce",
            "exec": "echo 'CREATE TABLE Requests (
                SourceIP VARCHAR(20),
                User VARCHAR(20),
                URI VARCHAR(256),
                Status VARCHAR(4),
                Size INT(7)
            );'; cat"
        } ]
    }

Note: this assumes URIs have no spaces in them.  A more robust solution would
use an actual Apache log parser in place of awk here, but the idea would be the
same.

Example output:

    CREATE TABLE Requests (
       SourceIP VARCHAR(20),
       User VARCHAR(20),
       URI VARCHAR(256),
       Status VARCHAR(4),
       Size INT(7)
    );

    INSERT INTO Requests (SourceIP, User, URI, Status, Size) VALUES
        ("127.0.0.1", "frank", "index.htm", 200, 136);
    INSERT INTO Requests (SourceIP, User, URI, Status, Size) VALUES
        ("192.168.0.1",  "bob", "index.htm", 200, 136);
    INSERT INTO Requests (SourceIP, User, URI, Status, Size) VALUES
        ("127.0.0.1", "frank", "kart.htm", 200, 15378);

The resulting file could be imported into a SQL database.


## Finding active customers (with a join)

XXX

## Hadoop

XXX

## Build system

Marlin could be an effective distributed build system, at least for Joyent
internally.  Using Marlin this way guarantees that each repo is built in a clean
zone with whichever dataset it requires, the builds themselves are automatically
distributed for parallelism, and the artifacts end up in Manta where we probably
want them anyway.

The main challenge is that git repos themselves would be hard to store in Manta,
since they're typically a whole tree of files presumably updated with partial
writes.  But git repos can be bundled into a single file using "git bundle", and
the resulting bundle stored in Manta.  If we add a post-commit hook to our git
repos that save a "git bundle" into Manta, then the distributed build system is
a pretty simple map job:

    {
        "jobName": "build SDC",
        "phases": [ {
            "dataset": "smartos-1.6.3",
            "exec": "git clone $mr_input_file repo && cd repo && make &&
                mpipe < build/dist/latest.tar.bz2"
        } ]
    }

The input keys would be the keys identifying the git bundle for each repo in
Manta.  If different repos needed to be built with different datasets, those
would have to be separate jobs.


# Task Control API

At the lowest level, the user code running as a task is managed by an in-zone
agent that's part of Marlin.  This agent runs an HTTP server that communicates
securely with the global zone agent, which proxies most requests directly to the
Manta front door.  Since the global zone agent knows which zone it's talking to,
all requests can be automatically authenticated for the user running the job.
This enables user code to make authenticated requests to operate on Manta
objects without having to worry about secure keys.

In addition to proxying the full Manta API, the in-zone server supports a few
additional entry points that are used to pass parameters and control information
between the global zone agent and the in-zone agent.  To avoid conflicting with
anything else in Manta, these URIs are prefixed with `/my/jobs/task`.  (As
described above, the global zone agent always knows which job and task are being
operated on since it knows which zone made the request.)

* `GET  /my/jobs/task/task`: retrieve job and task parameters
* `POST /my/jobs/task/live`: report task liveness
* `POST /my/jobs/task/commit`: report successful task completion
* `POST /my/jobs/task/fail`: report task failure

These entry points are not for direct consumption by user code, and are not
available at all outside the context of a task running inside a compute zone.
These APIs are *not* used to monitor task status; that's provided by the
public-facing Job API.


## GET /my/jobs/task/task

Retrieves information about the current job and task.

### Returns

This field returns the entire job info (as `GET /jobs/:jobid` does) plus the
following fields:

|| **Field** || **Type** || **Description** ||
|| taskId || String || Task identifier ||
|| taskPhase || Object || See "job" object in Jobs API ||
|| taskErrorKey || String || Suggested key for task stderr object, if any ||
|| taskOutputKey || String || Suggested key for first task output object ||
|| taskOutputBase || String || Suggested base key for task output objects ||
|| taskInputKeys || Array of Strings || List of keys to process ||
|| taskInputDone || Boolean || Whether all input keys have been passed (reduce only) ||
|| taskInputRemote || String || URL of Manta for use in fetching records (non-local data only) ||
|| taskInputFile || String || Filename of first input key (local data only) ||

Object metadata (e.g., user-specified headers) are not included in the task
info.  Users can retrieve this by querying Manta directly.


## POST /my/jobs/task/commit

Report that the given key has been successfully processed.  Tasks *must* report
when they finish processing each key.

This information is used for two purposes: first, it is recorded in the job
state so that end users can see what progress is being made while the job runs.
Second, any objects created since the last progress report are committed to the
job.  See `PUT /:key`.

When the key has been completed, the task may be terminated immediately,
potentially before this request even completes.  Any cleanup the task wishes to
complete should be done before the final call to this URI.

### Input

|| **Field** || **Type** || **Description** ||
|| key || String || Key for input object successfully completed ||

### Returns

Returns 204 (no content) on success.

### Example

Request:

    {
        "key": "logs/2012-03-11.log"
    }

Response:

    HTTP/1.1 204 No content
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:19:26 GMT
    X-Request-Id: 4bcf467e-4b88-4ab4-b7ab-65fad7464de9
    X-Response-Time: 32


## PUT /:key

Writes a new Manta object named "key" associated with this task.  The arguments,
semantics, and return value for this URI exactly match the Manta API for
creating a new object, with the following additions:

* Any object created using this URI since the last call to `POST /commit` will
  be discarded if the job fails.  Such objects will not be included in the
  job's output, but will be listed in the task state for *postmortem*
  debugging.
* Jobs using pipelined reducers may specify a "X-Marlin-Reducer" header whose
  value is an integer from 0 to N - 1, where N is the number of reducers in the
  next phase.  Any other input results in a 409 Conflict error.  This value
  indicates which of several reducers this output file should be fed to.  If
  this value is unspecified and there are multiple reducers in the next phase,
  output keys will be randomly assigned to reducers.
* The "X-Marlin-Reference" header may be set to the value "true" to indicate
  that the given key should be output by reference, instead of overwriting
  whatever's there.

For examples, see the Manta documentation.

## POST /my/jobs/task/fail

Fail the current task.  Any objects created since the last call to `POST
/commit` will be discarded (but will be linked in the task state for
*postmortem* analysis).

### Input

The input should have at least the following fields:

|| **message** || String || human-readable error message ||

It's also recommended to include a "code" field for a machine-parseable error
code in string form.  Other fields may also be specified up to a maximum
request size.

### Returns

Returns 204 (no content) on success, but may not return at all, as the task may
be terminated immediately.

Callers can assume that this call will never fail.


### Example

Request:

    {
        "message": "corrupt log record"
    }

Response:

    HTTP/1.1 204 No content
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:19:26 GMT
    X-Request-Id: 4bcf467e-4b88-4ab4-b7ab-65fad7464de9
    X-Response-Time: 12

## POST /my/jobs/task/abort

Not yet implemented.

Exactly like `POST /fail`, except that additional *postmortem* state will be
kept around and linked to the job state.  Core dumps of currently running user
processes and the entire filesystem contents will be preserved.


# Advanced topics

## Built-in facilities

We'll provide several facilities for dealing with common tasks in MR jobs.

### mpipe: output to the next job pipeline

mpipe [&lt;manta_key&gt;] [reducer]

Reads content from stdin and outputs it from this task.  If &lt;manta\_key&gt;
is not given, a new name will be created.

This is exactly equivalent to:

    $ curl -X PUT -T- http://localhost/$manta_key

If "reducer" is specified, it must be an integer from 0 to N - 1, where N is the
number of reducers in the next stage of the job pipeline.  This indicates that
this object should be routed to the given reducer.


### msplit: demux a stream for reducers

msplit [-n number_of_reducers] [-d &lt;delimiter&gt;] [-f &lt;field_list&gt;]

Reads content from stdin and outputs to the number of mpipes for the number of
reducers that are specified.  The field list is an optional list of fields that
are used as input to the partitioning function.  The field list defaults to [1].
The delimiter is used to split the line to extract the key fields.  The
delimiter defaults to (tab).  For example, this will split stdin by comma and
use the 5th and 3rd fields for the partioning key, going to 4 reducers:

    $ msplit -d ',' -f 5,3 -n 4


### mtee: save stdout to a manta object in a stream of commands

mtee [manta_object_key]

mtee is used to capture stdin and write to both stdout and a manta object.
For example, this will capture the output of cmd to manta object
/$MANTA_USER/stor/tee.out and still pipe what was coming from cmd to cmd2:

    $ cmd | mtee /$MANTA_USER/stor/tee.out | cmd2


### aggr: aggregate rows of numbers

aggr &lt;sum|avg|pctile99...&gt;

Reads a set of "key: value" pairs on stdin, aggregates them key-wise according
to the given function, and emits the result.

Example input:

    $ cat data.txt
    manta: 15
    marlin: 12.8
    manta: 38
    moray: 3

Example output:

    $ aggr sum < data.txt
    manta: 53
    marlin: 12.8
    moray: 3

The "key:" bit may also be omitted.


### apache2json: parse Apache log records and emit JSON objects


## Pipelined reducers

Sometimes it's necessary to pipeline the reduce phase so that instead of
processing all input keys in one pass, the input keys are processed in chunks
across multiple passes.  For example, you may want to process half of the input
keys in each of two parallel tasks, and then process the output of that, to
avoid having to load the entire data set in memory at once.

We already support multiple "reduce" phases in a job, but to support this you
also need to parallelize a single reduce phase.  You can do this with the
"count" property:

    "phases": [
        ... /* any number of map phases */
        , {
            "type": "reduce",
            "exec": ...,
            "count": 2
        }, {
            "type": "reduce",
            "exec": ...
        }
    ]

In this example, some number of map phases are followed by a phase with two
reducers running in parallel, followed by a final reduce stage.  The two
parallel reducers are identical except for their input and output keys.  By
default, input keys are randomly assigned to one of the reducers.  If a more
sophisticated assignment is necessary, the previous map phase may specify to
which reducer a given output object will be assigned using the X-Marlin-Reducer
header, which must be an integer between 0 and N - 1, where N is the total
number of reducers in the next phase.


## Node, Python, Ruby, etc. interface

The Jobs API is built in terms of Unix shell commands for generality.  This
makes it easy to run simple jobs like "grep" and "wc".  For slightly more
complex cases, custom utilities can be built in the Unix style and composed with
existing system utilities without writing significant new code.  For more
complex cases you want entirely custom code, you can simply run your program via
the "exec" string.

We'll also provide a higher-level interface for Node.js, so that people that
want to build jobs with custom code can use Node easily.  Likely we'll have
consumers implement an interface like:

    function processObject(file, callback) { ... }

and we'll provide convenience routines for API operations (like "taskParams()",
"commit()", "abort()", and so on).

Without fleshing this out completely, we know such an interface can be
implemented without changes to the Jobs API.  When people want to use the Node
API, we tell them to write phases like this:

    "phases": [ {
        "assets": [ "my/npm/package" ],
        "exec": "mnode /assets/my/npm/package"
    } ]

"mnode" is an executable Node.js program included with the dataset that installs
the given npm package (and therefore its dependencies, too) and then invokes our
Marlin interface in that package.

If we care to, we can do something similar for Python, Ruby, R, or any other
environment.  It would be just as easy for third parties to build their own.
Users would only need to reference a third party asset:

    "phases": [ {
        "assets": [
            "third-party/framework/start.sh"
            "my/script"
        ],
        "exec": "/assets/third-party/framework/start.sh /assets/my/script"
    } ]


# Marlin implementation

## Network architecture

Marlin makes use of two generic Manta components:
* muskie, the Manta web tier, which handles the Jobs API (requests to POST new
  jobs and GET job state).
* moray, where all Marlin state is stored.

On top of these, Marlin adds two components:

* Job worker tier, which locates new and abandoned jobs in Moray, assigns task
  groups to individual storage and compute nodes, and monitors job progress.
* Compute node agents, which execute and monitor tasks and report progress and
  completion.

There's a cluster of web tier workers and job workers in each datacenter, but
there's no real DC-awareness among these components.

**Job state**

To keep the service resilient to partitions and individual node failures, **all
job state is stored in Moray** (i.e., all components are essentially
stateless), and **all communication happens via Moray**.  Rather than talking
to each other directly, each component drops state changes into Moray and polls
for whatever other state changes it needs to know about.  This model makes it
relatively easy to deal with individual component failure and to scale
horizontally.  See "Invariants and failure modes" below for details.

Recall that Moray spans datacenters, is eventually consistent, and remains
available in the case of minority DC partitions.  Using Moray in this way also
implies that Moray becoming unavailable makes Marlin unavailable, and that
Moray must be able to keep up with the Marlin workload.  See "Scalability"
below.

Jobs are stored inside a single Moray bucket called "marlinJobs", with indexed
fields that allow the web tier, job workers, and agents to poll on the subset of
jobs that they care about.


**Web tier**

muskie retrieves current job state by querying "marlinJobs" for the job with the
given jobId.

When a new job is submitted, muskie allocates a new jobId and drops the job
definition into "marlinJobs".


**Job workers**

To synchronize changes to job state, each job is assigned to one worker at a
time.  This assignment is stored in the job object in Moray.  An assignment is
valid as long as the job's modification time (mtime) is within the last
WORKER\_TIMEOUT seconds.

This works as follows: each job worker periodically queries Moray for jobs
whose state is not "done" and either (a) the "worker" field is undefined or (b)
the modification time is more than WORKER\_TIMEOUT seconds ago.  (This is a
relatively simple query on indexed fields.)  For each job the worker finds, it
uses a test-and-set operation to assign the job to itself.  In this way,
workers pick up both new jobs and jobs whose assigned workers appear to have
failed.  A worker must update each job every WORKER\_TIMEOUT seconds, even if
there's no meaningful progress to report, to indicate that the worker is still
handling that job.  If the worker sees that WORKER\_TIMEOUT seconds has elapsed
since it last updated the job, it must discard any job state it has and stop
working on the job to avoid conflicting with another worker who picks up the
job.  (In practice, the best way to implement this may simply be to have all
state updates use a test-and-set that checks whether worker == me.)

When a worker picks up a newly submitted job, the first thing it has to do is
query Moray to find out where all the input keys are.  Moray itself has many
independent partitions and uses a consistent hash ring to map a given key to
the Moray partition that holds the metadata for that key.  The worker takes all
the input keys, executes the consistent hash function to group them by Moray
partition, and performs a batch request to each partition to determine which
storage nodes each key is actually stored on.

The worker than divides the first phase of the job into **task groups**, each of
which will run on one storage node and process whichever input keys are stored
on that node.  These task assignments are written out to a Moray bucket called
"marlinTaskGroups" with a field identifying the hostname of the node on which
the task will run.  The agents running on each node poll Moray for tasks
assigned to them, process each one, and update the task object.  The job worker
polls on these task updates, updates the global job state with job progress (so
the web tier can see it) and advances the job when each phase completes.

When a phase completes (as determined by the worker when all of its tasks have
completed), the worker begins processing the next phase in the same way it did
the first one: by locating the objects within Manta and then assigning tasks to
individual nodes.  When all phases complete, the job is marked done.

**Compute node agents**

As described above, compute node agents monitor tasks assigned to them in
"marlinTaskGroups".  As their tasks make progress, they update the task record,
which is also monitored by a job worker.

**Invariants and failure modes**

Invariants:

* All state is stored in Moray, so all components are stateless.
* At most one worker can ever "own" a job.  Only the owner of a job can write
  task assignments or update the job record in "marlinJobs".
* To a first approximation, a component is partitioned iff it cannot reach
  Moray.  This is an oversimplification because Moray itself is sharded, so it's
  theoretically possible to be partitioned from one shard but not another.
  This case is very unlikely, since each shard has a presence in each DC, but
  the system will handle this.  Most importantly, the only partitioning that
  matters is that between a component and a given Moray shard.

If a user's code fails within a task (e.g., dumps core), and the on-node retry
policy has been exhausted, the compute agent reports the task as failed, and
that will become visible in the final job state.  It's the user's problem at
that point.

If a compute node agent fails transiently (e.g., dumps core and restarts), it
should be able to pick up where it left off.  This should be invisible to the
user's code and the rest of the system, modulo slightly increased task
completion time.  (The agent must synchronously write state to local disk to
facilitate this.  If this storage fails, that's equivalent to the compute node
agent failing persistently.)

If a compute node agent either fails for an extended period *or* becomes
partitioned from its Moray shard (which are indistinguishable to the outside
world), the job worker redispatches any uncompleted work to other nodes that
have the same data.  It does not cancel the task on the failed node.  If both
tasks eventually complete successfully, the worker will necessarily see one's
results first and discard the other's.  (That's why the state in
"marlinTaskGroups" is not authoritative job state, and why the worker must
"commit" this state to the job record in "marlinJobs" for the web tier to see
it, rather than having the web tier scan "marlinTaskGroups" for itself.)

If a job worker fails transiently, it should be able to pick up where it left
off, since all state is stored in Moray.  This should be invisible to the rest
of the system, modulo slightly increased job completion time.

If a job worker fails for an extended period, another job worker will pick up
its jobs.  This becomes exactly like the case where a worker fails transiently:
the state in Moray is sufficient for the new worker to pick up where the old one
left off.

If a job worker becomes partitioned from one or more Moray shards, then it will
be unable to process task updates from compute nodes reporting to those shards.
It may end up trying to redispatch tasks assigned to those nodes to other nodes.
If it can talk to enough shards to complete the phase, the phase will complete;
otherwise, it will stall until connectivity is restored, at which point the
worker will see the completed task state and finish the phase.

If a job worker becomes partitioned from the Moray shard storing the job state
for WORKER\_TIMEOUT seconds, this will be treated as an extended worker failure.
Another worker will pick up the job where the first one left off.  The first
worker won't clobber the job state after someone else has picked up the job
because it also knows that WORKER\_TIMEOUT has elapsed.

Web tier nodes are completely stateless.  Any failure results in failing any
in-flight requests.  Persistent failures have no additional impact as long as
enough nodes are in service to handle load.  Partitions between the web node and
the Moray shard storing job records would result in failed requests.

If an entire datacenter becomes partitioned from the rest, then its web tier
nodes certainly won't be able to service any requests.  All of its job workers
become partitioned (see above -- the work should eventually be picked up by
workers in other datacenters).  All of its compute node agents effectively also
become partitioned, and their work *may* be duplicated in other datacenters.
Importantly, jobs submitted to the majority partition should always be able to
complete successfully in this state, since there's always enough copies of the
data to process the job.

**Scalability**

Being stateless, web tier nodes can be scaled arbitrarily horizontally.  If load
exceeds capacity, requests will queue up and potentially eventually fail, but
alarms should ring long before that happens.

Similarly, job workers can be scaled arbitrarily horizontally, and capacity
monitoring should allow administrators to ensure that sufficient capacity is
always available.  The failure mode for excess load is simply increased job
completion time.  Task updates may accumulate in Moray faster than the worker
can process them, but the total space used in Moray is the same regardless of
how fast the job workers process them, so there's no cascading failure.  If job
workers cannot update their locks in Moray, the system may thrash as workers try
to take each others' jobs, but this would be very extreme.

Compute nodes rely on the OS to properly balance the workload within that box.
They may queue tasks after some very large number of concurrent tasks, but it's
not anticipated that we would ever reach that point in practice.  If this
became a problem, job workers could conceivably redispatch the task to another
node with the same data available.  If this truly became a serious problem, we
could support compute agents that download data, rather than operating locally
(just as reducers already do).

Given that there's enough capacity in each of the Marlin tiers, the remaining
question is Moray.  Moray will be sharded, and we will say that each compute
node reports its tasks to exactly one (uniformly distributed) shard.  All job
state will be stored on a single shard, since the number of updates is
relatively minimal.  We should be able to reshard to scale this out.

Additionally, each compute node can batch its Moray updates from all tasks for
up to N seconds, sharply reducing the number of requests required.  Similarly,
job workers can batch job state updates for many jobs up to N seconds.  Batching
of "location" GET requests is discussed above, under "job workers".  Thus, the
total number of Moray-wide requests should be boundable to something like (# of
workers \* # of compute zone agents) / (N seconds), where N is how frequently
each agent updates Moray.  These requests should mostly be distributed across
all shards.

Batching requests for the same record over multiple seconds reduces both network
throughput and database operations, as multiple state changes can be combined at
the source.

Batching requests over multiple records helps network throughput, but not the
total number of database operations, since each record must be operated on
separately.  The total number of database operations could still be around (# of
active tasks \* # of agents \* # of active jobs \* # of job workers).  Again,
these are distributed across many Moray shards, which can be scaled
horizontally.

## Objects

There are two types of objects stored in Moray: jobs and task group
assignments.  For the details, see the schema in lib/schema.js in marlin.git.

Web tier:

* POST /jobs (create new job)
* GET /jobs/jobId (get job status)
* PUT /jobs/jobId (rare case: use T&amp;S to set cancelled = true)

Workers:

* GET /jobs where worker == null or mtime &lt; ... (pick up new jobs)
* PUT /jobs/jobId (update job state)
* PUT /taskgroups/taskGroupId (create/update assignment)
* GET /taskgroups/taskGroupId (check task group state)

Agents:

* GET /taskgroups where host == me (pick up new task groups)
* GET /taskgroups/taskGroupId (check task group state)
* PUT /taskgroups/taskGroupId (post task group results)


## Cancellation

To cancel a job, the web tier writes "cancelled: true" to the job record.  At
this point, we could say that the job state is immutable and all future state
requests only show whatever state has been completed up to this point.

Asynchronously, the job worker will need to be polling on the job state
occasionally to notice the cancelled flag.  It propagates this to individual
task assignments, where the compute node agents will notice that the job has
been cancelled and abort them.  Eventually, all work on the job will complete,
though this may take some time to propagate through the system.


## CN agent

As described above, compute node agents receive task assignments via Moray and
report progress back to Moray.  Such progress includes keys processed and state
changes, including that a task is queued, loaded, running, timed out, failed, or
done.)

The agent maintains a pool of compute zones that are either "uninitialized",
"ready", or "busy".  Uninitialized zones are those currently in an arbitrary
state.  Those are asynchronously transitioned to "ready" by the following steps:

1. Halt the zone, if it's not already halted.
2. "zfs rollback" to the zone's origin snapshot
3. Boot the zone.
4. Sets up an agent inside the zone that listens for local HTTP requests to the
   Task Control API and forwards them via a zsock server to the CN agent.

When a new task is dispatched, if there are no "ready" zones available, tasks
are enqueued for later.  Otherwise, the agent picks a "ready" zone and does the
following:

1. Downloads the job's bundle's assets into the zone.
2. Creates a hyprlofs mount for the files.
3. Finally, invokes the bundle's exec script.

When the job completes, successfully or otherwise, the CN agent reclaims the
zone by marking it "uninitialized" again and asynchronously transitioning it to
"ready" again to be used for another task.

To implement the Task Control API:

`/info` is simple: it just retrieves data from the agent's persistent state,
which already has to exist to manage the job.

`/objects/:key` makes a note in the task state that this object is being
written, and then (first approach) proxies the request directly to manta. But
see "open questions" below.

`/commit` updates the persistent state for the task to indicate that all
objects written since the last checkpoing have been committed.

`/fail` also updates the persistent state, but indicates that new objects should
be discarded.  They won't actually be deleted, but marked as discarded in the
job state so users can look at them later.  After that, this request terminates
the zone.

`/abort` is like `/fail`, except that before terminating it gcores all processes
in the user's contract and does an incremental "zfs send" of the zone to a new
manta file.  (We'll want some automated mechanism to reconstruct the user's zone
based on this, presumably by "zfs receive" the dataset and then "zfs receive -i"
the stream associated with this task.)

In order to survive transient failure, the agent must store state on disk.  The
current plan is to use sqlite for this because it already has usable Node
bindings.

## Open questions

Roughly in order of importance:

* When user code wants to write files to manta, how exactly does that work? Do
  we have a way to tell Manta to write one copy on the local box, and if so, do
  we avoid sending it across the network twice?
* What metrics do we want to provide via CA?  How will we implement these?
  What other changes must be made to CA to support Marlin and Manta?

## Management API?

cancel tasks, take compute zones and servers in and out of service
initial setup: creating compute zones
tuning # of compute zones

## Reducers

Reducers are mainly different from mappers in three ways:

1. Reduce tasks operate on multiple keys at once, not just one.
2. Reduce nodes operate on data stored elsewhere in Manta, which means it must
   be downloaded first.  (We'd actually like to be able to do this for mappers
   too.)
3. Reduce tasks can run in any compute zone, which means we need a mechanism for
   discovering and choosing compute zones to run reduce tasks.

Here's the current plan:

* For testing purposes: have the worker assign reduce tasks to any host that it
  knows about, and by writing the same taskgroup record it would use for "map",
  except that there's a type field indicating "reduce".  Doing this first lets
  us use the rest of Marlin to iterate on the agent, where the bulk of the
  changes will be made.
* Modify the agent to support "reduce" tasks by processing them just as it would
  "generic" tasks except that all keys are processed in one invocation.
* Modify the agent to support downloading input files rather than mapping them
  in.  This should be supported for regular map tasks, too.  (This comes after
  adding "reduce" tasks because the implementation for map tasks alone may not
  be generalizable for reduce tasks.)
* Once this works, modify the agent to not specify input keys on the command
  line for reduce tasks.  (See below.)
* Implement a real zone scheduling algorithm.
* Rework agent/worker communication to use something other than the existing
  task group interface for assigning work, then stress test with large numbers
  of keys.


### Multiple keys at once

In order to process multiple keys at once, several pieces of both the global
zone agent and in-zone agent must be generalized because they assume only one
input key at a time, and that a sequence of input keys (tasks) make up a
complete task group.  These changes are straightforward.

There are some more serious scalability issues in processing up to millions of
keys at once:

1. Interface: It will be difficult to support a file-based (as opposed to
   stdin-based) interface for actually invoking the reducer, since command lines
   cannot support millions of arguments.  If we want to support a file-based
   interface, we may have to specify the keys on stdin or some other file that's
   passed as a command line argument.  There are other problems with this,
   though; see below.
2. Architecture: Today, the task group is the main unit of work inside the
   agent, and there's no way to share state across task groups.  But task groups
   are passed between worker and agent via Moray as JSON objects that must be
   parsed and stringified on both ends, stored in memory, and updated in
   entirety.  That's not going to scale well with millions of keys.


### File access

In order to deal with very large numbers of very large files, we'll need to
stream data when possible.  In the worst case, reducers may not be able to
physically store all of the data they will need to process.  But even in less
severe cases it may be a grossly inefficient use of both memory (valuable ARC
space) and disk to store all of the data at once when it could just as well be
streamed.

Streaming is relatively straightforward: the agent downloads input files and
pipes them over stdin to the reducer, which is invoked with no actual command
line arguments.  The agent may as well download the files sequentially, only
parallelizing to start the next file when the current one is near completion.

People may want access to files by name.  Lots of custom programs take
filenames and don't work on stdin/stdout.  And it's not hard to imagine reduce
tasks that by nature require random access to all of the input files.  This is
hard, since we must wait to start the reducer until all of the input file(s)
have been completely downloaded.  We can probably punt on this in the short
term.  We can always add new types of tasks in the future (e.g.,
"reduce-byfile") with different semantics, since such an abstraction is clearly
pretty different from streaming reduce.


### Scheduling compute zones

For now, we're going to take the simple (simplistic?) approach of selecting a
server at random among all known servers and sending the reduce task to that
agent, which will run it just like a map jobs.


### Scaling records

Instead of using taskgroups, we may need to switch to more fine-grained records.
See below.

# Project: Fine-grained records

Following is a proposal for splitting up the existing job and taskgroup records
into more fine-grained records.  This is necessary because processing large
records blocks Node programs (including muskie, moray, the marlin worker, and
the marlin agent) and other pieces in the middle.

The new design constraint is that the size of every JSON record in the system
must be bounded (and should be fairly small).  This applies to records received
from and sent back to users as well as records used internally.

## Streaming job input/output

A job's input and output are represented as streams.  The input stream is a list
of object keys in plain text, one per line.  The output stream looks just like a
Manta directory listing.  The public API endpoints will thus look like this:

* `POST /:userid/jobs`: submit a new job (JSON)
* `GET /:userid/jobs`: list jobs (JSON)
* `GET /:userid/jobs/:jobid`: retrieve current job state (JSON)
* `PUT /:userid/jobs/:jobid/input`: submit input keys for a job (text/plain)
* `GET /:userid/jobs/:jobid/output`: retrieve list of output keys for a job
  (JSON or text/plain)
* `GET /:userid/jobs/:jobid/phases/:i/results`: retrieves list of results for a
  given phase. (JSON object describing each *task*, including the input key, the
  result (success or failure), timing details, etc.)

Below are example implementations of several use cases.  In all of these
examples, it's assumed that the input is streamed in and the output is streamed
out.  The sequence for each of these examples is:

    $ curl -i -X POST -d @input.json '-Hcontent-type: application/json' \
        manta.joyent.us/:userid/jobs

    # extract new job URL from Location header

    $ curl -X PUT -d @input_keys.txt manta.joyent.us/:userid/jobs/:jobid/input

    $ curl manta.joyent.us/:userid/jobs/:jobid

    # repeat above until state == "done"

    $ curl -o output_keys.txt manta.joyent.us/:userid/jobs/:jobid/output

Alternatively, jobs may be pipelined, so that the input keys from one job come
from the output keys of a previous job.  To do this, use the "inputStream"
property on the job itself, as in:

    {
        "jobName": "part two"
        "phases": [ ... ],
        "inputStream": "/:userid/jobs/:jobid/output"
    }

"inputStream" may refer to any other job's output stream, or any Manta object.
The object must have type text/plain.  There's no corresponding "outputStream"
property, since every job has its own output stream at
`/:userid/jobs/:jobid/output` that can be used anywhere an input stream or Manta
file is required.


## Moray records

The Moray records exchanged among Marlin components must also be broken up so
that each one is bounded in size.  Primarily, this means replacing large task
group records with individual task records, and moving output key records from
being stored with each task to being stored separately.

As described above, the general constraint is that no single record be unbounded
in size.  Anything that contains an unbounded N of something must be split into
a set of smaller records.


### Jobs

Each **job record** has these properties:

|| **Field**       || **Type**          || **Description** ||
|| jobId           || string            || unique identifier, assigned at job submission time ||
|| jobName         || string            || non-unique user label for the job ||
|| owner           || string            || user's uuid who submitted the job ||
|| phases          || object&nbsp;array || description of what the job does ||
|| phases.i.type   || string            || "storage-map", "generic", or "reduce" ||
|| phases.i.exec   || string            || bash script to execute via "bash -c" ||
|| phases.i.uarg   || object            || arbitrary user object ||
|| phases.i.assets || string&nbsp;array || Manta objects to download into zone ||
|| worker          || string            || worker's uuid currently processing the job  ||
|| state           || string            || "queued", "running", or "done" ||
|| timeCreated     || datetime          || job creation (submission) time ||
|| timeAssigned    || datetime          || time when the job was first picked up by a worker ||
|| timeInputDone   || datetime          || time when the user finished writing input ||
|| timeCancelled   || datetime          || time when the user cancelled the job ||
|| timeDone        || datetime          || job completion time ||

The rules about job records are:

* The job record is initially written out by muskie when the user submits a job.
* Subsequently, muskie updates the record when the user finishes submitting
  input or when the user cancels the job.  This is always a test-and-set
  read-modify-write.
* The record is also updated by a job worker, but only the one which "owns" the
  job or one which is trying to take it over.  All writes are test-and-set based
  on the etag.  If the write fails, muskie has updated it, and the worker
  fetches the record, modifies it as needed, and writes it again.

Writes to this record are relatively uncommon, as they happen only in response
to one of two user actions and a small number of significant events in the
life of the job, plus periodic "heartbeat" writes by the worker.

As the user submits input, muskie also writes out immutable **job input
records** with the following properties to describe each input key for the job's
first phase:

|| **Field**  || **Type** || **Description** ||
|| jobId      || string   || see above ||
|| key        || string   || Manta key for the object emitted ||

These records are read by the worker to process the first phase.

### Task input

To process each job, workers write out **task records**.  Tasks represent atomic
chunks of work.  For map phases, tasks correspond 1:1 with input keys (though
not necessarily with output keys).  For reduce phases, one task covers all keys
in the phase.  By definition, a single task cannot be parallelized, but all
tasks within a job can in principle be executed in parallel.

All task records have the following fields:

|| **Field**      || **Type**          || **Description** ||
|| jobId          || string            || see above ||
|| taskId         || string            || unique identifier for this task ||
|| phaseNum       || integer           || which phase of the job this task goes with ||
|| server         || string            || server assigned to process this task ||
|| state          || string            || dispatched, cancelled, queued, running, aborted ||
|| machine        || string            || uuid of zonename where this task ran ||
|| result         || string            || "ok" or "fail" ||
|| error          || object            || describes any failure that occurred ||
|| error.code     || string            || programmatic error code ||
|| error.message  || string            || human-readable error message ||
|| timeDispatched || datetime          || when this task was created ||
|| timeQueued     || datetime          || when the task was picked up by an agent ||
|| timeStarted    || datetime          || when the task started running ||
|| timeDone       || datetime          || when the task completed ||
|| timeCommitted  || datetime          || when the worker committed this result to the job's output stream ||
|| nOutput        || integer           || total number of output keys, including those contained in firstOutput ||
|| firstOutputs   || object&nbsp;array || first N output keys (bounded N), in the same format as a task output record (see below) ||

Map tasks have the following extra fields:

|| **Field** || **Type** || **Description** ||
|| key       || string   || Manta key to be processed.  This is the user-visible key name, which means it contains the user's login name rather than the account uuid ||
|| account   || string   || Account uuid for the user owning the key.  This is used at the agent to actually locate the key. ||
|| objectid  || string   || Unique Manta objectid for the key to be processed ||
|| zonename  || string   || Mako zonename on remote server with a copy of this object ||

Reduce tasks have the following extra field:

|| **Field**     || **Type** || **Description** ||
|| timeInputDone || datetime || time when all input for this task was available ||
|| nInputs       || number   || total number of input keys ||

In the success case, a task assignment goes through these states:

* The worker writes out the assignment in the **dispatched** state.
* The agent quickly picks it up and puts it into the **queued** state.
* When a zone becomes available to process the task, the agent puts it into the
  **running** state.
* When the task has completed, the agent puts it into the **done** state.  For
  reduce tasks, this can't be done until all input keys have been processed,
  which can't be known until timeInputDone has also been set.
* When the worker sees that the task has completed, it **commits** the task,
  meaning that its output keys are propagated to the next phase (or the job's
  output keys, if this is the last phase).

Other things can happen:

* If the agent knows it will never be able to complete the task successfully, it
  will move it to the **aborted** state.  In this case, the worker can try to
  assign the same key again, possibly to a different server, to retry it.
* If the job is cancelled, or the worker decides for some other reason that it
  doesn't need the output of this task, the worker can move the task to the
  **cancelled** state, which tells the agent to forget about it.

In the case of retries, there may be more than one task for the same key (or
phase, in the case of reduce tasks), but at most one will end up committed.  In
the future, we could support speculative execution, in which we write out
multiple task assignments for the same keys concurrently (e.g., to see which one
finishes first, or in response to uncertainty about whether one of them is being
processed).

The input for map tasks is contained in the task record itself, since it's
always one key.  Input for reduce phases is written out as a series of **reduce
input key records**, each with the following fields:

|| **Field**     || **Type**          || **Description** ||
|| jobId         || string            || see above ||
|| taskId        || string            || see above ||
|| key           || string            || input object's user-facing key ||
|| account       || string            || account uuid for key ||
|| objectid      || string            || input object's unique Manta identifier ||
|| servers       || object&nbsp;array || list of locations for this key ||

The current implementation will fetch the input key from muskie, but we may well
want to fetch it directly from remote servers, for efficiency and to make sure
we're getting the correct objectid.

While processing each task, agents update the task record with status and the
first few output keys.  If necessary, they also write out **task output
records**, each of which describes a single key emitted from a given task:

|| **Field**  || **Type** || **Description** ||
|| jobId      || string   || see above ||
|| taskId     || string   || see above ||
|| error      || object   || see above ||
|| key        || string   || Manta key for the object emitted (with login name, not account uuid) ||
|| created    || string   || time when the object was created ||

These records are immutable, written only once by the agent and subsequently
read by the worker.  Only "key" or "error" should be present: if "key", then
that key was emitted.  If "error", then this is an error emitted.  This is only
possible for reduce tasks, since map tasks may only have one error and can just
include it directly in the "task" record.

## Muskie

When the user **submits a job**, muskie validates it and writes out a new job
record.

When the user **submits input** for a job, muskie writes out new job key records
for each of the new keys.  When the input is complete or the user **cancels a
job**, muskie updates the job record using a read-modify-write-with-test-and-set
operation.

When the user **lists jobs** or **fetches a job's status**, muskie reads the job
record, filters and sanitizes fields, and then emits the results back to the
user.  The result includes the information initially submitted with the job, the
current state, timing information, whether the job has been cancelled, whether
it's waiting for input, and so on.

When the user **fetches detailed job status for a given phase**, muskie first
fetches the job record, and then fetches all task assignment records for the
given phase.  For each task, the user can see the state of this task, the
result, error information (if any), and timing information.  For map tasks, the
input key is also included.  This is for reference and debugging, since
intermediate keys are not directly available from Manta.

When the user **fetches job output**, muskie first fetches the job record in
order to see how many phases it has, then searches moray for all tasks with
jobId = this job, phaseNum = last phase, timeCommitted not NULL (indicating this
result has been committed).  The output keys contained within these records are
emitted to the client.  If there are additional output keys in separate records,
these will be fetched separately and emitted back to the client.


## Worker

Job workers will poll on Moray for new and abandoned jobs, just like they do
today, and attempt to take them over using a test-and-set write.  This is a
periodic query for (worker == null or mtime &lt; now - timeout).

Upon picking up a job for the first time, the worker fetches all of the job
input keys, tasks, reduce input keys, and task output keys.  It can't do
anything else until it has all of these.  From these, it figures out what needs
to be done.

While there are first phase input keys not covered by already-saved tasks, these
keys are first located within Manta and then assigned.  For map phases, new task
records are saved.  For reduce phases, a reduce task is written if there isn't
one already, and then reduce input keys are written out.

While there are tasks which have completed successfully but not committed,
commit them by simply updating the task record.

While there are committed tasks for a non-final phase that have output keys
(either in the task itself or separate output key records) that haven't been
propagated to the next phase, do so.  This is exactly like propagating job input
keys at the beginning of the task.

While there are task records that haven't been updated within a specified
timeout, mark them "cancelled".  (When we add retry support later, we should
simply re-issue them without cancelling the other one until the job is done.)

If the job's input stream has seen EOF, and all of the above has been completed,
and all tasks have either aborted, cancelled, or finished, then the job is done.

If the job becomes cancelled, cancel all outstanding tasks.

Things we'll add later:

* Chained jobs (inputStream == another job): works the same way, but instead of
  fetching job input key records, they poll on the output of a previous job.
  XXX should we try to assign such jobs to the same worker?
* Retries.  Don't cancel tasks that seem abandoned, but assign new tasks for
  them.  Do the same for tasks that have explicitly failed.  This requires more
  bookkeeping for each key, which may now be in multiple tasks.
* Deleting intermediate objects.  Intermediate objects can be queued for
  deletion when a task where they're used as an input key becomes committed, or
  when the job fails.


## Agent

Agents find work by polling for tasks with server == their uuid.  For map tasks,
incoming tasks with the same jobId and phaseNum are grouped together, since they
can be executed in series in the same compute zone without resetting the zone.
For reduce tasks, the agent looks for (and polls on) reduce input key records
with a matching taskid.  As with map tasks, these keys are combined into a
single task group.  Thus, for both types of phases, the agent maintains a task
group with a queue of keys to be processed in order within the same compute
zone.

The agent can start processing both map and reduce task groups as soon as any
input keys are available.  It allocates a zone, downloads any required assets,
and starts processing tasks.

For map phases, we process the keys serially by mapping each key into the zone,
signalling the in-zone agent to invoke the user's command, unmapping the key,
updating the task record with the result, and moving on to the next key.  As new
tasks arrive for the same group, we just add them to the back of the queue.  If
the queue runs out, we can reset the zone for use by another job, or hold onto
it for a few seconds in case more keys arrive.  (If we reset the zone and more
keys arrive, we just start processing the new keys as a new group.)

For reduce phases, there's only ever one invocation of the user's code.  We
start downloading the first input key from Manta and stream it via the in-zone
agent to the user's code.  As each key completes, we start downloading the next
one and feeding its contents in.  As new keys come in, we add it to the queue to
be downloaded and fed to the user.  Unlike with the map phase, the agent cannot
stop executing the task until it knows all input keys have been processed and
the user's code has received EOF.  Once the "timeInputDone" field of the reduce
task is set (by the worker, only after writing out all reduce input keys), the
agent makes one last query for new input keys, processes all remaining keys, and
finishes the task.

For both map and reduce tasks, as the user's code emits new objects, the task
itself is updated and (if necessary) new task output keys are saved.

To cancel tasks, the worker marks them as "cancelled".  For tasks that are
queued, the agent simply removes them.  For tasks currently running, we reset
the zone and then remove them.  XXX This could be a pretty expensive operation
since the worker will have to update O(n) records -- one for each outstanding
key to be processed.  The alternative is to have the agent watching some other
job-wide record to notice failure.


## mrstat (status tool)

Important operations include:

* list jobs with basic job status: query for job records and maybe job input
  keys (just to count them).
* for a job, show summary status information: query for job record, job input
  keys, tasks, reduce input keys, and task output keys.  For each of these
  groups, show only the first N and last N, and summary information for all of
  them.
* for a job, show complete information: same as above, but make available all
  records, not just the first N and last N.  The current tool punts on this by
  providing a command-line invocation that queries Moray for the full data.  We
  can probably do the same.


## Related changes

As part of these changes, we should also modify the worker to have only one
connection to Moray rather than one per job, which should simplify a bunch of
things, and makes it possible to throttle on a per-node basis rather than
per-job.
