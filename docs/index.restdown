---
title: Marlin
markdown2extras: wiki-tables, code-friendly
apisections: Job API, Task API
---

# Marlin

Marlin is a facility for running compute jobs on Manta storage nodes.  This
document describes the desired end state for version 1.


# Design goals

Marlin facilitates the following use cases:

* MapReduce in a generic environment (e.g., video transcoding)
* MapReduce in a node-based environment (i.e., make it particularly easy for
  node apps; e.g., log analysis)
* Straight Map with no reduce (e.g., rendering)
* Allowing others to run compute on your data

Of course, we want this to be easy to use, monitor, and debug.  A good
barometer is that word count, the MR equivalent of "hello world", should be
trivial to write.  

Marlin defines two public APIs:

* Job API: allows end users to submit, manage, and monitor compute jobs
* Task API: used by user code running inside Marlin to coordinate execution and
  report progress.

All DateTimes in these APIs use W3C DTF in UTC ("YYYY-MM-DDThh:mm:ss.sZ").


# Job API

The Job API is a public-facing API that sits alongside the public Manta endpoint
and lets end users submit, manage, and monitor compute jobs.


## POST /jobs

Submit a new **job** to be run immediately under Marlin.

Each job defines a set of input Manta objects (identified by keys) on which to
operate and a **bundle** of code that will run within Marlin to process a set of
objects.  When the job is submitted, the **bundle** is distributed to Manta
nodes that store the input objects.  On each node, one or more **tasks** will be
spun up inside **compute zones** to process the objects stored on that node.
Each task gets its own copy of the bundle and invokes it to process its input
objects.  Tasks must report when they have succeeded or failed, and may also
report progress as they go.  The system may time out tasks that are taking too
long.

While the job is running, users can retrieve the state of the job to see what
tasks have completed, what tasks are running, which keys have been processed,
and which keys are left.

When all tasks complete, the job itself completes.  The job succeeds if all
tasks succeed, and the job fails otherwise.  Failed tasks are not retried.


### Inputs

|| **Field** || **Type** || **Description** ||
|| name || String || Human-readable job label (not necessarily unique). ||
|| input_keys || Array&nbsp;of&nbsp;Strings || List of Manta objects to process. ||
|| bundle_key || String || Manta key for the job's bundle. ||
|| bundle_exec || String || For tarball bundles only, the script that starts each job. For non-tarball bundles, the bundle itself is assumed to be executable, and this property is ignored. ||
|| bundle_contents || String || Inline bundle only: base64-encoded contents of the bundle. ||
|| bundle_type || String || Inline bundle only: specifies the bundle's content type. ||
|| job_args || Object || Arbitrary JSON object that will be available to each task.  For MapReduce, this could specify a list of reducer nodes to stream data to.  This can be an arbitrary JSON object; applications that don't use JSON can use any base64-encoded content here. ||

Bundle contents are specified in one of two ways:

1. As a Manta object identified by `bundle_key`.
2. Inline, using `bundle_contents` to encode the file contents and `bundle_type`
   to specify the content type.  This form is primarily for convenience for
   simple examples.

Based on the content type, Marlin will determine whether the bundle is an
archive or directly executable.  For archive bundles, the archive will be
extracted at `/` inside each compute zone, and then the executable identified by
`bundle_start` will be invoked.  For executable bundles, the bundle will be
dropped into each compute zone and executed directly, so it can be any native
binary or a script with a shebang line that works in the compute zone (e.g.,
`#!/usr/bin/bash`).

### Returns

On success, returns 201 with the URI for the job in a Location header and the
full state of the job (as returned by `GET /jobs/:jobid`) in the response.

### Errors

||**Error Code**||**Description**||
||InvalidArgument||If the job parameters are invalid||
||MissingParameter||If required job parameters are missing||


## GET /jobs/:jobid

Returns the job's current state.  The system makes a best effort to keep this
information updated, but any of the runtime state may be outdated or
inconsistent while a job is in the "running" state.


### Returns

|| **Job&nbsp;field** || **Type** || **Description** ||
|| jobid || String || Unique identifier for this job. ||
|| name || String || Human-readable job label (not necessarily unique), provided when the job was created. ||
|| input_keys || Array&nbsp;of&nbsp;Strings || List of keys to be processed by this job. ||
|| nkeys || Number || Total number of keys to be processed by this job. ||
|| bundle || String || Either "inline" or a Manta key identifying the bundle for this job. ||
|| job_args || Object || Job argument provided when the job was created. ||
|| create_time || DateTime&nbsp;String || Time at which the job was submitted. ||
|| state || String&nbsp;(see below) || Describes current job state. ||
|| keys_completed || Array&nbsp;of&nbsp;Strings || the list of keys which have been successfully processed. ||
|| nkeys_completed || Number || the number of keys which have been successfully processed. ||
|| tasks || Array&nbsp;of&nbsp;Objects || Individual task states (see below). ||
|| finish_time || DateTime&nbsp;String || If the job state is "done", this is the approximate time at which the job completed. ||
|| status || String&nbsp;(see&nbsp;below) || If the job state is "done", this describes whether the job completed successfully. ||
|| error_code || String&nbsp;(see&nbsp;below) || If the job status is "failed", this describes why the job failed. ||
|| output_keys || Array&nbsp;of&nbsp;Strings || List of Manta objects output by this job ||
|| job_log || Array&nbsp;of&nbsp;Objects || Log of job activity ||

The job **state** is one of:

|| **Job&nbsp;state** || **Meaning** ||
|| queued || The job has been created but it's not yet running. That is, no tasks have been instantiated to execute the job yet. ||
|| running || The job is currently running on at least one storage node. That is, one or more tasks have been instantiated to execute the job. Some or all of those tasks may have already completed, but not all tasks have been instantiated and completed. ||
|| done || All of the job's tasks have completed or the job has been aborted. ||

For completed jobs, the job **status** is one of "success" (if all tasks
completed successfully) or "failed" (otherwise).

For failed jobs, the **error_code** describes why the job failed:

|| **Job&nbsp;error&nbsp;code** || **Meaning** ||
|| job_cancelled || the job was explicitly cancelled ||
|| task_failed || one or more of the job's tasks failed ||

#### Job log

The **job_log** field includes entries for all state changes for the job itself
as well as all tasks associated with the job.  This may be useful for
understanding job failures and performance *postmortem*.  Each entry has the
following fields:

|| **Job&nbsp;log&nbsp;entry&nbsp;field** || **Type** || **Description** ||
|| date || DateTime&nbsp;String || Time of log entry ||
|| taskid&nbsp;(task&nbsp;entries&nbsp;only) || String || Identifier for a given task ||
|| event || String || Either "state_change" or "message" ||
|| message || String || Human-readable message ||
|| new_state&nbsp;(state&nbsp;change&nbsp;entries&nbsp;only) || String || New state name ||

#### Job progress

Callers can get a high level view of job progress by looking at
**nkeys_completed** compared with **nkeys**.  They can see precisely which keys
have been completed by examining "keys_completed".  These mechanisms rely on
tasks to report progress as they process each object, though tasks are not
required to report progress until they finish processing all of their input.

The **tasks** field shows more detailed information about how the job was
distributed to various servers and the status of each task on each server.  Each
entry of this array describes an individual task, including:

|| **Task&nbsp;field** || **Type** || **Meaning** ||
|| taskid || String || Unique identifier for this task within this job. ||
|| host || String || Unique identifier for the host on which this task ran. ||
|| zonename || String || Unique identifier for the compute zone in which the task ran. ||
|| input_keys || Array&nbsp;of&nbsp;Strings || List of keys assigned to this task. ||
|| start_time || DateTime&nbsp;String || When this task was instantiated. ||
|| state || String || Describes the current state of the task (see below). ||
|| output_keys || Array&nbsp;of&nbsp;Strings || List of keys output by this task. ||
|| end_time || DateTime&nbsp;String || For completed tasks, the time when the task finished. ||
|| status || String || For tasks in the "done" state, this describes whether the task completed successfully. ||
|| task_data || String || Arbitrary task data (intended for task debugging). ||
|| partial_objects || List&nbsp;of&nbsp;Strings || List of Manta objects created by the task but not committed because the task failed or is still running. ||

The task **state** is one of:

|| **Task&nbsp;state** || **Meaning** ||
|| queued || The task has been planned, but has not started running yet. ||
|| loading || Marlin is preparing the compute zone to run the task. This step includes downloading and extracting the job's bundle. ||
|| running || The task is currently running. ||
|| done || The task is no longer running. ||

For tasks that are "done", the task **status** is either "success" or "failure".
For failed tasks, the **error_code** indicates why the task failed:

|| **Error&nbsp;code** || **Meaning** ||
|| abnormal_exit || The task exited with non-zero status, or exited without indicating that it had finished processing. ||
|| failed || The task explicitly failed. ||
|| timeout || The task did not complete within the alloted time interval. ||


## POST /jobs/:jobid/cancel

Asynchronously cancels the given job, if the job has not yet completed.  If the
job has completed, this operation does nothing.  It may take several minutes to
cancel a job.

### Returns

Returns 204 (no content) on success.

# Job API examples

## Create a job

Here's a job specification:

    $ cat job-input.json
    {
        "name": "weekly log rollup",
        "input_keys": [
            "logs/2012-03-11.log",
            "logs/2012-03-12.log",
            "logs/2012-03-13.log",
            "logs/2012-03-14.log",
            "logs/2012-03-15.log",
            "logs/2012-03-16.log",
            "logs/2012-03-17.log"
        ],
        "bundle_key": [ "bin/log_rollup" ],
        "job_args": {
            "reduce_url": "example.com/reduce"
        }
    }

Submit the job:

    $ curl -i -X POST -H'content-type: application/json' -T job-input.json \
          http://$MANTA/jobs

The request looks like this:

    POST /jobs HTTP/1.1
    Authorization: ...
    Host: api.example.com
    Accept: application/json
    Content-Length: 387
    Content-Type: application/json

    {
        "name": "weekly log rollup",
        "input_keys": [
            "logs/2012-03-11.log",
            "logs/2012-03-12.log",
            "logs/2012-03-13.log",
            "logs/2012-03-14.log",
            "logs/2012-03-15.log",
            "logs/2012-03-16.log",
            "logs/2012-03-17.log"
        ],
        "bundle_key": [ "bin/log_rollup" ],
        "job_args": {
            "reduce_url": "example.com/reduce"
        }
    }

The response looks like this:

    HTTP/1.1 201 Created
    Location: /jobs/5e42cd1e-34bb-402f-8796-bf5a2cae47db
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:19:26 GMT
    X-Request-Id: 4bcf467e-4b88-4ab4-b7ab-65fad7464de9
    X-Response-Time: 754
    Content-Type: application/json
    Content-MD5: a0cf22d52296f733b0850a721c150ff7
    Content-Length: 762

    {
        "jobid": "5e42cd1e-34bb-402f-8796-bf5a2cae47db",
        "name": "weekly log rollup",
        "input_keys": [
            "logs/2012-03-11.log",
            "logs/2012-03-12.log",
            "logs/2012-03-13.log",
            "logs/2012-03-14.log",
            "logs/2012-03-15.log",
            "logs/2012-03-16.log",
            "logs/2012-03-17.log"
        ],
        "nkeys": 7,
        "bundle": "bin/log_rollup",
        "job_args": {
            "reduce_url": "example.com/reduce"
        },
        "create_time": "2012-03-20T17:19:26.732Z",
        "state": "queued",
        "keys_completed": [],
        "nkeys_completed": 0,
        "tasks": [],
        "output_keys": [],
        "job_log": [ {
            "date": "2012-03-20T17:19:26.732Z",
            "event": "state_change",
            "new_state": "queued",
            "message": "job created and queued"
        } ]
    }

## Checking job state while it is running

    $ curl http://$MANTA/jobs/5e42cd1e-34bb-402f-8796-bf5a2cae47db

The response looks like this:

    HTTP/1.1 200 OK
    Location: /jobs/5e42cd1e-34bb-402f-8796-bf5a2cae47db
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:21:40 GMT
    X-Request-Id: a9ad378a-7449-11e1-a7f2-57be1e07436a
    X-Response-Time: 652
    Content-Type: application/json
    Content-MD5: 306f9070b03edd6b3ad9eab91fb2fbb4
    Content-Length: 3503
    
    {
        "jobid": "4bcf467e-4b88-4ab4-b7ab-65fad7464de9",
        "name": "weekly log rollup",
        "input_keys": [
            "logs/2012-03-11.log",
            "logs/2012-03-12.log",
            "logs/2012-03-13.log",
            "logs/2012-03-14.log",
            "logs/2012-03-15.log",
            "logs/2012-03-16.log",
            "logs/2012-03-17.log"
        ],
        "nkeys": 7,
        "bundle": "bin/log_rollup",
        "job_args": {
            "reduce_url": "example.com/reduce"
        },
        "create_time": "2012-03-20T17:19:26.732Z",
        "state": "running",
        "keys_completed": [
                "logs/2012-03-11.log",
                "logs/2012-03-12.log",
                "logs/2012-03-13.log",
                "logs/2012-03-14.log",
                "logs/2012-03-16.log"
        ],
        "nkeys_completed": 5,
        "tasks": [ {
            "taskid": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "host": "2F16",
            "zonename": "e7b35c46-c36d-4e02-8cde-6fdf2695af15", 
            "input_keys": [
                "logs/2012-03-11.log",
                "logs/2012-03-12.log",
                "logs/2012-03-16.log"
            ],
            "output_keys": [
                "log_summary/2012-03-11.txt",
                "log_summary/2012-03-12.txt",
                "log_summary/2012-03-16.txt"
            ],
            "start_time": "2012-03-20T17:19:30.102Z",
            "state": "done",
            "end_time": "2012-03-20T17:21:25.327Z",
            "status": "success"
        }, {
            "taskid": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "host": "2F20",
            "zonename": "b01698b6-7447-11e1-8f74-fbabac25f69a",
            "input_keys": [
                "logs/2012-03-13.log",
                "logs/2012-03-14.log",
                "logs/2012-03-15.log",
                "logs/2012-03-17.log"
            ],
            "start_time": "2012-03-20T17:19:33.302Z",
            "state": "running",
            "output_keys": [
                "log_summary/2012-03-13.txt",
                "log_summary/2012-03-14.txt"
            ],
            "partial_keys": [
                "log_summary/2012-03-15.txt"
            ]
        } ],
        "output_keys": [
            "log_summary/2012-03-11.txt",
            "log_summary/2012-03-12.txt",
            "log_summary/2012-03-13.txt",
            "log_summary/2012-03-14.txt",
            "log_summary/2012-03-15.txt",
            "log_summary/2012-03-16.txt",
            "log_summary/2012-03-17.txt"
        ],
        "job_log": [ {
            "date": "2012-03-20T17:19:26.732Z",
            "event": "state_change",
            "new_state": "queued",
            "message": "job created and queued"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "event": "state_change",
            "new_state": "running",
            "message": "job started running"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "taskid": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "state_change",
            "new_state": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:33.302Z",
            "taskid": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "state_change",
            "new_state": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:40.102Z",
            "taskid": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "state_change",
            "new_state": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:19:41.502Z",
            "taskid": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "state_change",
            "new_state": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:21:25.327Z",
            "taskid": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "state_change",
            "new_state": "done",
            "message": "task completed"
        } ]
    }

This job processes each input object (a daily log file) and outputs a summary
object (for that day's events).  You can see the precise sequence of events by
examining the job log.

In this example, the job was distributed to two nodes, 2F16 and 2F20, where one
task was run on each node.  Each task got a few of the 7 input objects.  The
first task has completed its 3 objects, while the second is processing its 3rd
input key.

## Checking completed job state

The response for a successfully completed job looks like this:

    HTTP/1.1 200 OK
    Location: /jobs/5e42cd1e-34bb-402f-8796-bf5a2cae47db
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:21:40 GMT
    X-Request-Id: a9ad378a-7449-11e1-a7f2-57be1e07436a
    X-Response-Time: 652
    Content-Type: application/json
    Content-MD5: 306f9070b03edd6b3ad9eab91fb2fbb4
    Content-Length: 3503
    
    {
        "jobid": "4bcf467e-4b88-4ab4-b7ab-65fad7464de9",
        "name": "weekly log rollup",
        "input_keys": [
            "logs/2012-03-11.log",
            "logs/2012-03-12.log",
            "logs/2012-03-13.log",
            "logs/2012-03-14.log",
            "logs/2012-03-15.log",
            "logs/2012-03-16.log",
            "logs/2012-03-17.log"
        ],
        "nkeys": 7,
        "bundle": "bin/log_rollup",
        "job_args": {
            "reduce_url": "example.com/reduce"
        },
        "create_time": "2012-03-20T17:19:26.732Z",
        "state": "done",
        "finish_time": "2012-03-20T17:22:35.246Z",
        "status": "success",
        "keys_completed": [
                "logs/2012-03-11.log",
                "logs/2012-03-12.log",
                "logs/2012-03-13.log",
                "logs/2012-03-14.log",
                "logs/2012-03-15.log",
                "logs/2012-03-16.log",
                "logs/2012-03-17.log"
        ],
        "nkeys_completed": 7,
        "tasks": [ {
            "taskid": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "host": "2F16",
            "zonename": "e7b35c46-c36d-4e02-8cde-6fdf2695af15", 
            "input_keys": [
                "logs/2012-03-11.log",
                "logs/2012-03-12.log",
                "logs/2012-03-16.log"
            ],
            "output_keys": [
                "log_summary/2012-03-11.txt",
                "log_summary/2012-03-12.txt",
                "log_summary/2012-03-16.txt"
            ],
            "start_time": "2012-03-20T17:19:30.102Z",
            "state": "done",
            "end_time": "2012-03-20T17:21:25.327Z",
            "status": "success"
        }, {
            "taskid": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "host": "2F20",
            "zonename": "b01698b6-7447-11e1-8f74-fbabac25f69a",
            "input_keys": [
                "logs/2012-03-13.log",
                "logs/2012-03-14.log",
                "logs/2012-03-15.log",
                "logs/2012-03-17.log"
            ],
            "output_keys": [
                "log_summary/2012-03-13.txt",
                "log_summary/2012-03-14.txt",
                "log_summary/2012-03-15.txt",
                "log_summary/2012-03-17.txt"
            ],
            "start_time": "2012-03-20T17:19:33.302Z",
            "state": "done",
            "end_time": "2012-03-20T17:22:35.246Z",
            "status": "success"
        } ],
        "output_keys": [
            "log_summary/2012-03-11.txt",
            "log_summary/2012-03-12.txt",
            "log_summary/2012-03-13.txt",
            "log_summary/2012-03-14.txt",
            "log_summary/2012-03-15.txt",
            "log_summary/2012-03-16.txt",
            "log_summary/2012-03-17.txt"
        ],
        "job_log": [ {
            "date": "2012-03-20T17:19:26.732Z",
            "event": "state_change",
            "new_state": "queued",
            "message": "job created and queued"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "event": "state_change",
            "new_state": "running",
            "message": "job started running"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "taskid": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "state_change",
            "new_state": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:33.302Z",
            "taskid": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "state_change",
            "new_state": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:40.102Z",
            "taskid": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "state_change",
            "new_state": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:19:41.502Z",
            "taskid": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "state_change",
            "new_state": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:21:25.327Z",
            "taskid": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "state_change",
            "new_state": "done",
            "message": "task completed"
        }, {
            "date": "2012-03-20T17:22:35.246Z",
            "taskid": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "state_change",
            "new_state": "done",
            "message": "task completed"
        }, {
            "date": "2012-03-20T17:22:35.246Z",
            "event": "state_change",
            "new_state": "done",
            "message": "job completed"
        } ]
    }

## Checking failed job state

Here's what a failed job's state looks like:

    HTTP/1.1 200 OK
    Location: /jobs/5e42cd1e-34bb-402f-8796-bf5a2cae47db
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:21:40 GMT
    X-Request-Id: a9ad378a-7449-11e1-a7f2-57be1e07436a
    X-Response-Time: 652
    Content-Type: application/json
    Content-MD5: 306f9070b03edd6b3ad9eab91fb2fbb4
    Content-Length: 3503
    
    {
        "jobid": "4bcf467e-4b88-4ab4-b7ab-65fad7464de9",
        "name": "weekly log rollup",
        "input_keys": [
            "logs/2012-03-11.log",
            "logs/2012-03-12.log",
            "logs/2012-03-13.log",
            "logs/2012-03-14.log",
            "logs/2012-03-15.log",
            "logs/2012-03-16.log",
            "logs/2012-03-17.log"
        ],
        "nkeys": 7,
        "bundle": "bin/log_rollup",
        "job_args": {
            "reduce_url": "example.com/reduce"
        },
        "create_time": "2012-03-20T17:19:26.732Z",
        "state": "done",
        "finish_time": "2012-03-20T17:22:35.246Z",
        "status": "failed",
        "error_code": "task_failed",
        "keys_completed": [
                "logs/2012-03-11.log",
                "logs/2012-03-12.log",
                "logs/2012-03-13.log",
                "logs/2012-03-14.log",
                "logs/2012-03-16.log",
                "logs/2012-03-17.log"
        ],
        "nkeys_completed": 6,
        "tasks": [ {
            "taskid": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "host": "2F16",
            "zonename": "e7b35c46-c36d-4e02-8cde-6fdf2695af15", 
            "input_keys": [
                "logs/2012-03-11.log",
                "logs/2012-03-12.log",
                "logs/2012-03-16.log"
            ],
            "output_keys": [
                "log_summary/2012-03-11.txt",
                "log_summary/2012-03-12.txt",
                "log_summary/2012-03-16.txt"
            ],
            "start_time": "2012-03-20T17:19:30.102Z",
            "state": "done",
            "end_time": "2012-03-20T17:21:25.327Z",
            "status": "success"
        }, {
            "taskid": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "host": "2F20",
            "zonename": "b01698b6-7447-11e1-8f74-fbabac25f69a",
            "input_keys": [
                "logs/2012-03-13.log",
                "logs/2012-03-14.log",
                "logs/2012-03-17.log"
            ],
            "output_keys": [
                "log_summary/2012-03-13.txt",
                "log_summary/2012-03-14.txt",
                "log_summary/2012-03-17.txt"
            ],
            "partial_keys": [
                "log_summary/2012-03-15.txt"
            ]
            "start_time": "2012-03-20T17:19:33.302Z",
            "state": "done",
            "end_time": "2012-03-20T17:22:35.246Z",
            "status": "failed"
            "error_code": "failed",
            "task_data": {
                "message": "corrupt log record"
            }
        } ],
        "output_keys": [
            "log_summary/2012-03-11.txt",
            "log_summary/2012-03-12.txt",
            "log_summary/2012-03-13.txt",
            "log_summary/2012-03-14.txt",
            "log_summary/2012-03-16.txt",
            "log_summary/2012-03-17.txt"
        ],
        "job_log": [ {
            "date": "2012-03-20T17:19:26.732Z",
            "event": "state_change",
            "new_state": "queued",
            "message": "job created and queued"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "event": "state_change",
            "new_state": "running",
            "message": "job started running"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "taskid": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "state_change",
            "new_state": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:33.302Z",
            "taskid": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "state_change",
            "new_state": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:40.102Z",
            "taskid": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "state_change",
            "new_state": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:19:41.502Z",
            "taskid": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "state_change",
            "new_state": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:21:25.327Z",
            "taskid": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "state_change",
            "new_state": "done",
            "message": "task completed"
        }, {
            "date": "2012-03-20T17:22:35.246Z",
            "taskid": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "state_change",
            "new_state": "done",
            "message": "task failed"
        }, {
            "date": "2012-03-20T17:22:35.246Z",
            "event": "state_change",
            "new_state": "done",
            "message": "job completed"
        } ]
    }

In this example, we can see that the second task failed partway through.  We
see this from both the task state and the job log.  Since we see that it had
started writing the summary for log_summary/2012-03-15.txt, we know it failed
on this file, and we could next look to see how far it got.  We also have the
error message reported by the task: "corrupt log record".  The more information
the task reports, the better the chance of root causing the failure from the
first occurrence.


# Task API

The Task API is used by tasks (user code running in compute zones on Manta
storage nodes) to retrieve parameters and report on progress.  The API is
serviced not by a public endpoint, but by a small server running inside the
task's own compute zone that's implicitly scoped to that zone's current task.
As a result, task identifiers are not actually necessary in the task API.

## GET /info

Retrieves information about the current job and task.

### Returns

|| **Field** || **Type** || **Description** ||
|| jobid || String || Job identifier ||
|| job_name || String || Job label (human-readable, not unique) ||
|| job_args || Object || Arbitrary user-specified job arguments ||
|| taskid || String || Task identifier ||
|| input_keys || Array&nbsp;of&nbsp;Array&nbsp;of&nbsp;Strings || List of Manta objects this task is expected to process. Each entry of this array is an array with two fields: the object key, and the full path to the corresponding file. ||
|| task_data || Object || Arbitrary task data (intended for task debugging) ||

Object metadata (e.g., user-specified headers) are not included in the task
info.  Users can retrieve this by querying Manta directly.

### Example

    HTTP/1.1 200 OK
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:21:40 GMT
    X-Request-Id: a9ad378a-7449-11e1-a7f2-57be1e07436a
    X-Response-Time: 5
    Content-Type: application/json
    
    {
        "jobid": "5e42cd1e-34bb-402f-8796-bf5a2cae47db",
        "job_name": "weekly log rollup",
        "job_args": {
            "reduce_url": "example.com/reduce"
        },
        "taskid": "be3559ee-713b-43eb-8deb-6ee93f441c23",
        "input_keys": [
            "logs/2012-03-11.log",
            "logs/2012-03-12.log",
            "logs/2012-03-16.log"
        ]
    }
 

## POST /checkpoint

Report that the given keys have been successfully processed.  Tasks *must*
report when they finish processing all of their keys, and *may* also report
piecemeal progress.

This information is used for two purposes: first, it is recorded in the job
state so that end users can see what progress is being made while the job runs.
Second, any objects created since the last progress report are committed to the
job.  See `PUT /object/:key`.

When the last keys have been completed, the task may be terminated immediately,
potentially before this request even completes.  Any cleanup the task wishes to
complete should be done before the final call to this URI.

### Input

|| **Field** || **Type** || **Description** ||
|| keys || Array&nbsp;of&nbsp;Strings || List of objects successfully completed ||

### Returns

Returns 204 (no content) on success.

### Example

Request:

    POST /checkpoint HTTP/1.1
    Authorization: ...
    Host: api.example.com
    Accept: application/json
    Content-Type: application/json

    {
        "keys": [ "logs/2012-03-11.log" ]
    }

Response:

    HTTP/1.1 204 No content
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:19:26 GMT
    X-Request-Id: 4bcf467e-4b88-4ab4-b7ab-65fad7464de9
    X-Response-Time: 32


## PUT /object/:key

Writes a new Manta object named "key" associated with this task.  The arguments,
semantics, and return value for this URI exactly match the Manta API for
creating a new object, except that any object created using this URI since the
last call to `POST /checkpoint` will be discarded if the job fails.  Such
objects will not be included in the job's output, but will be listed in the task
state for *postmortem* analysis.

For examples, see the Manta documentation.

## PUT /task_data

Record up to 16K of task-specific data that will be retrievable by the end user
when they retrieve the job status.  This is intended primarily for debugging:
use it to record error messages, more sophisticated error objects, stats, or
whatever other information may be useful *postmortem*.

### Input

An arbitrary JSON payload smaller than 16K.  Applications where JSON is not
suitable can use a JSON string whose contents are base64-encoded, which allows
you to represent any other kind of data.

For data larger than 16K, create new objects on Manta instead, and if necessary,
record the keys in the task_data.

### Returns

Returns 204 (no content) on success.

### Example

Request:

    POST /task_data HTTP/1.1
    Authorization: ...
    Host: api.example.com
    Accept: application/json
    Content-Type: application/json

    {
        "message": "corrupt log record"
    }

Response:

    HTTP/1.1 204 No content
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:19:26 GMT
    X-Request-Id: 4bcf467e-4b88-4ab4-b7ab-65fad7464de9
    X-Response-Time: 12

## POST /fail

Fail the current task (and job).  Any objects created since the last call to
`POST /checkpoint` will be discarded (but will be linked in the task state for
*postmortem* analysis).

### Input

Any input is saved to task_data as though it had been PUT directly to `/task_data`.
If not input is specified, task_data is unchanged.

### Returns

Returns 204 (no content) on success, but may not return at all, as the task may
be terminated immediately.

### Example

Request:

    POST /fail HTTP/1.1
    Authorization: ...
    Host: api.example.com
    Accept: application/json
    Content-Type: application/json

    {
        "message": "corrupt log record"
    }

Response:

    HTTP/1.1 204 No content
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:19:26 GMT
    X-Request-Id: 4bcf467e-4b88-4ab4-b7ab-65fad7464de9
    X-Response-Time: 12

## POST /abort

Exactly like `POST /fail`, except that additional *postmortem* state will be
kept around and linked to the job state.  Core dumps of currently running user
processes and the entire filesystem contents will be preserved.

# Implementing use cases

## MapReduce

# Implementation

## Compute zones

Sizing
Create, reset
Tuning

## Job implementation

## Task implementation

zsock

## Management API?

cancel tasks, take compute zones and servers in and out of service
