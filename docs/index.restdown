---
title: Marlin
markdown2extras: wiki-tables, code-friendly
apisections: Job API, Task Control API
---

# Marlin

Marlin is a facility for running compute jobs on Manta storage nodes.  This
document describes the desired end state for version 1.


# Design goals

Marlin facilitates the following use cases:

* MapReduce in a generic environment (e.g., log analysis)
* MapReduce in a node-based environment (i.e., make it particularly easy for
  node apps)
* Straight Map with no reduce (e.g., video transcoding)
* Allowing others to run compute on your data (e.g., for government
  organizations with lots of public data, e.g.,
  http://community.topcoder.com/pds-challenge/)
* Hadoop itself?

Of course, we want this to be easy to use, monitor, and debug.  A good
barometer is that word count, the MR equivalent of "hello world", should be
trivial to write.

Marlin defines two public APIs:

* Job API: allows end users to submit, cancel, and monitor compute jobs.  This
  will ultimately become part of the Manta API.
* Task Control API: used by user code running inside Marlin to coordinate
  execution and report progress.

All DateTimes in these APIs use W3C DTF in UTC ("YYYY-MM-DDThh:mm:ss.sZ").

This document assumes familiarity with the Manta object store.


# Overview by example

To explain Marlin, we'll outline how a user would implement distributed word
count algorithm.  This is a distributed version of "wc -w".  On a single
system, that might look like this:

    $ wc -w /usr/share/dict/*
          69 /usr/share/dict/README
         150 /usr/share/dict/connectives
        1323 /usr/share/dict/propernames
      234936 /usr/share/dict/web2
      121847 /usr/share/dict/web2a
      234936 /usr/share/dict/words
      593261 total

Alternatively, you might do this:

    $ cat /usr/share/dict/* | wc -w
    593261

The idea of the distributed version is that you'd like to do something like:

    $ wc -w file1 file2 ...

where file1, file2, ... may be stored on completely different systems, and get
back summary output:

    593261

Let's assume the input files are already stored in Manta:

* file1, file2, file4 on storage node S1
* file3, file5 on storage node S2

though the user doesn't know which files are on which nodes.

To execute the word count, the user submits a Marlin **job** that includes the
following information:

    "jobName": "distributed word count example",
    "inputKeys": [ "file1", "file2", "file3", "file4", "file5" ],
    "phases": [ {
        "exec": "wc -w",
    }, {
        "type": "reduce",
        "exec": "awk '{sum += $1} END{ print sum; }'"
    } ]

"jobName" and "inputKeys" are pretty self-explanatory.  "phases" describes what
to actually do with the data.  By default, phases are **map** operations, so
in the first phase above, Marlin invokes "wc -w" once for each input file, and
passes the files contents on stdin.  Each of these invocations is called a
**task**.  The stdout for "wc" will become input for the next phase.

Because the first phase operates on all five files, but these are stored on
different storage nodes, Marlin splits this phase into two **task groups**, one
for each node containing input files.  The first task group runs three
separate **tasks** in a **compute zone** on node S1, equivalent to "wc -w &lt;
file1", "wc -w &lt; file2", "wc -w &lt; file4".  The second task group runs two
tasks on S2, equivalent to "wc -w &lt; file3" 'and "wc -w &lt; file5".

The stdout of each of these commands becomes the input for the second phase.  As
an implementation detail, these will be stored as temporary files in Manta, but
this is not visible to end users.

The second phase is a **reduce phase** that takes the output objects from the
previous phase and runs the given awk script, producing a single total that's
the sum of all words in all files.  Since there's only one task for a reduce
phase and it combines data from many different nodes, it runs on a
general-purpose compute node, not a Manta storage node.  The output is saved as
a new Manta object which will become the output of the job.

With this example we've covered:

* *jobs*, which users submit to do work with data
* *phases*, which describe individual processing steps of a job
* *map phases*, which run on individual nodes where input objects are stored and
  process one object at a time
* *reduce phases*, which run on separate compute nodes and combine outputs from
  previous phases
* *tasks*, which are individual pieces of a map or reduce phase
* *task groups*, which are the set of tasks that run on a single node
* *compute zones*, which are the environments in which tasks run


# Job API

The Job API is a public-facing API that sits alongside the public Manta endpoint
and lets end users submit, manage, and monitor compute jobs.


## POST /jobs

Submit a new job to be run as soon as possible under Marlin.

Each job defines a set of input Manta objects to process and a set of
**phases**, each describing what to do with each of the input objects.  Each
phase may output any number of manta objects for input to the next phase.  The
output from the last phase is the output of the job.

Each phase of the job is divided into discrete task groups that run one piece of
the phase.  While the job is running, users can retrieve the status of the job
to see which tasks have completed, which tasks are running, which keys have been
processed, and which keys are left.

When all tasks complete, the job itself completes.  Failed tasks are not
retried (though retry options may be provided in a future version of this API).
By default, if a task fails, other tasks are unaffected.  The system continues
running tasks until all of them either fail or complete in order to make as
much progress as possible.  This behavior can be controlled with the "failFast"
job option.


### Inputs

|| **Field** || **Type** || **Description** ||
|| jobName || String || Human-readable job label (not necessarily unique). ||
|| phases || Array&nbsp;of&nbsp;Objects || Array of compute phases.  See below.  ||
|| phaseN.type (optional) || String: "map"&nbsp;or&nbsp;"reduce" || Map phases (the default) process individual objects and are executed on the physical system where those objects are stored.  Reduce phases process many objects at once and are generally executed on separate compute nodes. ||
|| phaseN.count (optional) || Integer || For reduce jobs only, indicates how many reducers to use. See "Pipelined Reducers" below. ||
|| phaseN.assets (optional) || Array of Strings || List of Manta objects that will be made available in the compute zone.  These may be scripts, binaries, libraries, or other objects used to actually run the job, not inputs to the job. ||
|| phaseN.exec || String || Shell command to launch the task.  This are passed directly to "bash -c". ||
|| phaseN.args (optional) || Object || User-defined task arguments.  This is just an arbitrary JSON object that will be available to each task.  Many jobs won't need this because they use shell arguments directly in the "exec" line, but complex tasks may find it easier to process a structured JSON argument. ||
|| phaseN.datasetVersion (optional) || String || Version of compute zone dataset to use.  By default, the latest stable dataset is used. ||
|| inputKeys || Array&nbsp;of&nbsp;Strings || List of Manta objects to process. ||
|| failFast (optional) || Boolean || Cancel all tasks when any task fails.  Otherwise, tasks continue running even after one of them fails (in attempt to make as much progress as possible). ||

Jobs can have any number of map or reduce phases.  For examples:

* N &gt; 1 map phases, 0 reduce phases: file conversion (e.g., video
  transcoding, image rendering)
* N &gt; 1 map phases, 1 reduce phase: traditional map-reduce (e.g., word count)
* N &gt; 1 map phases, M &gt; 1 reduce phases: MR with pipelined reducers
* 0 map phases, N reduce phases: in-place compute

To process a map phase, the system divides the phase into task groups, where
each task group runs a piece of the overall job on a Manta node where some of
the input objects are stored.  The task group is divided into tasks, each of
which processes exactly one input key.  Reduce phases are also divided into
tasks, with one task per reducer.  (But most jobs will have at most 1 reducer.)

Both map and reduce tasks must report when they have succeeded or failed.  The
system may time out map tasks that are taking too long.

You specify the code that runs in each task using the "assets", "exec", and
"args" properties of the phase.  Any Manta keys listed in "phaseN.assets" will
be made available under "/assets" in the compute zone before the task is
started.  This can be used to distribute applications and libraries that will
actually process the input to each compute zone.  "assets" should *not* be used
to specify input keys.  Use "inputKeys" for that.

The task is started by invoking "phaseN.exec".  This may be any shell-style
command, including environment variables, shell arguments, and pipelines.
The shell variable "$mc\_input\_file" may be used to refer to input key files.

Additional structured arguments may be stored in "args", which can be retrieved
by the task's code using the Task Control API.

### Returns

On success, returns 201 with the URI for the job in a Location header and the
state of the job as returned by `GET /jobs/:jobId`.

### Examples

Here's an example job that uses a script stored on Manta for the body of the
task:

    {
        "jobName": "monthly log summary",
        "phases": [ {
            "assets": [ "bin/log_rollup" ],
            "exec": "/assets/bin/log_rollup",
            "args": { "reducer": "example.com/reducer" }
        } ],
        "inputKeys": [
            "logs/2012-03-01.log",
            "logs/2012-03-02.log",
            "logs/2012-03-03.log",
            ...
            "logs/2012-03-31.log"
        ]
    }

See use cases below for more examples.


## GET /jobs

Returns the list of a user's jobs.

### Inputs

|| **Field** || **Type** || **Description** ||
|| state || Array of Strings || Returns only jobs in one of the given states. ||

### Returns

On success, returns 200 with an array of objects, each with the following
fields:

|| **Field** || **Type** || **Description** ||
|| jobId || String || See "Inputs" to `POST /jobs`. ||
|| jobName  || String || See "Inputs" to `POST /jobs`. ||
|| state || String || See "Returns" for `GET /jobs/:jobId`. ||

### Example

Example input:

    GET /jobs?state=running

Example return value:

    [ {
        "jobId": "acfec1f0-7779-11e1-81f9-979db30d72bf",
        "jobName": monthly log summary",
        "state": "running",
    } ]


## POST /jobs/:jobId/cancel

Asynchronously cancels the given job, if the job has not yet completed.  If the
job has completed, this operation does nothing.  It may take several minutes to
cancel a job.

### Returns

Returns 204 (no content) on success.


## GET /jobs/:jobId

Returns basic information about a submitted job.  For detailed runtime status,
see `GET /jobs/:jobId/status`.


### Returns

Returns 200 with at least the following fields:

|| **Job&nbsp;field** || **Type** || **Description** ||
|| jobId || String || Unique identifier for this job. ||
|| jobName || String || See "Inputs" to `POST /jobs`. ||
|| phases || Array of Objects || See "Inputs" to `POST /jobs`. ||
|| inputKeys || Array&nbsp;of&nbsp;Strings || See "Inputs" to `POST /jobs`. ||
|| createTime || DateTime&nbsp;String || Job submission time. ||
|| state || String&nbsp;(see&nbsp;below) || Describes whether the job is queued, running, or done.  See below. ||

The job **state** is one of:

|| **Job&nbsp;state** || **Meaning** ||
|| queued || The job has been created but it's not yet running. That is, no tasks have been instantiated to execute the job yet. ||
|| running || The job is currently running on at least one storage node. That is, one or more tasks have been instantiated to execute the job. Some or all of those tasks may have already completed, but not all tasks have been instantiated and completed. ||
|| done || All of the job's tasks have completed or the job has been aborted. ||

For jobs in the "done" state, these additional fields will be present:

|| finishTime || DateTime&nbsp;String || Job completion time. ||
|| result || "success" or "failure" || Describes whether the job completed successfully. ||
|| reason || String&nbsp;(see&nbsp;below) || If the job result is "failed", this describes why the job failed. ||

For failed jobs, the **reason** describes why the job failed:

|| **Job&nbsp;error&nbsp;code** || **Meaning** ||
|| jobCancelled || the job was explicitly cancelled ||
|| taskFailed || one or more of the job's tasks failed ||


### Example

Here's example state for a job which has not yet started running:

    {
        "jobId": "acfec1f0-7779-11e1-81f9-979db30d72bf",
        "jobName": monthly log summary",
        "phases": [ {
            "assets": "bin/log_rollup",
            "exec": "/assets/bin/log_rollup"
        } ],
        "inputKeys": [
            "logs/2012-03-01.log",
            "logs/2012-03-02.log",
            "logs/2012-03-03.log",
            ...
            "logs/2012-03-31.log"
        ],
        "createTime": "2012-03-20T17:19:26.732Z",
        "state": "queued"
    }

Here's an example state for the same job after it has finished successfully:

    {
        "jobId": "acfec1f0-7779-11e1-81f9-979db30d72bf",
        "jobName": monthly log summary",
        "phases": [ {
            "assets": "bin/log_rollup",
            "exec": "/assets/bin/log_rollup",
            "args": { "reducer": "example.com/reducer" }
        } ],
        "inputKeys": [
            "logs/2012-03-01.log",
            "logs/2012-03-02.log",
            "logs/2012-03-03.log",
            ...
            "logs/2012-03-31.log"
        ],
        "createTime": "2012-03-20T17:19:26.732Z",
        "state": "done",
        "result": "success",
        "finishTime": "2012-03-20T18:37:52.182Z"
    }

Here's an example where the job failed:

    {
        "jobId": "acfec1f0-7779-11e1-81f9-979db30d72bf",
        "jobName": monthly log summary",
        "phases": [ {
            "assets": "bin/log_rollup",
            "exec": "/assets/bin/log_rollup"
        } ],
        "inputKeys": [
            "logs/2012-03-01.log",
            "logs/2012-03-02.log",
            "logs/2012-03-03.log",
            ...
            "logs/2012-03-31.log"
        ],
        "createTime": "2012-03-20T17:19:26.732Z",
        "state": "done",
        "result": "failed",
        "reason": "taskFailed",
        "finishTime": "2012-03-20T18:37:52.182Z"
    }


## GET /jobs/:jobId/status

Retrieves detailed status information about a job.  The system makes a best
effort to keep this information up to date, but any of the runtime state may be
outdated or inconsistent while a job is in the "running" state.

### Returns

This URI returns all of the fields returned by `GET /jobs/:jobId`, plus these
extra fields:

|| **Detailed job fields** || **Type** || **Description** ||
|| doneKeys || Array&nbsp;of&nbsp;Strings || Manta keys that have been processed successfully. ||
|| outputKeys || Array&nbsp;of&nbsp;Strings || Manta keys output by this job. ||
|| jobLog || Array&nbsp;of&nbsp;Objects || Log of job activity (see below). ||
|| taskGroups || Array&nbsp;of&nbsp;Objects || Individual task group states (see below). ||

#### Job log

The **jobLog** field includes entries for all state changes for the job itself
as well as all tasks associated with the job.  This is useful for understanding
job failures and performance *postmortem*.  Each entry has the following
fields:

|| **Job&nbsp;log&nbsp;entry&nbsp;field** || **Type** || **Description** ||
|| date || DateTime&nbsp;String || Time of log entry ||
|| taskId&nbsp;(task&nbsp;entries&nbsp;only) || String || Identifier for a given task ||
|| event || String || Either "stateChange" or "message" ||
|| message || String || Human-readable message ||
|| newState&nbsp;(state&nbsp;change&nbsp;entries&nbsp;only) || String || New state name ||

#### Job progress

Callers can see precisely which keys have been completed by examining
"doneKeys".  For a rough measure of progress, callers can compare
"doneKeys.length" to "inputKeys.length".

The **taskGroups** field shows more detailed information about how the job was
distributed to various servers and the status of each task on each server.  Each
entry of this array describes an individual task, including:

XXX This section will need more changes to reflect the change from task to task
group.  Users may only care about tasks, though operators will likely care about
task groups as well.

|| **Task&nbsp;field** || **Type** || **Meaning** ||
|| taskId || String || Unique identifier for this task within this job. ||
|| host || String || Unique identifier for the physical server on which this task ran. ||
|| machine || String || Unique identifier for the compute zone in which the task ran. ||
|| inputKeys || Array&nbsp;of&nbsp;Strings || List of keys assigned to this task. ||
|| startTime || DateTime&nbsp;String || When this task was started. ||
|| state || String || Describes the current state of the task (see below). ||
|| outputKeys || Array&nbsp;of&nbsp;Strings || List of keys output by this task. ||
|| endTime || DateTime&nbsp;String || The time when the task finished (done tasks only). ||
|| result || "result" or "failure" || Describes whether the task completed successfully (done tasks only). ||
|| partialKeys || List&nbsp;of&nbsp;Strings || List of Manta objects created by the task but not committed because the task failed or is still running. ||

The task **state** is one of:

|| **Task&nbsp;state** || **Meaning** ||
|| queued || The task has been planned, but has not started running yet. ||
|| loading || Marlin is preparing the compute zone to run the task. This step includes downloading and extracting the job's assets. ||
|| running || The task is currently running. ||
|| done || The task is no longer running. ||

For failed tasks, the **reason** indicates why the task failed:

|| **Error&nbsp;code** || **Meaning** ||
|| abnormalExit || The task exited with non-zero status, or exited without indicating that it had finished processing. ||
|| failed || The task explicitly failed. ||
|| timeout || The task did not complete within the alloted time interval. ||


### Examples

The response for a successfully completed job looks like this:

    {
        "jobId": "4bcf467e-4b88-4ab4-b7ab-65fad7464de9",
        "jobName": "weekly log summary",
        "phases": [ {
            "assets": "bin/log_rollup",
            "exec": "/assets/bin/log_rollup"
        } ],
        "inputKeys": [
            "logs/2012-03-11.log",
            "logs/2012-03-12.log",
            ...
            "logs/2012-03-17.log"
        ],
        "createTime": "2012-03-20T17:19:26.732Z",
        "state": "done",
        "finishTime": "2012-03-20T17:22:35.246Z",
        "result": "success",
        "doneKeys": [
            "logs/2012-03-11.log",
            "logs/2012-03-12.log",
            ...
            "logs/2012-03-17.log"
        ],
        "taskGroups": [ {
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "host": "2F16",
            "machine": "e7b35c46-c36d-4e02-8cde-6fdf2695af15", 
            "inputKeys": [
                "logs/2012-03-11.log",
                "logs/2012-03-12.log",
                "logs/2012-03-16.log"
            ],
            "outputKeys": [
                "log_summary/2012-03-11.txt",
                "log_summary/2012-03-12.txt",
                "log_summary/2012-03-16.txt"
            ],
            "startTime": "2012-03-20T17:19:30.102Z",
            "state": "done",
            "endTime": "2012-03-20T17:21:25.327Z",
            "result": "success"
        }, {
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "host": "2F20",
            "machine": "b01698b6-7447-11e1-8f74-fbabac25f69a",
            "inputKeys": [
                "logs/2012-03-13.log",
                "logs/2012-03-14.log",
                "logs/2012-03-15.log",
                "logs/2012-03-17.log"
            ],
            "outputKeys": [
                "log_summary/2012-03-13.txt",
                "log_summary/2012-03-14.txt",
                "log_summary/2012-03-15.txt",
                "log_summary/2012-03-17.txt"
            ],
            "startTime": "2012-03-20T17:19:33.302Z",
            "state": "done",
            "endTime": "2012-03-20T17:22:35.246Z",
            "result": "success"
        } ],
        "outputKeys": [
            "log_summary/2012-03-11.txt",
            "log_summary/2012-03-12.txt",
            "log_summary/2012-03-13.txt",
            "log_summary/2012-03-14.txt",
            "log_summary/2012-03-15.txt",
            "log_summary/2012-03-16.txt",
            "log_summary/2012-03-17.txt"
        ],
        "jobLog": [ {
            "date": "2012-03-20T17:19:26.732Z",
            "event": "stateChange",
            "newState": "queued",
            "message": "job created and queued"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "event": "stateChange",
            "newState": "running",
            "message": "job started running"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:33.302Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:40.102Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:19:41.502Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:21:25.327Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "done",
            "message": "task completed"
        }, {
            "date": "2012-03-20T17:22:35.246Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "done",
            "message": "task completed"
        }, {
            "date": "2012-03-20T17:22:35.246Z",
            "event": "stateChange",
            "newState": "done",
            "message": "job completed"
        } ]
    }

In this example, the job was distributed to two nodes, 2F16 and 2F20, where one
task group was run on each node.  Each task group got a few of the 7 input
objects.  Both task groups have completed processing all of their objects.

Here's what a failed job's state looks like:

    {
        "jobId": "4bcf467e-4b88-4ab4-b7ab-65fad7464de9",
        "jobName": "weekly log summary",
        "phases": [ {
            "assets": "bin/log_rollup",
            "exec": "/assets/bin/log_rollup"
        } ],
        "inputKeys": [
            "logs/2012-03-11.log",
            "logs/2012-03-12.log",
            "logs/2012-03-13.log",
            "logs/2012-03-14.log",
            "logs/2012-03-15.log",
            "logs/2012-03-16.log",
            "logs/2012-03-17.log"
        ],
        "createTime": "2012-03-20T17:19:26.732Z",
        "state": "done",
        "finishTime": "2012-03-20T17:22:35.246Z",
        "result": "failed",
        "reason": "taskFailed",
        "doneKeys": [
                "logs/2012-03-11.log",
                "logs/2012-03-12.log",
                "logs/2012-03-13.log",
                "logs/2012-03-14.log",
                "logs/2012-03-16.log",
                "logs/2012-03-17.log"
        ],
        "taskGroups": [ {
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "host": "2F16",
            "machine": "e7b35c46-c36d-4e02-8cde-6fdf2695af15", 
            "inputKeys": [
                "logs/2012-03-11.log",
                "logs/2012-03-12.log",
                "logs/2012-03-16.log"
            ],
            "outputKeys": [
                "log_summary/2012-03-11.txt",
                "log_summary/2012-03-12.txt",
                "log_summary/2012-03-16.txt"
            ],
            "startTime": "2012-03-20T17:19:30.102Z",
            "state": "done",
            "endTime": "2012-03-20T17:21:25.327Z",
            "result": "success"
        }, {
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "host": "2F20",
            "machine": "b01698b6-7447-11e1-8f74-fbabac25f69a",
            "inputKeys": [
                "logs/2012-03-13.log",
                "logs/2012-03-14.log",
                "logs/2012-03-17.log"
            ],
            "outputKeys": [
                "log_summary/2012-03-13.txt",
                "log_summary/2012-03-14.txt",
                "log_summary/2012-03-17.txt"
            ],
            "partialKeys": [
                "log_summary/2012-03-15.txt"
            ]
            "startTime": "2012-03-20T17:19:33.302Z",
            "state": "done",
            "endTime": "2012-03-20T17:22:35.246Z",
            "result": "failed",
            "reason": "failed",
            "error": {
                "message": "corrupt log record"
            }
        } ],
        "outputKeys": [
            "log_summary/2012-03-11.txt",
            "log_summary/2012-03-12.txt",
            "log_summary/2012-03-13.txt",
            "log_summary/2012-03-14.txt",
            "log_summary/2012-03-16.txt",
            "log_summary/2012-03-17.txt"
        ],
        "jobLog": [ {
            "date": "2012-03-20T17:19:26.732Z",
            "event": "stateChange",
            "newState": "queued",
            "message": "job created and queued"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "event": "stateChange",
            "newState": "running",
            "message": "job started running"
        }, {
            "date": "2012-03-20T17:19:30.102Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:33.302Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "loading",
            "message": "task started loading"
        }, {
            "date": "2012-03-20T17:19:40.102Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:19:41.502Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "running",
            "message": "task started running"
        }, {
            "date": "2012-03-20T17:21:25.327Z",
            "taskId": "be3559ee-713b-43eb-8deb-6ee93f441c23",
            "event": "stateChange",
            "newState": "done",
            "message": "task completed"
        }, {
            "date": "2012-03-20T17:22:35.246Z",
            "taskId": "50a4ec2a-7447-11e1-89c2-d7602b9e1ea2",
            "event": "stateChange",
            "newState": "done",
            "message": "task failed"
        }, {
            "date": "2012-03-20T17:22:35.246Z",
            "event": "stateChange",
            "newState": "done",
            "message": "job completed"
        } ]
    }

In this example, we can see that the second task group failed partway through.
We see this from both the task group state and the job log.  Since we see that
it had started writing the summary for log\_summary/2012-03-15.txt, we know it
failed on this file, and we could next look to see how far it got.  We also have
the error message reported by the task group: "corrupt log record".  The more
information the task reports, the better the chance of root causing the failure
from the first occurrence.


# Task Control API

The Task Control API is used by tasks (user code running in compute zones on
Manta storage nodes) to retrieve parameters and report on progress.  The API is
serviced not by a public endpoint, but by a small server running inside the
task's own compute zone that's implicitly scoped to that zone's current task.
As a result, task identifiers are not actually necessary in the task API.

This API is *not* used to monitor task status; that's provided by the
public-facing Job API.

## GET /info

Retrieves information about the current job and task.

### Returns

This field returns the entire job info (as `GET /jobs/:jobid` does) plus the
following fields:

|| **Field** || **Type** || **Description** ||
|| taskId || String || Task identifier ||
|| taskInputKey || Array&nbsp;of&nbsp;Strings || An array with two fields: the object key, and the full path to the corresponding file. ||

Object metadata (e.g., user-specified headers) are not included in the task
info.  Users can retrieve this by querying Manta directly.
 

## POST /commit

Report that the given key has been successfully processed.  Tasks *must* report
when they finish processing each key.

This information is used for two purposes: first, it is recorded in the job
state so that end users can see what progress is being made while the job runs.
Second, any objects created since the last progress report are committed to the
job.  See `PUT /object/:key`.

When the key has been completed, the task may be terminated immediately,
potentially before this request even completes.  Any cleanup the task wishes to
complete should be done before the final call to this URI.

### Input

|| **Field** || **Type** || **Description** ||
|| key || String || Key for input object successfully completed ||

### Returns

Returns 204 (no content) on success.

### Example

Request:

    {
        "key": "logs/2012-03-11.log"
    }

Response:

    HTTP/1.1 204 No content
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:19:26 GMT
    X-Request-Id: 4bcf467e-4b88-4ab4-b7ab-65fad7464de9
    X-Response-Time: 32


## PUT /object/:key

Writes a new Manta object named "key" associated with this task.  The arguments,
semantics, and return value for this URI exactly match the Manta API for
creating a new object, with the following additions:

* Any object created using this URI since the last call to `POST /commit` will
  be discarded if the job fails.  Such objects will not be included in the
  job's output, but will be listed in the task state for *postmortem*
  debugging.
* Jobs using pipelined reducers may specify a "X-Marlin-Reducer" header whose
  value is an integer from 0 to N - 1, where N is the number of reducers in the
  next phase.  Any other input results in a 409 Conflict error.  This value
  indicates which of several reducers this output file should be fed to.  If
  this value is unspecified and there are multiple reducers in the next phase,
  output keys will be randomly assigned to reducers.

For examples, see the Manta documentation.

## POST /fail

Fail the current task.  Any objects created since the last call to `POST
/commit` will be discarded (but will be linked in the task state for
*postmortem* analysis).

### Input

The input should have at least the following fields:

|| **message** || String || human-readable error message ||

It's also recommended to include a "code" field for a machine-parseable error
code in string form.  Other fields may also be specified up to a maximum
request size.

### Returns

Returns 204 (no content) on success, but may not return at all, as the task may
be terminated immediately.

Callers can assume that this call will never fail.


### Example

Request:

    {
        "message": "corrupt log record"
    }

Response:

    HTTP/1.1 204 No content
    Access-Control-Allow-Origin: *
    Access-Control-Allow-Methods: GET, POST
    Connection: close
    Date: Tue, 20 Mar 2012 17:19:26 GMT
    X-Request-Id: 4bcf467e-4b88-4ab4-b7ab-65fad7464de9
    X-Response-Time: 12

## POST /abort

Exactly like `POST /fail`, except that additional *postmortem* state will be
kept around and linked to the job state.  Core dumps of currently running user
processes and the entire filesystem contents will be preserved.

# Implementing use cases

## Built-in facilities

We'll provide several facilities for dealing with common tasks in MR jobs.

### mpipe: output to the next job pipeline

mpipe [&lt;manta_key&gt;] [reducer]

Reads content from stdin and outputs it from this task.  If &lt;manta\_key&gt;
is not given, a new name will be created.

This is exactly equivalent to:

    $ curl -X PUT -T- http://localhost/object/$manta_key

If "reducer" is specified, it must be an integer from 0 to N - 1, where N is the
number of reducers in the next stage of the job pipeline.  This indicates that
this object should be routed to the given reducer.


### aggr: aggregate rows of numbers

aggr &lt;sum|avg|pctile99...&gt;

Reads a set of "key: value" pairs on stdin, aggregates them key-wise according
to the given function, and emits the result.

Example input:

    $ cat data.txt
    manta: 15
    marlin: 12.8
    manta: 38
    moray: 3

Example output:

    $ aggr sum < data.txt
    manta: 53
    marlin: 12.8
    moray: 3

The "key:" bit may also be omitted.


## Total word count

Input: a bunch of text files

Output: a count of the total number of words in all files

    {
        "jobName": "total word count"
        "inputKeys": [ ... ],
        "phases": [ {
            "exec": "wc -w"
        }, {
            "type": "reduce",
            "exec": "aggr sum"
        } ]
    }

## Specific word count

Input: a bunch of text files and a word "baseball"

Output: the total number of occurences of "baseball" in all files

    {
        "jobName": "'baseball' count"
        "inputKeys": [ ... ],
        "phases": [ {
            "exec": "grep -c baseball"
        }, {
            "type": "reduce",
            "exec": "aggr sum"
        } ]
    }

## Word frequency count (and using a custom asset)

Input: a bunch of files

Output: a text table for the frequency count of each word; e.g.:

    marlin: 527
    manta: 328
    moray: 356

For this case, you'll need a script that computes this for a given text file.
This will do:

    $ cat wordfreq
    #!/usr/bin/perl -W
    
    $counts = {};
    
    while (<>) {
        foreach(split) {
            s/^\W+//;
            s/\W+$//;
            next unless $_;
            $counts->{lc($\_)}++ 
        }
    }
    
    while (($word, $count) = each(%$counts)) {
        printf("%s: %d\n", $word, $count);
    }

You'll first need to PUT this to manta, say at /dap/bin/wordfreq.  Then the job
looks like this:

    {
        "jobName": "word frequency count"
        "inputKeys": [ ... ],
        "phases": [ {
            "assets": [ "/dap/bin/wordfreq" ],
            "exec": "/dap/bin/wordfreq"
        }, {
            "type": "reduce",
            "exec": "aggr sum",
        } ]
    }

If you wanted, you could rewrite "wordfreq" as an awk one-liner and include it
directly in the "exec" property, but this may be a bit unwieldy.

## Pipelined reducers

Sometimes it's necessary to pipeline the reduce phase so that instead of
processing all input keys in one pass, the input keys are processed in chunks
across multiple passes.  For example, you may want to process half of the input
keys in each of two parallel tasks, and then process the output of that, to
avoid having to load the entire data set in memory at once.

We already support multiple "reduce" phases in a job, but to support this you
also need to parallelize a single reduce phase.  You can do this with the
"count" property:

    "phases": [
        ... /* any number of map phases */
        , {
            "type": "reduce",
            "exec": ...,
            "count": 2
        }, {
            "type": "reduce",
            "exec": ...
        }
    ]

In this example, some number of map phases are followed by a phase with two
reducers running in parallel, followed by a final reduce stage.  The two
parallel reducers are identical except for their input and output keys.  By
default, input keys are randomly assigned to one of the reducers.  If a more
sophisticated assignment is necessary, the previous map phase may specify to
which reducer a given output object will be assigned using the X-Marlin-Reducer
header, which must be an integer between 0 and N - 1, where N is the total
number of reducers in the next phase.


## Node, Python, Ruby, etc. interface

The Jobs API is built in terms of Unix shell commands for generality.  This
makes it easy to run simple jobs like "grep" and "wc".  For slightly more
complex cases, custom utilities can be built in the Unix style and composed with
existing system utilities without writing significant new code.  For more
complex cases you want entirely custom code, you can simply run your program via
the "exec" string.

We'll also provide a higher-level interface for Node.js, so that people that
want to build jobs with custom code can use Node easily.  Likely we'll have
consumers implement an interface like:

    function processObject(file, callback) { ... }

and we'll provide convenience routines for API operations (like "taskParams()",
"commit()", "abort()", and so on).

Without fleshing this out completely, we know such an interface can be
implemented without changes to the Jobs API.  When people want to use the Node
API, we tell them to write phases like this:

    "phases": [ {
        "assets": [ "my/npm/package" ],
        "exec": "mnode /assets/my/npm/package"
    } ]

"mnode" is an executable Node.js program included with the dataset that installs
the given npm package (and therefore its dependencies, too) and then invokes our
Marlin interface in that package.

If we care to, we can do something similar for Python, Ruby, R, or any other
environment.  It would be just as easy for third parties to build their own.
Users would only need to reference a third party asset:

    "phases": [ {
        "assets": [
            "third-party/framework/start.sh"
            "my/script"
        ],
        "exec": "/assets/third-party/framework/start.sh /assets/my/script" 
    } ]

## Log analysis (MapReduce)

## Image conversion / video transcoding (Straight Map)

This job uses the "convert" command that's part of the ImageMagick suite.  It
converts image files between types based on file extension.

    {
        "jobName": "convert GIFs to PNGs",
        "inputKeys": [ "file1.gif", "file2.gif", ... ]
        "phases": [ {
            "exec": "output=$(basename $mc_input_file .gif).png; convert $mc_input_file $output && mpipe < $output"
        }
    }


## Node interface

TBD.

## Hadoop proper

TBD.


# Marlin implementation

## Marlin network architecture

There are four main components in the Marlin architecture:

* Web tier, which handles requests to GET job state and POST new jobs. 
* Job worker tier, which assigns tasks to individual storage and compute nodes
  and monitor task progress.
* Compute node agents, which execute and monitor tasks and report progress and
  completion.
* Moray, where all Marlin state is stored.

There's a cluster of web tier workers and job workers in each datacenter, but
there's no real DC-awareness among these components.

**Job state**

To keep the service resilient to partitions and individual node failures, **all
job state is stored in Moray** (i.e., all components are essentially
stateless), and **all communication happens via Moray**.  Rather than talking
to each other directly, each component drops state changes into Moray and polls
for whatever other state changes it needs to know about.  This model makes it
relatively easy to deal with individual component failure and to scale
horizontally.  See "Invariants and failure modes" below for details.

Recall that Moray spans datacenters, is eventually consistent, and remains
available in the case of minority DC partitions.  Using Moray in this way also
implies that Moray becoming unavailable makes Marlin unavailable, and that
Moray must be able to keep up with the Marlin workload.  See "Scalability"
below.

Jobs are stored inside a single Moray bucket called "marlin\_jobs", with
indexed fields that allow the web tier, job workers, and agents to poll on the
subset of jobs that they care about.


**Web tier**

Workers retrieve current job state by querying "marlin\_jobs" for the job with
the given jobId.

When a new job is submitted, the web tier allocates a new jobId and drops the
job definition into "marlin\_jobs".


**Job workers**

To synchronize changes to job state, each job is assigned to one worker at a
time.  This assignment is stored in the job object in Moray.  An assignment is
valid as long as the job's modification time (mtime) is within the last
WORKER\_TIMEOUT seconds.

This works as follows: each job worker periodically queries Moray for jobs
whose state is not "done" and either (a) the "worker" field is undefined or (b)
the modification time is more than WORKER\_TIMEOUT seconds ago.  (This is a
relatively simple query on indexed fields.)  For each job the worker finds, it
uses a test-and-set operation to assign the job to itself.  In this way,
workers pick up both new jobs and jobs whose assigned workers appear to have
failed.  A worker must update each job every WORKER\_TIMEOUT seconds, even if
there's no meaningful progress to report, to indicate that the worker is still
handling that job.  If the worker sees that WORKER\_TIMEOUT seconds has elapsed
since it last updated the job, it must discard any job state it has and stop
working on the job to avoid conflicting with another worker who picks up the
job.  (In practice, the best way to implement this may simply be to have all
state updates use a test-and-set that checks whether worker == me.)

When a worker picks up a newly submitted job, the first thing it has to do is
query Moray to find out where all the input keys are.  Moray itself has many
independent partitions and uses a consistent hash ring to map a given key to
the Moray partition that holds the metadata for that key.  The worker takes all
the input keys, executes the consistent hash function to group them by Moray
partition, and performs a batch request to each partition to determine which
storage nodes each key is actually stored on.

The worker than divides the first phase of the job into **task groups**, each of
which will run on one storage node and process whichever input keys are stored
on that node.  These task assignments are written out to a Moray bucket called
"marlin\_tasks" with a field identifying the hostname of the node on which the
task will run.  The agents running on each node poll Moray for tasks assigned to
them, process each one, and update the task object.  The job worker polls on
these task updates, updates the global job state with job progress (so the web
tier can see it) and advances the job when each phase completes.

When a phase completes (as determined by the worker when all of its tasks have
completed), the worker begins processing the next phase in the same way it did
the first one: by locating the objects within Manta and then assigning tasks to
individual nodes.  When all phases complete, the job is marked done.

**Compute node agents**

As described above, compute node agents monitor tasks assigned to them in
"marlin\_tasks".  As their tasks make progress, they update the task record,
which is also monitored by a job worker.

**Invariants and failure modes**

Invariants:

* All state is stored in Moray, so all components are stateless.
* At most one worker can ever "own" a job.  Only the owner of a job can write
  task assignments or update the job record in "marlin\_jobs".
* To a first approximation, a component is partitioned iff it cannot reach
  Moray.  This is an oversimplification because Moray itself is sharded, so it's
  theoretically possible to be partitioned from one shard but not another.
  This case is very unlikely, since each shard has a presence in each DC, but
  the system will handle this.  Most importantly, the only partitioning that
  matters is that between a component and a given Moray shard.

If a user's code fails within a task (e.g., dumps core), and the on-node retry
policy has been exhausted, the compute agent reports the task as failed, and
that will become visible in the final job state.  It's the user's problem at
that point.

If a compute node agent fails transiently (e.g., dumps core and restarts), it
should be able to pick up where it left off.  This should be invisible to the
user's code and the rest of the system, modulo slightly increased task
completion time.  (The agent must synchronously write state to local disk to
facilitate this.  If this storage fails, that's equivalent to the compute node
agent failing persistently.)

If a compute node agent either fails for an extended period *or* becomes
partitioned from its Moray shard (which are indistinguishable to the outside
world), the job worker redispatches any uncompleted work to other nodes that
have the same data.  It does not cancel the task on the failed node.  If both
tasks eventually complete successfully, the worker will necessarily see one's
results first and discard the other's.  (That's why the state in "marlin\_tasks"
is not authoritative job state, and why the worker must "commit" this state to
the job record in "marlin\_jobs" for the web tier to see it, rather than having
the web tier scan "marlin\_tasks" for itself.)

If a job worker fails transiently, it should be able to pick up where it left
off, since all state is stored in Moray.  This should be invisible to the rest
of the system, modulo slightly increased job completion time.

If a job worker fails for an extended period, another job worker will pick up
its jobs.  This becomes exactly like the case where a worker fails transiently:
the state in Moray is sufficient for the new worker to pick up where the old one
left off.

If a job worker becomes partitioned from one or more Moray shards, then it will
be unable to process task updates from compute nodes reporting to those shards.
It may end up trying to redispatch tasks assigned to those nodes to other nodes.
If it can talk to enough shards to complete the phase, the phase will complete;
otherwise, it will stall until connectivity is restored, at which point the
worker will see the completed task state and finish the phase.

If a job worker becomes partitioned from the Moray shard storing the job state
for WORKER\_TIMEOUT seconds, this will be treated as an extended worker failure.
Another worker will pick up the job where the first one left off.  The first
worker won't clobber the job state after someone else has picked up the job
because it also knows that WORKER\_TIMEOUT has elapsed.

Web tier nodes are completely stateless.  Any failure results in failing any
in-flight requests.  Persistent failures have no additional impact as long as
enough nodes are in service to handle load.  Partitions between the web node and
the Moray shard storing job records would result in failed requests.

If an entire datacenter becomes partitioned from the rest, then its web tier
nodes certainly won't be able to service any requests.  All of its job workers
become partitioned (see above -- the work should eventually be picked up by
workers in other datacenters).  All of its compute node agents effectively also
become partitioned, and their work *may* be duplicated in other datacenters.
Importantly, jobs submitted to the majority partition should always be able to
complete successfully in this state, since there's always enough copies of the
data to process the job.

**Scalability**

Being stateless, web tier nodes can be scaled arbitrarily horizontally.  If load
exceeds capacity, requests will queue up and potentially eventually fail, but
alarms should ring long before that happens.

Similarly, job workers can be scaled arbitrarily horizontally, and capacity
monitoring should allow administrators to ensure that sufficient capacity is
always available.  The failure mode for excess load is simply increased job
completion time.  Task updates may accumulate in Moray faster than the worker
can process them, but the total space used in Moray is the same regardless of
how fast the job workers process them, so there's no cascading failure.  If job
workers cannot update their locks in Moray, the system may thrash as workers try
to take each others' jobs, but this would be very extreme.

Compute nodes rely on the OS to properly balance the workload within that box.
They may queue tasks after some very large number of concurrent tasks, but it's
not anticipated that we would ever reach that point in practice.  If this
became a problem, job workers could conceivably redispatch the task to another
node with the same data available.  If this truly became a serious problem, we
could support compute agents that download data, rather than operating locally
(just as reducers already do).

Given that there's enough capacity in each of the Marlin tiers, the remaining
question is Moray.  Moray will be sharded, and we will say that each compute
node reports its tasks to exactly one (uniformly distributed) shard.  All job
state will be stored on a single shard, since the number of updates is
relatively minimal.  We should be able to reshard to scale this out.

Additionally, each compute node can batch its Moray updates from all tasks for
up to N seconds, sharply reducing the number of requests required.  Similarly,
job workers can batch job state updates for many jobs up to N seconds.  Batching
of "location" GET requests is discussed above, under "job workers".  Thus, the
total number of Moray-wide requests should be boundable to something like (# of
workers \* # of compute zone agents) / (N seconds), where N is how frequently
each agent updates Moray.  These requests should mostly be distributed across
all shards.

Batching requests for the same record over multiple seconds reduces both network
throughput and database operations, as multiple state changes can be combined at
the source.

Batching requests over multiple records helps network throughput, but not the
total number of database operations, since each record must be operated on
separately.  The total number of database operations could still be around (# of
active tasks \* # of agents \* # of active jobs \* # of job workers).  Again,
these are distributed across many Moray shards, which can be scaled
horizontally.

## Objects

There are two types of objects stored in Moray: jobs and task group
assignments.  Here's what an internal job record looks like:

    {
        "version": 0,                           /* versioning */
        "jobId": "...",                         /* see API above */
        "jobName": "...",                       /* see API above */
        "phases": [ {                           /* see API above */
                "type": "...",
                "count": "...",
                "datasetVersion": "...",
                "assets": [ "..." ],
                "args": { "...": "..." },
                "exec": "..."
        } ],
        "inputKeys": [ "..." ],                 /* see API above */
        "failFast": false,                      /* see API above */
    
        "state": "...",                         /* see API above */
        "error": "...",                         /* see API above */
        "createTime": "...",                    /* see API above */
        "finishTime": "...",                    /* see API above */
    
        "results": [
                [                               /* phase 1 results */
                        {                       /* input key 1 result */
                                "machine": "...",       /* zonename */
                                "input": "...",         /* input key */
                                "state": "...",         /* queued, etc. */
                                "result": "...",        /* success? */
                                "partial": [ "..." ],   /* only while running */
                                "outputs": [ "..." ],   /* only on success */
                                "discarded": [ "..." ], /* only on failure */
                                "startTime": "...",
                                "doneTime": "...",
                                "error": "..."          /* only on failure */
                        },
                        {                       /* input key 2 result */
                                "machine": "...",
                                "input": "...",
                                "state": "...",
                                "result": "...",
                                "partial": [ "..." ],
                                "outputs": [ "..." ],
                                "discarded": [ "..." ],
                                "startTime": "...",
                                "doneTime": "...",
                                "error": "..."
                        }
                ] /* end phase 1 results */
        ]
    
        "jobLog": [ "..." ],
    
        /* internal state */
        "worker": "...",                        /* owner worker's uuid */
        "lastCompletedPhase": "..."             /* index into phases, or -1 */
    }

Here's what an internal task group assignment looks like:

    {
        "version": 0,                           /* version written by worker */
        "maxVersion": 0,                        /* max version worker accepts */
        "jobId": "...",                         /* see API above */
        "taskGroupId": "...",                   /* identifies this record */
        "host": "...",                          /* destination host */
        "inputKeys": [ "..." ],                 /* keys for this task group */
        "phase": {                              /* phase for this task group */
                "datasetVersion": "...",
                "assets": [ "..." ],
                "args": { "...": "..." },
                "exec": "..."
        },
        "failFast": false,                      /* see API above */
        "state": "...",                         /* current status */
        "error": "...",                         /* only on failure */
        "results": [ {                          /* see job record */
                "machine": "...",
                "input": "...",
                "state": "...",
                "result": "...",
                "partial": [ "..." ],
                "outputs": [ "..." ],
                "discarded": [ "..." ],
                "startTime": "...",
                "doneTime": "...",
                "error": "..."
        }, {
                "machine": "...",
                "input": "...",
                "state": "...",
                "result": "...",
                "partial": [ "..." ],
                "outputs": [ "..." ],
                "discarded": [ "..." ],
                "startTime": "...",
                "doneTime": "...",
                "error": "..."
        } ]
    }

Here are the Moray operations on these objects:

Web tier:

* POST /jobs (create new job)
* GET /jobs/jobId (get job status)
* PUT /jobs/jobId (rare case: use T&S to set cancelled = true)

Workers:

* GET /jobs where worker == null or mtime < ... (pick up new jobs)
* PUT /jobs/jobId (update job state)
* PUT /taskgroups/taskGroupId (create/update assignment)
* GET /taskgroups/taskGroupId (check task group state)

Agents:

* GET /taskgroups where host == me (pick up new task groups)
* GET /taskgroups/taskGroupId (check task group state)
* PUT /taskgroups/taskGroupId (post task group results)


## Cancellation

To cancel a job, the web tier writes "cancelled: true" to the job record.  At
this point, we could say that the job state is immutable and all future state
requests only show whatever state has been completed up to this point.

Asynchronously, the job worker will need to be polling on the job state
occasionally to notice the cancelled flag.  It propagates this to individual
task assignments, where the compute node agents will notice that the job has
been cancelled and abort them.  Eventually, all work on the job will complete,
though this may take some time to propagate through the system.


## CN agent

As described above, compute node agents receive task assignments via Moray and
report progress back to Moray.  Such progress includes keys processed and state
changes, including that a task is queued, loaded, running, timed out, failed, or
done.)

The agent maintains a pool of compute zones that are either "uninitialized",
"ready", or "busy".  Uninitialized zones are those currently in an arbitrary
state.  Those are asynchronously transitioned to "ready" by the following steps:

1. Halt the zone, if it's not already halted.
2. "zfs rollback" to the zone's origin snapshot
3. Boot the zone.
4. Sets up an agent inside the zone that listens for local HTTP requests to the
   Task Control API and forwards them via a zsock server to the CN agent.

When a new task is dispatched, if there are no "ready" zones available, tasks
are enqueued for later.  Otherwise, the agent picks a "ready" zone and does the
following:

1. Downloads the job's bundle's assets into the zone.
2. Creates a hyprlofs mount for the files.
3. Finally, invokes the bundle's exec script.

When the job completes, successfully or otherwise, the CN agent reclaims the
zone by marking it "uninitialized" again and asynchronously transitioning it to
"ready" again to be used for another task.

To implement the Task Control API:

`/info` is simple: it just retrieves data from the agent's persistent state,
which already has to exist to manage the job.

`/objects/:key` makes a note in the task state that this object is being
written, and then (first approach) proxies the request directly to manta. But
see "open questions" below.

`/commit` updates the persistent state for the task to indicate that all
objects written since the last checkpoing have been committed.

`/fail` also updates the persistent state, but indicates that new objects should
be discarded.  They won't actually be deleted, but marked as discarded in the
job state so users can look at them later.  After that, this request terminates
the zone.

`/abort` is like `/fail`, except that before terminating it gcores all processes
in the user's contract and does an incremental "zfs send" of the zone to a new
manta file.  (We'll want some automated mechanism to reconstruct the user's zone
based on this, presumably by "zfs receive" the dataset and then "zfs receive -i"
the stream associated with this task.)

In order to survive transient failure, the agent must store state on disk.  The
current plan is to use sqlite for this because it already has usable Node
bindings.

## Open questions

Roughly in order of importance:

* When user code wants to write files to manta, how exactly does that work? Do
  we have a way to tell Manta to write one copy on the local box, and if so, do
  we avoid sending it across the network twice?
* How do we monitor the user code's contract? Presumably we'll need an agent
  in the zone that bootstraps their code and watches the contract (but then we
  need to figure out what to do if that guy dies -- adopt the contract?)
* What metrics do we want to provide via CA?  How will we implement these?
  What other changes must be made to CA to support Marlin and Manta?
* What's the algorithm for assigning tasks to compute nodes? A pretty dumb one
  will probably suffice for a while, but it might be nice if the worker kept
  track of what everyone was working on to better distribute work.
* Should the job log be a manta object so that user's can add their own entries?

## Management API?

cancel tasks, take compute zones and servers in and out of service
initial setup: creating compute zones
tuning # of compute zones

## Rough roadmap

The basic idea is to get a prototype up and running relatively quickly and
build that out as needed.  This is a *very* rough draft of how to get there.

Today we have:

* a Moray implementation that supports GET/DEL/PUT (with test-and-set) and
  search, as well as a client-side library that supports throttling.
* a mock Moray implementation, at least for testing the job worker
* an agent that takes task group specifications and executes each task in the
  group, including support for assets and both "generic" and "map" tasks, using
  hyprlofs to map files in.
* a job worker that picks up unassigned jobs in Moray, executes them, and
  can pick up partially-completed jobs.  Also times out idle task groups.
* a very simple web tier that just drops jobs into Moray and reads state back.

Remaining work not necessarily required for the end-to-end demo includes:

* moray: search by bucket change number
* moray client: support for consistent hashing
* agent: automated tests
* agent: crash recovery (storing state on disk)
* agent: reliable monitoring for the user's program (using contracts)
* agent: implement "abort" (which saves all zone state)
* agent/worker: implement explicit fail
* agent/worker: implement retry
* worker: locate objects in Manta
* all: Moray record versioning
* all: implement cancel
* all: implement tools for observing global system state
* all: deployment
